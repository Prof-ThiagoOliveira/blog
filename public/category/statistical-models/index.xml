<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Models | Thiago Oliveira</title>
    <link>https://prof-thiagooliveira.netlify.com/category/statistical-models/</link>
      <atom:link href="https://prof-thiagooliveira.netlify.com/category/statistical-models/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistical Models</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Thiago Oliviera</copyright><lastBuildDate>Fri, 19 Jan 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prof-thiagooliveira.netlify.com/media/icon_hucac388deec264b13c6395804f04d3e9e_484996_512x512_fill_lanczos_center_3.png</url>
      <title>Statistical Models</title>
      <link>https://prof-thiagooliveira.netlify.com/category/statistical-models/</link>
    </image>
    
    <item>
      <title>Exploring Polynomial, Fractional Polynomial, and Spline Models</title>
      <link>https://prof-thiagooliveira.netlify.com/post/statistics/</link>
      <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://prof-thiagooliveira.netlify.com/post/statistics/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt;
The ability to accurately model and interpret complex data sets is paramount. This technical exploration delves into three sophisticated modelling techniques:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polynomial Models&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fractional Polynomials&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spline Models&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;justify&#34;&gt;
Each of these models serves as a fundamental tool in the statistical toolkit, enabling us to capture and understand the intricacies of linear and non-linear relationships inherent in real-world data.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
As a bio-statistician entrenched in the technical aspects of data analysis, I recognize the critical importance of these models. We will commence with an examination of Polynomial Models, discussing their mathematical underpinnings and practical applications in capturing curvilinear trends. Next, we will navigate through the Fractional Polynomials, a more flexible extension of traditional polynomials, adept at modelling asymmetric patterns. Lastly, we will explore Spline Models, one of the most flexible approach in data fitting, capable of adapting to complex and segmented patterns in data.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
This post is designed not just to inform but to provide a technical understanding of these models with examples, illustrating their relevance and application in contemporary data analysis. Whether you are a data scientist, a statistician, or a researcher grappling with complex data sets, this exploration aims to enhance the modelling arsenal, offering insights into when and how to apply these models effectively.
&lt;/p&gt;
&lt;h1 id=&#34;polynomial-models&#34;&gt;Polynomial Models&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
[Polynomial Models](https://en.wikipedia.org/wiki/Polynomial), represented by functions of the form $$y = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0,$$ are foundational in modelling curvilinear relationships. In this formulation, each $a_i$ (where $i = 0, 1, \ldots, n$) denotes the coefficient corresponding to the $i$-th term of the polynomial, and $x$ is the independent or exploratory variable. The degree $n$ of the polynomial determines the model&#39;s complexity, with higher degrees allowing for more intricate curve shapes.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
These models are particularly useful in capturing the non-linear dynamics often observed in real-world data. For instance, a quadratic model (where $n = 2$) can describe simple parabolic trends, while higher-degree models, such as cubic ($n = 3$) or quartic ($n = 4$), enable the representation of more complex and varied behaviours (Figure 1).
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r,echo=FALSE,fig.cap=&amp;quot;Comparative&#34;&gt;knitr::include_graphics(&#39;polynomial.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
However, increasing the degree $n$ also increases the risk of overfitting, a phenomenon where the model adapts too closely to the specificities of the training data, including noise, at the expense of generalizability to new data (Figure 2). Overfitting leads to models that perform poorly in predictive scenarios, failing to capture the true underlying trend.
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r,echo=FALSE,fig.cap=&amp;quot;Visualizing&#34;&gt;knitr::include_graphics(&#39;poly_overfit.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
Polynomial models are extensively applied across various disciplines. In physics, they are instrumental in [modelling motion](https://aapm.onlinelibrary.wiley.com/doi/abs/10.1118/1.2739811) under uniform acceleration, among other phenomena. In economics, [polynomial trends are fitted to time series data](https://www.sciencedirect.com/science/article/abs/pii/S0378475405000418) to understand market dynamics. In biological sciences, these models aid in [interpreting growth rates](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1574-0862.2010.00450.x) and [simple gene expression patterns](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-017-4304-3). The interpretation of the coefficients $a_i$ can provide significant insights; for instance, in the quadratic model $y = ax^2 + bx + c$, the sign of $a$ determines the direction in which the parabola opens, offering crucial information about the nature of the relationship being modelled (Figure 3).
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r,echo=FALSE,fig.cap=&amp;quot;Parabola&#34;&gt;knitr::include_graphics(&#39;parabola.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
In practical applications, the selection of the polynomial degree $n$ is critical. It is a balance between capturing the complexity of the data and avoiding overfitting (Figure 2). Techniques such as cross-validation, where the data is divided into training and testing sets, can be used to determine the optimal degree of the polynomial. Additionally, statistical measures like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) are often employed to select the most appropriate model by balancing model fit and complexity.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In summary, Polynomial Models are a versatile and powerful tool in statistical modelling. Their ability to approximate complex functions with a relatively straightforward mathematical formulation makes them a fundamental component in various fields of data analysis.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
While traditional polynomial models are highly effective, they sometimes lack the flexibility required for certain types of data, particularly those exhibiting asymmetric trends. This limitation led to the development of fractional polynomial models, which extend the concept of polynomial models by allowing for fractional exponents. This advancement provides a greater ability to fit a wider range of curves and is especially useful in cases where the relationship between variables is not adequately captured by integer exponents.
&lt;/p&gt;
&lt;h1 id=&#34;fractional-polynomial-models&#34;&gt;Fractional Polynomial Models&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
[Fractional Polynomials](https://academic.oup.com/jrsssc/article-abstract/43/3/429/6990357) represent an advanced evolution of traditional polynomial models, marked by their use of non-integer, real-number exponents in the independent variable. Mathematically, they are expressed as $$y = \beta_0 + \beta_1 x^{p_1} + \beta_2 x^{p_2} + \ldots + \beta_n x^{p_n},$$ where $x$ is the independent variable, $\beta_i$ are coefficients, and $p_i$ are the variable powers. These powers, unlike the integer-only powers in traditional polynomials, can include any real number like $0.5$, $-1$, or $2.3$. This flexibility significantly broadens the modeling capability of polynomials, allowing for more precise fitting to complex and asymmetric data patterns. Terms such as $x^{-1}$ and $x^{0.5}$, representing the reciprocal and square root of $x$ respectively, enable the modeling of relationships that exhibit dramatic changes over different ranges of $x$, a task challenging for standard polynomial models with integer exponents.
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r,echo=FALSE,fig.cap=&amp;quot;Comparative&#34;&gt;knitr::include_graphics(&#39;frac_poly.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
The utility of fractional polynomials extends to various fields, notably in medical statistics and biological data analysis, where data patterns often defy symmetry. They are particularly adept in modeling phenomena like dose-response curves in pharmacokinetics and progression rates of diseases, where the response changes in a non-linear fashion. Such flexibility makes them invaluable in scenarios where data exhibit complex, non-standard behaviors.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
However, the complexity of fractional polynomials can pose interpretational challenges, and like their traditional counterparts, they are susceptible to overfitting. This risk is heightened with the inclusion of multiple fractional terms or higher degrees, necessitating careful model selection and validation processes. Methods such as cross-validation or the use of information criteria like AIC or BIC are often employed to balance model fit against the risk of overfitting.
&lt;/p&gt;
&lt;h2 id=&#34;finding-optimal-power-values-in-fractional-polynomials&#34;&gt;Finding Optimal Power Values in Fractional Polynomials&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The process of finding the best values for $p_1, p_2, \ldots, p_n$ is inherently iterative and may require a combination of statistical testing, validation techniques, and expert judgement. The goal is to have a balance between a model that fits the data well, is not overly complex, and is robust to variations in model parameters.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In Figure 5, I exemplify this process by fitting various fractional polynomial models to the dataset. Through the application of the Bayesian Information Criterion (BIC), I identified the most parsimonious model, which is succinctly expressed mathematically as $$y = \beta_0 + \beta_1 x^{1.13},$$ and the corresponding BIC value for this model is denoted as $â144.34$.
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r,echo=FALSE,fig.cap=&amp;quot;Comparative&#34;&gt;knitr::include_graphics(&#39;fractional_polynomial_fit_with_bic.gif&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
Below I have described some possible approaches to achieve this objective:
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Initial Power Selection**: Start with a set of candidate powers, often including a mix of positive, negative, and fractional values. Common choices are \(-2, -1, -0.5, 0, 0.5, 1, 2, 3\). The selection of these initial powers is guided by prior knowledge about the data, theoretical considerations, or exploratory analysis.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Model Fitting and Comparison**: Fit fractional polynomial models using different combinations of these candidate powers. This fitting can be done using least squares regression or other suitable methods depending on the nature of the data. For each model, compute a goodness-of-fit statistic, such as the residual sum of squares (RSS) or the Akaike Information Criterion (AIC).
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Iterative Testing**: Employ an iterative approach to test various combinations of powers. This might involve starting with a simple model and gradually adding complexity (increasing the number of terms) while monitoring the improvement in the fit. 
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Cross-Validation**: To guard against overfitting, especially in models with higher degrees or more terms, use cross-validation. Divide your data into training and testing sets. Fit the model to the training set and evaluate its performance on the testing set. This step helps in assessing the model&#39;s predictive accuracy.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Statistical Significance**: Assess the statistical significance of the coefficients associated with each term in the model. Non-significant terms might suggest that certain powers do not contribute meaningfully to the model and could be excluded.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Model Selection Criteria**: Use model selection criteria like AIC or BIC to compare models with different combinations of powers. These criteria balance the goodness of fit with the complexity of the model, helping to choose a model that is both accurate and parsimonious.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Sensitivity Analysis**: Conduct sensitivity analyses by varying the powers slightly to see how robust the model is to changes in these parameters. This step is crucial to understand the stability and reliability of the model.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
While fractional polynomials offer enhanced modelling flexibility over traditional polynomials, they sometimes fall short in handling data with distinct behavioural changes across different segments. This limitation summed to the difficulty in determine $p_i$ are where spline models come into play. Spline models, constructed as piecewise polynomials, provide localized fitting capabilities, adapting seamlessly to variations within different data segments. Such adaptability is particularly useful in datasets with distinct phases or regimes. Thus, spline models emerge as a natural progression when fractional polynomials alone are insufficient to model the intricate patterns present in the data
&lt;/p&gt;
&lt;h1 id=&#34;spline-models&#34;&gt;Spline Models&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
[Splines](https://www.taylorfrancis.com/chapters/edit/10.1201/9780203738535-7/generalized-additive-models-trevor-hastie) are a class of mathematical functions used in statistical modeling, defined as piecewise polynomials that are joined at specific points called knots. Mathematically, if we denote a spline function by $S(x)$, it can be represented as:
$$S(x) = \begin{cases} 
P_1(x) &amp; \text{for } x \leq k_1, \\
P_2(x) &amp; \text{for } k_1 &lt; x \leq k_2, \\
\vdots \\
P_n(x) &amp; \text{for } x &gt; k_{n-1},
\end{cases}$$
where $P_i(x)$ are polynomial functions of degree $d$, typically represented as $$P_i(x) = a_{i0} + a_{i1}x + a_{i2}x^2 + \ldots + a_{id}x^d$$ for each piece of the spline, with $a_{i0}, a_{i1}, \ldots, a_{id}$ being the coefficients that vary from one segment of the spline to another. The knots $k_1, k_2, \ldots, k_{n-1}$ are specific values in the domain of $x$ where these polynomial pieces meet. Splines ensure continuity and smoothness at these knots by enforcing that both the function and its derivatives up to degree $d-1$ are continuous across the knots. This mathematical formulation allows splines to flexibly model a wide range of functions by varying the number and position of the knots as well as the degree of the polynomial pieces.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Splines, with their diverse forms, can have a variety of applications, each type distinguished by its unique features and mathematical formulation. Linear splines represent the most basic form, where each segment $P_i(x)$ is a linear function, typically $P_i(x) = a_{i0} + a_{i1}x$. These are straightforward to compute and are used in applications requiring simple, piecewise linear approximations.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Cubic splines elevate the complexity and smoothness, with each $P_i(x)$ being a third-degree polynomial: $$P_i(x) = a_{i0} + a_{i1}x + a_{i2}x^2 + a_{i3}x^3$$. Their widespread adoption is attributed to their ability to model smooth, continuous curves, making them highly suitable for applications in curve fitting, computer-aided design, and modelling non-linear relationships in data.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
[B-splines](https://link.springer.com/content/pdf/10.1007/978-0-387-84858-7_5.pdf), or basis splines, while they can also be cubic, offer a more general and flexible approach. They are defined not by the splines themselves, but by a set of basis functions, which are piecewise polynomials of a specified degree. The key advantage of B-splines is their local control; adjustments in one part of the spline affect only a limited region around that part. This is due to their basis functions having minimal support, which means each function is non-zero only over a small interval. 
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For B-splines, the mathematical representation shifts from individual polynomial pieces to a sum of basis functions, each weighted by a coefficient. These basis functions are defined over the knots and have local support. The B-spline representation can be expressed as:
&lt;/p&gt;
&lt;p&gt;$$S(x) = \sum_{i=0}^{n+d} B_{i,d}(x) \cdot c_i$$&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Here, $B_{i,d}(x)$ are the B-spline basis functions of degree $d$, and $c_i$ are the coefficients. The basis functions $B_{i,d}(x)$ are defined recursively, starting with degree 0 (piecewise constant functions) and building up to the desired degree. The recursive definition for a B-spline basis function of degree $d$ is given by:
&lt;/p&gt;
&lt;p&gt;$$ B_{i,0}(x) = \begin{cases} 
1 &amp;amp; \text{if } k_i \leq x &amp;lt; k_{i+1}, \
0 &amp;amp; \text{otherwise},
\end{cases} $$&lt;/p&gt;
&lt;p&gt;$$ B_{i,d}(x) = \frac{x - k_i}{k_{i+d} - k_i} B_{i,d-1}(x) + \frac{k_{i+d+1} - x}{k_{i+d+1} - k_{i+1}} B_{i+1,d-1}(x) $$&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
The knots $k_0, k_1, \ldots, k_{n+d}$ in the case of B-splines extend beyond the range of the data to ensure that the spline is well-defined at the boundaries. This expansion of the knot sequence provides a framework for the B-spline basis functions to cover the entire domain of the data. Unlike traditional splines, where each polynomial piece is defined explicitly, B-splines construct the spline function as a linear combination of these basis functions, providing a high degree of flexibility and local control over the shape of the spline. This formulation allows for efficient computation and adjustments in a localized manner, making B-splines a powerful tool for modelling complex data in various applications.&lt;/p&gt;
&lt;/p&gt;
&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
Figure 6 visualizes the fitting of four distinct statistical models to a dataset: 
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a cubic polynomial,&lt;/li&gt;
&lt;li&gt;a fractional polynomial,&lt;/li&gt;
&lt;li&gt;a cubic spline, and&lt;/li&gt;
&lt;li&gt;a B-spline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;justify&#34;&gt;
The explanatory variable is shown along the x-axis, while the response variable is on the y-axis.
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r,echo=FALSE,fig.cap=&amp;quot;Comparison&#34;&gt;knitr::include_graphics(&#39;splines.png&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
The plot provides a comparative visualization of four distinct statistical models applied to the same data set, with the cubic polynomial model (solid red line) demonstrating a poor fit. It captures the global tendency but lacks the intricacy to model local data variations, likely due to its restrictive assumption of a single global function without &#39;breaks&#39;. In contrast, the fractional polynomial (dotted blue line) incorporates power transformations of the explanatory variable, allowing for a more flexible functional form and better accommodation of non-linear relationships within the data. Nevertheless, it continues to apply a global approach to the data, which is evidently insufficient for modelling the localized fluctuations observed in the dataset. Both models underperform, suggesting that their global fitting strategies are inadequate for datasets. A more complex polynomial might offer an enhanced fit, but at the risk of overfitting, which must be judiciously managed.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
More nuanced fits are achieved with the spline-based models. The cubic spline (dashed green line) employs piecewise third-degree polynomials, joining at the knotsâstrategically placed along the domain of the explanatory variableâto enhance model flexibility. This allows the cubic spline to conform more closely to the data&#39;s local variations and structural shifts. The B-spline model (long-dashed purple line), with its basis function approach, offers a superior level of smoothness and local control, manifesting in its ability to trace the dataset&#39;s oscillatory pattern with considerable precision.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
The visual assessment thus underscores the critical role of model selection in statistical analysis, emphasizing that the appropriateness of a model is contingent upon its alignment with the data&#39;s inherent patterns and the analysis objectives. The choice of model has profound implications for the accuracy of predictions and the robustness of inferences drawn from the data.
&lt;/p&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The primary challenge in using spline models lies in the selection of knots. The number and location of knots significantly influence the model&#39;s complexity and its ability to capture underlying patterns in data. Too few knots can lead to underfitting, while too many can cause overfitting. There is no universal method for optimal knot placement, often requiring a combination of data-driven techniques and expert judgement. Additionally, spline models can become computationally intensive with an increasing number of knots.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Splines are implemented through specialized algorithms that construct the piecewise polynomial functions and determine the appropriate knot positions, like in the `R` package `splines`. These algorithms often utilize basis functions, such as B-spline basis functions, which provide a stable and efficient way to represent spline curves. Computational techniques, like penalized least squares for smoothing splines, are employed to balance model fit and smoothness. Most statistical software packages offer built-in functions for spline modelling, simplifying their application in practical scenarios.
&lt;/p&gt;
&lt;h2 id=&#34;selection-process-for-spline-models&#34;&gt;Selection Process for Spline Models&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The selection process for spline models involves determining the optimal type and number of splines, along with the placement of knots, to best fit the data while avoiding overfitting. This process is a critical step in spline modelling and typically involves a blend of statistical techniques and domain expertise.
&lt;p align=&#34;justify&#34;&gt;
**Type Selection**: The first step is choosing the type of spline (e.g., linear, cubic, B-spline, smoothing spline). The choice depends on the nature of the data and the smoothness required in the model. For instance, cubic splines are often chosen for their balance between flexibility and smoothness.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Number of Knots**: Deciding on the number of knots is crucial as it controls the flexibility of the spline model. As explained previously, fewer knots result in a smoother, but potentially underfit model, while more knots can capture finer details but risk overfitting. Methods like cross-validation can be used to determine an optimal number of knots (`caret` package in `R`). In cross-validation, the data is divided into subsets; the model is trained on some subsets and validated on others, with the goal of minimizing prediction error.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Knot Placement**: The location of knots is equally important. Knots can be placed at quantiles of the independent variable, which is a common strategy for evenly distributing them across the range of data. Alternatively, adaptive methods can be used where knot placement is data-driven, focusing on regions where the function appears to change rapidly.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Model Complexity and Regularization**: For smoothing splines, the degree of smoothness is controlled by a smoothing parameter. Techniques like the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or [generalized cross-validation (GCV)](https://www.sciencedirect.com/science/article/abs/pii/S0005109817306416)
 are used to select this parameter. These methods aim to find a balance between the goodness of fit and the complexity of the model.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
**Model Validation**: After selecting the spline model, it is validated using unseen data or through resampling methods like bootstrapping. This step is crucial to ensure that the model generalizes well and does not merely capture the idiosyncrasies of the training data.
&lt;/p&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;For attribution, please cite this work as:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;::: div-1
Oliveira T.P. (2024, Jan. 25). Exploring the Technicalities of Data Fitting - Polynomial, Fractional Polynomial, and Spline Models
:::&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;BibTeX citation&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;@misc{oliveira2024polynomials,
  author = {Oliveira, Thiago},
  title = {Exploring the Technicalities of Data Fitting - Polynomial, Fractional Polynomial, and Spline Models},
  url = {https://prof-thiagooliveira.netlify.app/post/exploring-the-technicalities-of-data-fitting-polynomial-fractional-polynomial-and-spline-models/},
  year = {2024}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Did you find this page helpful? Consider sharing it ð&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How hard is it to predict COVID-19 cases?</title>
      <link>https://prof-thiagooliveira.netlify.com/post/how-hard-is-it-to-predict-covid-19-cases/</link>
      <pubDate>Wed, 16 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://prof-thiagooliveira.netlify.com/post/how-hard-is-it-to-predict-covid-19-cases/</guid>
      <description>


&lt;div id=&#34;tldr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;tl;dr&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Many different variables affect how the pandemic progresses and it is extremely difficult to identify each one, and precisely measure them.&lt;/li&gt;
&lt;li&gt;The data we have is surely innacurate, but could be a good proxy for understanding the behaviour of the coronavirus outbreak&lt;/li&gt;
&lt;li&gt;We developed a statistical model to obtain short-term forecasts of the number of COVID-19 cases&lt;/li&gt;
&lt;li&gt;We constantly update forecasts and make all results freely available to any country in the world through a web &lt;a href=&#34;link&#34;&gt;app&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-many-people-will-get-infected-tomorrow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How many people will get infected tomorrow?&lt;/h1&gt;
&lt;p&gt;âHow many cases do you think weâre going to have today?â, my fiancÃ¨e asked me just as Iâm writing this post â and quite frankly Iâve asked that myself many times over the last several months. Wouldnât it be great if we had a method to accurately predict the number of confirmed COVID-19 cases weâll have every single day for, say, the next month? If we could do that, weâd know whether our measures to contain the virus are working, whether we would be able to lift particular restrictions here and there, invest in intensive care units, or whether that wedding we had planned long ago would finally happen or have to be postponedâ¦ again.&lt;/p&gt;
&lt;p&gt;It is very hard, however, to pinpoint exactly every single factor that affects the number of reported COVID-19 cases, and most importantly, measure them all. Here we try and outline different techniques we could use to try and predict how the outbreak will behave in the future, and show a particular method we have developed to obtain short-term forecasts with a reasonable degree of accuracy. We have packaged the method into an app, which you can access &lt;a href=&#34;link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-strategies-can-we-use&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What strategies can we use?&lt;/h1&gt;
&lt;p&gt;There are many different strategies and mathematical/statistical tools we can use to attempt to predict the future. These can include what we call mechanistic, or compartment models, for example. These make assumptions based on empirical evidence of the biological system being studied and translate them into mathematical equations based on the flow of individuals to/from specific compartments. For COVID-19 the SEIR-type model has been widely used by many research groups to describe the behaviour of the outbreak (see our &lt;a href=&#34;https://www.hamilton.ie/covid19/posts/2020-09-11-how-long-will-covid-19-last-in-ireland/&#34;&gt;blog post&lt;/a&gt; on the use of SEIR models to predict when the pandemic will end). They are realistic in the sense that they reflect the epidemiological behaviour of the outbreak.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SEIR.png&#34; width=&#34;700px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are other alternatives that do not take into account the true biological nature of the phenomenon per se, but may use it as input in a different way. Many &lt;a href=&#34;https://en.wikipedia.org/wiki/Machine_learning&#34;&gt;machine learning&lt;/a&gt; techniques could sometimes be seen as black-box methods, that would e.g.Â take the reported number of past COVID-19 cases and other variables that we would believe could influence this number and spit out a prediction for tomorrow, or next week, next month, etc. There are cases where these methods are even more accurate than mechanistic models, however there is a trade-off to consider here in terms of prediction accuracy vs.Â explainability, as &lt;a href=&#34;https://royalsocietypublishing.org/doi/10.1098/rsbl.2017.0660&#34;&gt;discussed here&lt;/a&gt;. If a new event or variable comes into play, which could empirically be very important to dictate the future behaviour of the pandemic, it is very difficult to gauge its effects using a black-box method.&lt;/p&gt;
&lt;p&gt;We could also simply assume that the number of reported COVID-19 cases today is purely a reflection of the reported number of cases yesterday, and the day before, and so on. So we pretty much assume all variables that influence this process can be summarised purely by the outcomes we have observed in the past, and this can in turn be used to forecast what future numbers will be. Of course, there are plenty of different ways to include other variables in these types of models, but the important thing is to notice that we place a very heavy assumption on an underlying process that is able to explain its own behaviour. We usually refer to these models as &lt;a href=&#34;https://en.wikipedia.org/wiki/Time_series&#34;&gt;âtime seriesâ&lt;/a&gt; or âstate-spaceâ models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-is-it-so-difficult&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why is it so difficult?&lt;/h1&gt;
&lt;p&gt;There are many factors that influence our ability to predict the number of future COVID-19 cases. Imagine we have, for example, a fantastic SEIR-type model that can reproduce the dynamics of the disease almost perfectly up to today. To be able to predict with great accuracy what will happen tomorrow (or even further down the line), we must assume, among other things, that the assumptions that hold today will still hold tomorrow and so on. If any new variable comes into play, or if the variables that are involved change over time, our predictions can be completely off.&lt;/p&gt;
&lt;p&gt;This is not the worst problem, however. There are in fact many variables that weâre simply not able to measure with good precision. This includes knowing, for example, where everybody in the country is at all times, who they talk to, for how long, where they will be, etc. This is why it is important to do &lt;a href=&#34;https://www2.hse.ie/conditions/coronavirus/close-contact-and-casual-contact.html&#34;&gt;contact tracing&lt;/a&gt;, although this matters mostly in a retrospective way, not necessarily to predict what will happen in the future.&lt;/p&gt;
&lt;p&gt;But wait a minute now, we donât even know whether the data we can actually measure is in fact accurate! Or to be more specific, we do know that our data is definitely &lt;em&gt;not&lt;/em&gt; 100% accurate. Cases reported today could reflect infections that happened between a few days ago to several weeks. Tests are not 100% accurate either, so there is a pool of false positives in there, as well as false negatives not being included in the whole sum. Simply put, the data we have is pretty much a proxy of the real thing. Hence why it is so important to understand what these numbers could actually mean, and not &lt;a href=&#34;https://www.newscientist.com/article/mg24732954-000-david-spiegelhalter-how-to-be-a-coronavirus-statistics-sleuth/&#34;&gt;imbue them with improper meaning&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-short-term-forecasting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What about short-term forecasting?&lt;/h1&gt;
&lt;p&gt;So long-term forecasting is very prone to built-up variation and error, as we all know. Itâs just like predicting what time youâll wake up on your birthday 10 years from now. But there must be something we could do in the short-term, right? Well, it depends on how âlongâ this short-term is. And it also depends on how we want to use this information.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.00111&#34;&gt;We developed a modelling framework&lt;/a&gt; in an attempt to predict the number of reported COVID-19 cases for up to 7 days in the future. We fitted our models to the data collected by the &lt;a href=&#34;https://www.ecdc.europa.eu/en&#34;&gt;ECDC&lt;/a&gt; to generate the forecasts. See below for a validation study we carried out back in May/2020.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;forecast.png&#34; width=&#34;800px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The panels are in the logarithmic scale, but in essence, the closer the points are to the identity line (dashed line), the closer our model was in predicting the number of COVID-19 cases up to 7 days ahead (panels in part &lt;em&gt;A&lt;/em&gt;). In part &lt;em&gt;B&lt;/em&gt; we see that the accuracy of the method is high for all 7 days ahead, but we begin to lose in terms of precision from day four onwards. (&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; represents &lt;a href=&#34;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&#34;&gt;Pearsonâs linear correlation coefficient&lt;/a&gt;, the closest it is to 1 the better the method is; the same applies to the CCC - the &lt;a href=&#34;https://en.wikipedia.org/wiki/Concordance_correlation_coefficient&#34;&gt;concordance correlation coefficient&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;The idea behind this is not to be able to inform governments the exact numbers weâd expect tomorrow, but to give more perspective in terms of the types of trends we expect in the near future. This is useful to inform decision making related to the healthcare services. For instance, if a particular countryâs healthcare system is currently at capacity, and we are predicting an upward trend in the number of infections, then this could guide policy in terms of resource allocation to accommodate the extra patients that are likely to seek health professionals in the upcoming weeks. This is why it is so important to look at overall trends (for example, the number of cases per 100,000 people over the last 14 days).&lt;/p&gt;
&lt;p&gt;Our model creates predictions based on two components. The first, called the autoregressive component, uses information on the past number of cases to predict future ones. The second is included to account for extra variability that could occur for a variety of different reasons. The autoregressive component is directly linked to the behaviour of the outbreak, so it is useful to detect waves of the pandemic. See, for example, our latest estimates for Ireland:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Ireland.png&#34; width=&#34;500px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see that towards the end of July this second wave was already starting to take shape, and now we are aiming at a new peak of cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-countries-together&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Grouping countries together&lt;/h1&gt;
&lt;p&gt;Now that we have profiles for each country on how the pandemic is behaving in terms of number of cases, perhaps it would be a good idea to look at which countries present a similar behaviour over the last, say, 60 days. We created a dendrogram based on a cluster analysis performed using the values of the autoregressive parameter and produced the figure below â&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dendrogram.png&#34; width=&#34;1000px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that over looking at the past two months, the country that has presented the most similar behaviour to Ireland was Croatia. In our &lt;a href=&#34;link&#34;&gt;app&lt;/a&gt; you can play with different ways of presenting the dendrogram, as well as print names of different countries in bold to aid in finding them easily when looking at the picture. You can also change the number of clusters.&lt;/p&gt;
&lt;p&gt;Perhaps these comparisons would be useful in terms of comparing government policies on how to deal with the COVID-19 outbreak, and learn lessons from successful policies vs unsuccessful ones. Also, this type of modelling can help to detect a further wave of the outbreak sooner rather than when we are already in the middle of it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;all-models-are-wrong&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;All models are wrongâ¦&lt;/h1&gt;
&lt;p&gt;In the end of the day, there is no true, correct model we can apply. After all, it is impossible to know exactly what the data generating mechanism is. We can only attempt to understand it and reproduce its behaviour using mathematical/statistical tools. We hope, however, that our modelling approach can be useful. We could point a whole list of problems with it here, such as completely ignoring biological mechanisms and using just past behaviour to explain future behaviour without any additional context. But we believe it represents a reasonable attempt at forecasting the number of COVID-19 cases in the short-term.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Did you find this page helpful? Consider sharing it ð&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;citation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Citation&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For attribution, please cite this work as:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;div-1&#34;&gt;
&lt;p&gt;Moral, et al.Â (2020, Sept.Â 29). Irelandâs COVID-19 Data Dive: How hard is it to predict COVID-19 cases?. Retrieved from &lt;a href=&#34;https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/&#34; class=&#34;uri&#34;&gt;https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;BibTeX citation&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;@misc{moral2020how,
  author = {Moral, Rafael and Oliveira, Thiago and Parnell, Andrew},
  title = {Ireland&amp;#39;s COVID-19 Data Dive: How hard is it to predict COVID-19 cases?},
  url = {https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/},
  year = {2020}
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
