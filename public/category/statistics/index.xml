<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics | Thiago Oliveira</title>
    <link>https://prof-thiagooliveira.netlify.com/category/statistics/</link>
      <atom:link href="https://prof-thiagooliveira.netlify.com/category/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021 Thiago Oliviera</copyright><lastBuildDate>Fri, 19 Jan 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://prof-thiagooliveira.netlify.com/media/icon_hucac388deec264b13c6395804f04d3e9e_484996_512x512_fill_lanczos_center_3.png</url>
      <title>Statistics</title>
      <link>https://prof-thiagooliveira.netlify.com/category/statistics/</link>
    </image>
    
    <item>
      <title>Exploring the Technicalities of Data Fitting</title>
      <link>https://prof-thiagooliveira.netlify.com/post/statistics/</link>
      <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://prof-thiagooliveira.netlify.com/post/statistics/</guid>
      <description>


&lt;p align=&#34;justify&#34;&gt;
The ability to accurately model and interpret complex data sets is paramount. This technical exploration delves into three sophisticated modelling techniques:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Polynomial Models&lt;/strong&gt;,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fractional Polynomials&lt;/strong&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spline Models&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&#34;justify&#34;&gt;
Each of these models serves as a fundamental tool in the statistical toolkit, enabling us to capture and understand the intricacies of linear and non-linear relationships inherent in real-world data.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
As a bio-statistician entrenched in the technical aspects of data analysis, I recognize the critical importance of these models. We will commence with an examination of Polynomial Models, discussing their mathematical underpinnings and practical applications in capturing curvilinear trends. Next, we will navigate through the Fractional Polynomials, a more flexible extension of traditional polynomials, adept at modelling asymmetric patterns. Lastly, we will explore Spline Models, one of the most flexible approach in data fitting, capable of adapting to complex and segmented patterns in data.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
This post is designed not just to inform but to provide a technical understanding of these models with examples, illustrating their relevance and application in contemporary data analysis. Whether you are a data scientist, a statistician, or a researcher grappling with complex data sets, this exploration aims to enhance the modelling arsenal, offering insights into when and how to apply these models effectively.
&lt;/p&gt;
&lt;div id=&#34;polynomial-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomial Models&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
Polynomial Models, represented by functions of the form &lt;span class=&#34;math display&#34;&gt;\[y = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0,\]&lt;/span&gt; are foundational in modelling curvilinear relationships. In this formulation, each &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(i = 0, 1, \ldots, n\)&lt;/span&gt;) denotes the coefficient corresponding to the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th term of the polynomial, and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the independent or exploratory variable. The degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of the polynomial determines the model’s complexity, with higher degrees allowing for more intricate curve shapes.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
These models are particularly useful in capturing the non-linear dynamics often observed in real-world data. For instance, a quadratic model (where &lt;span class=&#34;math inline&#34;&gt;\(n = 2\)&lt;/span&gt;) can describe simple parabolic trends, while higher-degree models, such as cubic (&lt;span class=&#34;math inline&#34;&gt;\(n = 3\)&lt;/span&gt;) or quartic (&lt;span class=&#34;math inline&#34;&gt;\(n = 4\)&lt;/span&gt;), enable the representation of more complex and varied behaviours (Figure 1).
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;polynomial.png&#34; alt=&#34;Comparative Visualization of Polynomial Fits: Degrees 1 to 4&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Comparative Visualization of Polynomial Fits: Degrees 1 to 4
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
However, increasing the degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; also increases the risk of overfitting, a phenomenon where the model adapts too closely to the specificities of the training data, including noise, at the expense of generalizability to new data (Figure 2). Overfitting leads to models that perform poorly in predictive scenarios, failing to capture the true underlying trend.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;poly_overfit.png&#34; alt=&#34;Visualizing Model Complexity: Parsimonious vs Overfit Polynomial Fits&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Visualizing Model Complexity: Parsimonious vs Overfit Polynomial Fits
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
Polynomial models are extensively applied across various disciplines. In physics, they are instrumental in modelling motion under uniform acceleration, among other phenomena. In economics, polynomial trends are fitted to time series data to understand market dynamics. In biological sciences, these models aid in interpreting growth rates and simple gene expression patterns. The interpretation of the coefficients &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; can provide significant insights; for instance, in the quadratic model &lt;span class=&#34;math inline&#34;&gt;\(y = ax^2 + bx + c\)&lt;/span&gt;, the sign of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; determines the direction in which the parabola opens, offering crucial information about the nature of the relationship being modelled (Figure 3).
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;parabola.png&#34; alt=&#34;Parabola Opening Upwards and Downwards&#34; width=&#34;500px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Parabola Opening Upwards and Downwards
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
In practical applications, the selection of the polynomial degree &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is critical. It is a balance between capturing the complexity of the data and avoiding overfitting (Figure 2). Techniques such as cross-validation, where the data is divided into training and testing sets, can be used to determine the optimal degree of the polynomial. Additionally, statistical measures like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) are often employed to select the most appropriate model by balancing model fit and complexity.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In summary, Polynomial Models are a versatile and powerful tool in statistical modelling. Their ability to approximate complex functions with a relatively straightforward mathematical formulation makes them a fundamental component in various fields of data analysis.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
While traditional polynomial models are highly effective, they sometimes lack the flexibility required for certain types of data, particularly those exhibiting asymmetric trends. This limitation led to the development of fractional polynomial models, which extend the concept of polynomial models by allowing for fractional exponents. This advancement provides a greater ability to fit a wider range of curves and is especially useful in cases where the relationship between variables is not adequately captured by integer exponents.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fractional-polynomial-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fractional Polynomial Models&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
Fractional Polynomials represent an advanced evolution of traditional polynomial models, marked by their use of non-integer, real-number exponents in the independent variable. Mathematically, they are expressed as &lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x^{p_1} + \beta_2 x^{p_2} + \ldots + \beta_n x^{p_n},\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the independent variable, &lt;span class=&#34;math inline&#34;&gt;\(\beta_i\)&lt;/span&gt; are coefficients, and &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; are the variable powers. These powers, unlike the integer-only powers in traditional polynomials, can include any real number like &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;, or &lt;span class=&#34;math inline&#34;&gt;\(2.3\)&lt;/span&gt;. This flexibility significantly broadens the modeling capability of polynomials, allowing for more precise fitting to complex and asymmetric data patterns. Terms such as &lt;span class=&#34;math inline&#34;&gt;\(x^{-1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x^{0.5}\)&lt;/span&gt;, representing the reciprocal and square root of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; respectively, enable the modeling of relationships that exhibit dramatic changes over different ranges of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, a task challenging for standard polynomial models with integer exponents.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;frac_poly.png&#34; alt=&#34;Comparative Visualization of Fractional Polynomial Fits&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Comparative Visualization of Fractional Polynomial Fits
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
The utility of fractional polynomials extends to various fields, notably in medical statistics and biological data analysis, where data patterns often defy symmetry. They are particularly adept in modeling phenomena like dose-response curves in pharmacokinetics and progression rates of diseases, where the response changes in a non-linear fashion. Such flexibility makes them invaluable in scenarios where data exhibit complex, non-standard behaviors.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
However, the complexity of fractional polynomials can pose interpretational challenges, and like their traditional counterparts, they are susceptible to overfitting. This risk is heightened with the inclusion of multiple fractional terms or higher degrees, necessitating careful model selection and validation processes. Methods such as cross-validation or the use of information criteria like AIC or BIC are often employed to balance model fit against the risk of overfitting.
&lt;/p&gt;
&lt;div id=&#34;finding-optimal-power-values-in-fractional-polynomials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finding Optimal Power Values in Fractional Polynomials&lt;/h2&gt;
&lt;p align=&#34;justify&#34;&gt;
The process of finding the best values for &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \ldots, p_n\)&lt;/span&gt; is inherently iterative and may require a combination of statistical testing, validation techniques, and expert judgement. The goal is to have a balance between a model that fits the data well, is not overly complex, and is robust to variations in model parameters.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In Figure 5, I exemplify this process by fitting various fractional polynomial models to the dataset. Through the application of the Bayesian Information Criterion (BIC), I identified the most parsimonious model, which is succinctly expressed mathematically as &lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1 x^{1.13},\]&lt;/span&gt; and the corresponding BIC value for this model is denoted as &lt;span class=&#34;math inline&#34;&gt;\(−144.34\)&lt;/span&gt;.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;fractional_polynomial_fit_with_bic.gif&#34; alt=&#34;Comparative Visualization of Fractional Polynomial Fits&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Comparative Visualization of Fractional Polynomial Fits
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
Below I have described some possible approaches to achieve this objective:
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Initial Power Selection&lt;/strong&gt;: Start with a set of candidate powers, often including a mix of positive, negative, and fractional values. Common choices are &lt;span class=&#34;math inline&#34;&gt;\(-2, -1, -0.5, 0, 0.5, 1, 2, 3\)&lt;/span&gt;. The selection of these initial powers is guided by prior knowledge about the data, theoretical considerations, or exploratory analysis.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Model Fitting and Comparison&lt;/strong&gt;: Fit fractional polynomial models using different combinations of these candidate powers. This fitting can be done using least squares regression or other suitable methods depending on the nature of the data. For each model, compute a goodness-of-fit statistic, such as the residual sum of squares (RSS) or the Akaike Information Criterion (AIC).
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Iterative Testing&lt;/strong&gt;: Employ an iterative approach to test various combinations of powers. This might involve starting with a simple model and gradually adding complexity (increasing the number of terms) while monitoring the improvement in the fit.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Cross-Validation&lt;/strong&gt;: To guard against overfitting, especially in models with higher degrees or more terms, use cross-validation. Divide your data into training and testing sets. Fit the model to the training set and evaluate its performance on the testing set. This step helps in assessing the model’s predictive accuracy.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Statistical Significance&lt;/strong&gt;: Assess the statistical significance of the coefficients associated with each term in the model. Non-significant terms might suggest that certain powers do not contribute meaningfully to the model and could be excluded.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Model Selection Criteria&lt;/strong&gt;: Use model selection criteria like AIC or BIC to compare models with different combinations of powers. These criteria balance the goodness of fit with the complexity of the model, helping to choose a model that is both accurate and parsimonious.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Sensitivity Analysis&lt;/strong&gt;: Conduct sensitivity analyses by varying the powers slightly to see how robust the model is to changes in these parameters. This step is crucial to understand the stability and reliability of the model.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
While fractional polynomials offer enhanced modelling flexibility over traditional polynomials, they sometimes fall short in handling data with distinct behavioural changes across different segments. This limitation summed to the difficulty in determine &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; are where spline models come into play. Spline models, constructed as piecewise polynomials, provide localized fitting capabilities, adapting seamlessly to variations within different data segments. Such adaptability is particularly useful in datasets with distinct phases or regimes. Thus, spline models emerge as a natural progression when fractional polynomials alone are insufficient to model the intricate patterns present in the data
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;spline-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Spline Models&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
&lt;p&gt;Carrasco, J. L., King, T. S., &amp;amp; Chinchilli, V. M. (2009). The Concordance Correlation Coefficient for Repeated Measures Estimated by Variance Components. &lt;em&gt;Journal of Biopharmaceutical Statistics, 19&lt;/em&gt;, 90-105. DOI: 10.1080/10543400802527890&lt;/p&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;citation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Citation&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For attribution, please cite this work as:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;div-1&#34;&gt;
&lt;p&gt;Oliveira T.P. (2024, Jan. 25). Exploring the Technicalities of Data Fitting - Polynomial, Fractional Polynomial, and Spline Models&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;BibTeX citation&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;@misc{oliveira2024polynomials,
  author = {Oliveira, Thiago},
  title = {Exploring the Technicalities of Data Fitting - Polynomial, Fractional Polynomial, and Spline Models},
  url = {https://prof-thiagooliveira.netlify.app/post/exploring-the-technicalities-of-data-fitting-polynomial-fractional-polynomial-and-spline-models/},
  year = {2024}
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Precision &amp; Accuracy - The Role of Concordance Correlation in Research</title>
      <link>https://prof-thiagooliveira.netlify.com/post/statistics/</link>
      <pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://prof-thiagooliveira.netlify.com/post/statistics/</guid>
      <description>


&lt;p align=&#34;justify&#34;&gt;
The Concordance Correlation Coefficient (CCC) is a statistical measure designed to evaluate the agreement between two sets of measurements, such as those represented by two random variables, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Mathematically, the CCC is defined as:
&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho_c = \frac{2\sigma_{x,y}}{\sigma_x^2 + \sigma_y^2 + (\mu_x - \mu_y)^2} = \frac{2\rho\sigma_x\sigma_y}{\sigma_x^2 + \sigma_y^2 + (\mu_x - \mu_y)^2} = \rho \times C_b.\]&lt;/span&gt;&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In this formula, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_y\)&lt;/span&gt; are the standard deviations of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, respectively, showing the variability within each set of measurements. The means, &lt;span class=&#34;math inline&#34;&gt;\(\mu_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_y\)&lt;/span&gt;, represent the central tendency of each dataset. The term &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is Pearson’s correlation coefficient, expressing the linear association between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(C_b\)&lt;/span&gt;, the bias correction factor, quantifies the deviation of the best-fit line from the 45-degree line through the origin—the line of perfect agreement. A &lt;span class=&#34;math inline&#34;&gt;\(C_b\)&lt;/span&gt; of 1 indicates no bias, and values close to 1 signify an accurate agreement.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Understanding the concepts of precision and accuracy is crucial for grasping the essence of CCC (Concordance Correlation Coefficient). Let’s break down these components:
&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Precision&lt;/strong&gt;: This aspect refers to the consistency or repeatability of measurements (Figure 1). Imagine a target with a bullseye:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p align=&#34;justify&#34;&gt;
If you shoot a series of arrows, and they all land very close to each other but not necessarily near the bullseye, this demonstrates high precision. The arrows are consistently hitting the same spot, showing that your measurements (in this case, arrow shots) are repeatable and reliable.
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p align=&#34;justify&#34;&gt;
However, high precision does not guarantee that you are hitting the target accurately. Your consistent shots might be clustered in a corner of the target, far from the bullseye.
&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p align=&#34;justify&#34;&gt;
&lt;strong&gt;Accuracy&lt;/strong&gt;: This term refers to how close the measurements are to the ‘true’ or accepted value (Figure 1). Continuing with the target analogy:
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p align=&#34;justify&#34;&gt;
If your arrows hit or are very close to the bullseye, this indicates high accuracy. You are hitting the correct or intended spot.
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p align=&#34;justify&#34;&gt;
It’s possible to be accurate without being precise if your arrows are scattered all around the bullseye, each hitting close but not in a consistent pattern.
&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;dartboard_precision_accuracy.gif&#34; alt=&#34;Example of precision and accuracy concept. The red dot in the center represents the bullseye.&#34; width=&#34;500px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Example of precision and accuracy concept. The red dot in the center represents the bullseye.
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
Now, applying these concepts to CCC. The &lt;strong&gt;CCC measures both the precision and accuracy of a set of measurements&lt;/strong&gt;. It assesses the strength of the relationship between two variables (precision) and how closely these measurements agree with the ‘true’ or accepted values (accuracy). A high CCC indicates that not only are the measurements consistent with each other (precision), but they also closely match the true values (accuracy). In our analogy, this would be like consistently hitting the bullseye with every arrow.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Furthermore, in the context of CCC, the concept of a “true” value is central. One variable is considered the standard or “true” measurement against which the other is compared. This distinction is vital because CCC is not merely a correlation but a measure of agreement. The notion of a “true” value in scientific research is intricate, often defined by theoretical constructs or consensus standards, and the assumption of truth can be challenged or refined with advancing knowledge and technology. Thus, CCC offers a way to quantitatively assess how well our measurements reflect what we accept as true, acknowledging that our understanding of “true” can evolve.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
To illustrate, in Scenario 1, even with a Pearson correlation near 1, the CCC’s modest value signals a discrepancy from the true value, reflecting either a systematic bias or scale differences between the measurements. The dashed red line represents the line of perfect agreement, where the true values would ideally lie. This scenario underscores the necessity of considering both precision and accuracy—where precision alone can mislead, and accuracy is pivotal for measurements to be meaningful and trustworthy.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;scenario1.png&#34; alt=&#34;Scenario 1 - High Pearson correlation with modest CCC indicating possible systematic bias or scale differences. The dashed line represents perfect agreement&#34; width=&#34;500px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Scenario 1 - High Pearson correlation with modest CCC indicating possible systematic bias or scale differences. The dashed line represents perfect agreement
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
Scenario 2 presents a contrasting picture with a CCC value of 0.95, indicating not only a strong linear relationship but also a high degree of concordance. In this instance, the measurements not only follow a consistent pattern (precision) but also align closely with the line of perfect agreement (accuracy), suggesting that one set of measurements can be reliably used as a surrogate for the true values.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;scenario2.png&#34; alt=&#34;Scenario 2 - Strong linear relationship and high concordance with CCC at 0.95. The dashed line indicates the line of perfect agreement&#34; width=&#34;500px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Scenario 2 - Strong linear relationship and high concordance with CCC at 0.95. The dashed line indicates the line of perfect agreement
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
In Scenario 3, we explore a scenario characterized by a negative Pearson correlation coefficient of approximately -0.44 and a Concordance Correlation Coefficient (CCC) of -0.43. This specific case demands a tailored approach to understanding concordance within inverse relationships. Instead of using the &lt;span class=&#34;math inline&#34;&gt;\(Y = X\)&lt;/span&gt; line commonly associated with positive correlations, we introduce a new reference line represented by &lt;span class=&#34;math inline&#34;&gt;\(Y = 100 - X\)&lt;/span&gt; (depicted as the black dashed line in the plot). This reference line possesses a slope of -1 and serves as the benchmark for perfect inverse agreement.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;scenario3.png&#34; alt=&#34;Scenario 3: Inverse relationship with both Pearson correlation and CCC around -0.44. The red dashed line represents $Y = X$, the black dashed line is the $Y = 100 - X$, and the blue solid line is the best fit line&#34; width=&#34;500px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Scenario 3: Inverse relationship with both Pearson correlation and CCC around -0.44. The red dashed line represents &lt;span class=&#34;math inline&#34;&gt;\(Y = X\)&lt;/span&gt;, the black dashed line is the &lt;span class=&#34;math inline&#34;&gt;\(Y = 100 - X\)&lt;/span&gt;, and the blue solid line is the best fit line
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
The noteworthy aspect of this scenario lies in the high &lt;span class=&#34;math inline&#34;&gt;\(C_b\)&lt;/span&gt; value, which is approximately 0.98 (&lt;span class=&#34;math inline&#34;&gt;\(C_b = \frac{\rho_c}{\rho} = \frac{-0.43}{-0.44}\)&lt;/span&gt;). This value is calculated as the ratio of CCC to the Pearson correlation coefficient. While the negative Pearson correlation may not indicate a high precision, the elevated &lt;span class=&#34;math inline&#34;&gt;\(C_b\)&lt;/span&gt; value implies that &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are almost perfectly accurate.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Now, let’s take a closer look at an intriguing scenario. If we push the Pearson correlation to the extreme, reaching -1, we witness a perfect inverse agreement. In this scenario, every data point converges precisely onto the black dashed line, forming an impeccable alignment. Notably, even the best fit line mirrors this black dashed line in perfect harmony, showcasing the unparalleled concordance in this exceptional case.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;scenario4_gif.gif&#34; alt=&#34;Scenario 4: Inverse relationship with both Pearson correlation varying from -0.43 to -1. The red dashed line represents $Y = X$, the black dashed line is the $Y = 100 - X$, and the blue solid line is the best fit line&#34; width=&#34;600px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Scenario 4: Inverse relationship with both Pearson correlation varying from -0.43 to -1. The red dashed line represents &lt;span class=&#34;math inline&#34;&gt;\(Y = X\)&lt;/span&gt;, the black dashed line is the &lt;span class=&#34;math inline&#34;&gt;\(Y = 100 - X\)&lt;/span&gt;, and the blue solid line is the best fit line
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
In Scenario 5, the CCC value of 0 denotes absolute non-concordance, indicating that the variability between the measurements does not correspond to the variability expected by chance alone. This suggests that the measurements lack any systematic agreement and cannot be used interchangeably or as reliable estimates of one another. The complete absence of concordance highlights the importance of accuracy in measurement, underscoring that precision without accuracy does not yield valid or useful data in reflecting the assumed “true” values.
&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;scenario4.png&#34; alt=&#34;Scenario 5 - No agreement with CCC at 0&#34; width=&#34;500px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Scenario 5 - No agreement with CCC at 0
&lt;/p&gt;
&lt;/div&gt;
&lt;p align=&#34;justify&#34;&gt;
These scenarios vividly demonstrate the nuanced interplay between precision and accuracy within the framework of CCC. They bring to light the importance of accuracy in scientific measurement and the limitations of relying solely on correlation coefficients for assessing the validity and reliability of data.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
To further strengthen the discussion, we must consider the assumptions and limitations of CCC. It assumes that the data scales are continuous and that the relationship between the measures is linear. The presence of outliers can unduly influence the CCC, and it may not be suitable for all data types. Additionally, the CCC does not account for random error, which can affect measurements variably.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
When considering alternative measures, such as Bland-Altman plots, we can address scenarios where CCC is less suitable, like non-normal data or comparisons of more than two measurement sets.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
In practical terms, CCC can be used to validate new measurement methods against established gold standards, underscoring its application in method comparison studies. Real-world examples where CCC has been pivotal could include its use in clinical settings for comparing measurement techniques, methods of colour measurements in agriculture, and model diagnostic in statistics.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For computational purposes, the statistical environment &lt;code&gt;R&lt;/code&gt; has functions and libraries dedicated to calculating CCC, making it accessible for researchers and practitioners to apply this measure to their data. The &lt;code&gt;epiR&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;, for instance, provides a function &lt;code&gt;epi.ccc&lt;/code&gt; specifically for calculating Lin’s CCC.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
If one prefers to write a custom function in &lt;code&gt;R&lt;/code&gt; for educational or analytical purposes, the following function can be used to compute a point estimate for Lin’s Concordance Correlation Coefficient (CCC):
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#&amp;#39; Calculate Lin&amp;#39;s Concordance Correlation Coefficient (CCC)
#&amp;#39;
#&amp;#39; This function computes Lin&amp;#39;s Concordance Correlation Coefficient to evaluate the agreement
#&amp;#39; between two sets of measurements. It returns the CCC which combines measures of precision
#&amp;#39; and accuracy to determine how well the data from the two sets conform to the line of
#&amp;#39; perfect concordance.
#&amp;#39;
#&amp;#39; @param x A numeric vector of measurements.
#&amp;#39; @param y A numeric vector of measurements, where each element corresponds to the element in `x`.
#&amp;#39;
#&amp;#39; @return The Concordance Correlation Coefficient as a numeric value.
#&amp;#39;
#&amp;#39; @examples
#&amp;#39; x &amp;lt;- c(1, 2, 3, 4, 5)
#&amp;#39; y &amp;lt;- c(1.1, 1.9, 3.1, 4.2, 4.8)
#&amp;#39; ccc(x, y)
#&amp;#39;
#&amp;#39; @export
ccc &amp;lt;- function(x, y) {
  # Check if inputs are numeric vectors
  stopifnot(is.numeric(x), is.numeric(y))

  # Calculate Pearson&amp;#39;s correlation coefficient
  rho &amp;lt;- cor(x, y)

  # Calculate means of x and y
  mean_x &amp;lt;- mean(x)
  mean_y &amp;lt;- mean(y)

  # Calculate variances of x and y
  var_x &amp;lt;- var(x)
  var_y &amp;lt;- var(y)

  # Calculate CCC based on the formula
  ccc_value &amp;lt;- (2 * rho * sqrt(var_x) * sqrt(var_y)) / 
               (var_x + var_y + (mean_x - mean_y)^2)

  return(ccc_value)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p align=&#34;justify&#34;&gt;
This function begins by calculating Pearson’s correlation coefficient (&lt;span class=&#34;math inline&#34;&gt;\(rho\)&lt;/span&gt;) for the input vectors &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. It then computes the means (&lt;span class=&#34;math inline&#34;&gt;\(mean_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(mean_y\)&lt;/span&gt;) and variances (&lt;span class=&#34;math inline&#34;&gt;\(var_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(var_y\)&lt;/span&gt;) of the two sets of measurements. The CCC is calculated by combining these values according to the formula, thereby quantifying the agreement between the two measurements in terms of both precision and accuracy.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Using the custom &lt;code&gt;ccc&lt;/code&gt; function provides a straightforward computation of Lin’s Concordance Correlation Coefficient (CCC) for research applications, particularly when a simple estimate of concordance is required. However, for a more comprehensive analysis, the &lt;code&gt;epi.ccc&lt;/code&gt; function from the &lt;a href=&#34;https://cran.r-project.org/web/packages/epiR/index.html&#34;&gt;&lt;code&gt;epiR&lt;/code&gt; package&lt;/a&gt; is advantageous as it not only computes CCC but also provides confidence intervals for the CCC value, offering a statistical range within which the true concordance lies with a certain probability. This is crucial for making inferences about the precision of the agreement between measurements in research.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For those looking to apply CCC specifically to Lin’s method, both the &lt;a href=&#34;https://cran.r-project.org/web/packages/epiR/index.html&#34;&gt;&lt;code&gt;epiR&lt;/code&gt; package&lt;/a&gt; and the &lt;a href=&#34;https://cran.r-project.org/web/packages/DescTools/index.html&#34;&gt;&lt;code&gt;DescTools&lt;/code&gt; package&lt;/a&gt; on CRAN offer robust tools. The &lt;code&gt;DescTools&lt;/code&gt; package provides a comprehensive collection of statistical functions, including methods for calculating CCC.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
For studies involving repeated measures, where the same subjects are measured under different conditions or at different times, the &lt;a href=&#34;https://cloud.r-project.org/web/packages/lcc/index.html&#34;&gt;&lt;code&gt;lcc&lt;/code&gt; package&lt;/a&gt; offers functions for calculating CCC for longitudinal data, taking into account the within-subject correlation.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Additionally, the &lt;a href=&#34;https://cran.r-project.org/web/packages/cccrm/index.html&#34;&gt;&lt;code&gt;cccrm&lt;/code&gt; package&lt;/a&gt; provides functions for calculating the CCC for repeated (and non-repeated) measures, catering to a wide range of research designs and ensuring that the variability inherent in repeated measures is appropriately accounted for.
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
These packages are valuable additions to the toolkit of researchers, statisticians, and data analysts, enhancing the reliability and interpretability of concordance assessments in scientific studies.
&lt;/p&gt;
&lt;div id=&#34;r-code-used-to-produce-the-data-and-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R code used to produce the data and plots&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;# Load necessary libraries
library(MASS)
library(ggplot2)

# Function to calculate Concordance Correlation Coefficient (CCC)
ccc &amp;lt;- function(x, y) {
  # Check if inputs are numeric vectors
  stopifnot(is.numeric(x), is.numeric(y))

  # Calculate Pearson&amp;#39;s correlation coefficient
  rho &amp;lt;- cor(x, y)

  # Calculate means of x and y
  mean_x &amp;lt;- mean(x)
  mean_y &amp;lt;- mean(y)

  # Calculate variances of x and y
  var_x &amp;lt;- var(x)
  var_y &amp;lt;- var(y)

  # Calculate CCC based on the formula
  ccc_value &amp;lt;- (2 * rho * sqrt(var_x) * sqrt(var_y)) / 
               (var_x + var_y + (mean_x - mean_y)^2)

  return(ccc_value)
}

# Set seed for reproducibility
set.seed(123)

# Function to generate and plot data for each scenario
data_plot &amp;lt;- function(mean, cov, scenario_number) {
  data &amp;lt;- mvrnorm(100, mu = mean, Sigma = cov)
  df &amp;lt;- data.frame(x = data[,1], y = data[,2])
  
  title &amp;lt;- ifelse(scenario_number == 3 || scenario_number == 4,
                  paste(&amp;quot;Scenario&amp;quot;, scenario_number, &amp;quot;: Pearson =&amp;quot;, round(cor(df$x, df$y), 2), 
                        &amp;quot;CCC =&amp;quot;, round(ccc(df$x, df$y), 2)),
                  paste(&amp;quot;Scenario&amp;quot;, scenario_number, &amp;quot;: CCC =&amp;quot;, round(ccc(df$x, df$y), 2)))
  
  p &amp;lt;- ggplot(df, aes(x = x, y = y)) + 
    geom_point(color = &amp;quot;blue&amp;quot;, alpha = 0.6, size = 3) +
    ggtitle(title) +
    xlab(&amp;quot;X values&amp;quot;) +
    ylab(&amp;quot;Y values&amp;quot;) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, face = &amp;quot;bold&amp;quot;, size = 14),
          axis.title.x = element_text(face = &amp;quot;bold&amp;quot;, size = 12),
          axis.title.y = element_text(face = &amp;quot;bold&amp;quot;, size = 12),
          axis.text.x = element_text(size = 10),
          axis.text.y = element_text(size = 10))
  
  if (scenario_number == 3 || scenario_number == 4) {
    p &amp;lt;- p + geom_smooth(method = &amp;quot;lm&amp;quot;, color = &amp;quot;blue&amp;quot;, se = FALSE) +
      geom_abline(slope = -1, intercept = 100, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;black&amp;quot;) +
      geom_abline(slope = 1, intercept = 0, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;)
  } else {
    p &amp;lt;- p + geom_abline(slope = 1, intercept = 0, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;)
  }
  
  print(p)
}

# Scenario 1: High Pearson correlation, modest CCC
data_plot(c(50, 70), matrix(c(100, 0.99 * sqrt(100) * sqrt(150), 0.99 * sqrt(100) * sqrt(150), 150), 2), 1)

# Scenario 2: High CCC
data_plot(c(50, 50), matrix(c(100, 0.95 * sqrt(100) * sqrt(100), 0.95 * sqrt(100) * sqrt(100), 100), 2), 2)

# Scenario 3: Negative Pearson and CCC
data_plot(c(50, 50), matrix(c(100, -0.3 * sqrt(100) * sqrt(150), -0.3 * sqrt(100) * sqrt(150), 150), 2), 3)

# Scenario 4 (previously 5): Perfect inverse agreement
data_plot(c(50, 50), matrix(c(100, -1 * sqrt(100) * sqrt(100), -1 * sqrt(100) * sqrt(100), 100), 2), 4)

# Scenario 5 (previously 4): No correlation (CCC = 0)
data_plot(c(50, 50), matrix(c(100, 0, 0, 150), 2), 5)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p align=&#34;justify&#34;&gt;
Carrasco, J. L., King, T. S., &amp;amp; Chinchilli, V. M. (2009). The Concordance Correlation Coefficient for Repeated Measures Estimated by Variance Components. &lt;em&gt;Journal of Biopharmaceutical Statistics, 19&lt;/em&gt;, 90-105. DOI: 10.1080/10543400802527890
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Oliveira, T. de P., Hinde, J., &amp;amp; Zocchi, S. S. (2018). Longitudinal Concordance Correlation Function Based on Variance Components: An Application in Fruit Color Analysis. &lt;em&gt;Journal of Agricultural, Biological, and Environmental Statistics, 23&lt;/em&gt;(2), 233-254. DOI: 10.1007/s13253-018-0321-1
&lt;/p&gt;
&lt;p align=&#34;justify&#34;&gt;
Rathnayake, L. N., &amp;amp; Choudhary, P. K. (2017). Semiparametric Modeling and Analysis of Longitudinal Method Comparison Data. &lt;em&gt;Statistics in Medicine, 36&lt;/em&gt;, 2003-2015. DOI: 10.1002/sim.7261
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;citation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Citation&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For attribution, please cite this work as:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;div-1&#34;&gt;
&lt;p&gt;Oliveira T.P. (2024, Jan. 10). Precision &amp;amp; Accuracy - The Role of Concordance Correlation in Research&lt;/p&gt;
&lt;/div&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;BibTeX citation&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;@misc{oliveira2024concordance,
  author = {Oliveira, Thiago},
  title = {Precision &amp;amp; Accuracy - The Role of Concordance Correlation in Research},
  url = {https://prof-thiagooliveira.netlify.app/post/precision-and-accuracy-the-role-of-concordance-correlation-in-research/},
  year = {2024}
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
