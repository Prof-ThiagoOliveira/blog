<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Benchmarking | Thiago Oliveira</title>
    <link>http://localhost:4321/tags/benchmarking/</link>
      <atom:link href="http://localhost:4321/tags/benchmarking/index.xml" rel="self" type="application/rss+xml" />
    <description>Benchmarking</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 06 Jul 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/media/logo_hu79b57d1cb18e719812a49f2a6d8a26be_34849_300x300_fit_lanczos_3.png</url>
      <title>Benchmarking</title>
      <link>http://localhost:4321/tags/benchmarking/</link>
    </image>
    
    <item>
      <title>Comparing data read and write performance in R</title>
      <link>http://localhost:4321/post/data-read-write-performance/</link>
      <pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/data-read-write-performance/</guid>
      <description>&lt;style&gt;
/* Blog post container */
body {
   font-family: &#39;Helvetica Neue&#39;, Arial, sans-serif;
   font-size: 1rem;
   line-height: 1.8;
   color: #333;
   text-align: justify;
   background-color: #fafafa;
   margin: 0;
   padding: 0 20px;
}

/* Header styling */
h1, 
h2, 
h3, 
h4, 
h5, 
h6 {
  font-weight: 600; /* Semi-bold for a professional look */
  margin-bottom: 0.75em; /* Slightly reduced bottom margin */
  color: #0d0d0d;
  line-height: 1.2;
  margin-top: 1.5em; /* Added top margin for consistency */
}

h1 {
  font-size: 1.75rem; 
  border-bottom: 2px solid #3b80d1;
  padding-bottom: 0.3em; /* Padding for visual separation */
  margin-top: 1em; 
}

h2 {
  font-size: 1.5rem; 
  color: #3b80d1;
  padding-bottom: 0.2em; /* Padding for visual separation */
}

h3 {
  font-size: 1.25rem; 
  color: #333;
}

h4 {
  font-size: 1.125rem; 
  color: #333;
}

h5 {
  font-size: 1rem; 
  color: #333;
}

h6 {
  font-size: 0.875rem; 
  color: #333;
}

/* Link styling */
a {
  color: #3b80d0;
  text-decoration: none;
  transition: color 0.3s ease;
}

a:hover {
  text-decoration: underline;
  color: #1a57a0;
}

/* Code styling */
pre, 
.code-input {
  background-color: #f5f5f5;
  border: 1px solid #ddd;
  padding: 10px;
  font-size: 0.9rem;
  border-radius: 5px;
  margin: 20px 0;
  overflow-x: auto;
}

code {
  font-size: 0.9rem;
  background-color: #f5f5f5;
  padding: 2px 4px;
  border-radius: 3px;
}

/* Table styling */
table {
  width: 100%;
  border-collapse: collapse;
  margin-bottom: 1.5em;
  text-align: left;
}

th, 
td {
  padding: 12px;
  border: 1px solid #ddd;
}

th {
  background-color: #3b80d1;
  color: white;
}

/* Div options - color box text */
.div-1 {
  color: black;
  background-color: #d6edd3;
  padding: 10px;
  border-radius: 5px;
  margin-bottom: 1.5em;
}

.div-2 {
  color: black;
  background-color: #cfbe7e;
  padding: 10px;
  border-radius: 5px;
  margin-bottom: 1.5em;
}

/* Article content */
.article-content {
  text-align: justify;
}

/* Image styling */
img {
  max-width: 100%;
  height: auto;
  border-radius: 5px;
  margin-bottom: 1.5em;
}
&lt;/style&gt;
&lt;h1 id=&#34;post-summary&#34;&gt;Post Summary&lt;/h1&gt;
&lt;p&gt;Efficient data handling is crucial for daily data analysis tasks. In this post, we will compare the performance of various data formats in &lt;code&gt;R&lt;/code&gt; for reading and writing operations. The formats considered include RDS, CSV (using &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;readr&lt;/code&gt;), FST, Feather, Parquet, and QS. We will benchmark the speed, file sizes, and memory usage of each format across different data sizes.&lt;/p&gt;
&lt;h1 id=&#34;database-definition&#34;&gt;Database Definition&lt;/h1&gt;
&lt;p&gt;In this section, we will describe each of the data formats in detail, highlighting their strengths and weaknesses. We will provide insights into the specific use cases where each format excels. The choice of format will depend on the specific needs of the data analysis tasks.&lt;/p&gt;
&lt;h2 id=&#34;rds&#34;&gt;RDS&lt;/h2&gt;
&lt;p&gt;RDS is a format native to &lt;code&gt;R&lt;/code&gt; that is highly efficient for saving single R objects. The &lt;code&gt;saveRDS()&lt;/code&gt; and &lt;code&gt;readRDS()&lt;/code&gt; functions are used to write and read &lt;code&gt;R&lt;/code&gt; objects, respectively. RDS seamlessly integrates with &lt;code&gt;R&lt;/code&gt;, retaining object metadata including data types and structures, which is beneficial for preserving the exact state of an &lt;code&gt;R&lt;/code&gt; object. It provides fast read and write operations for moderate data sizes, making it suitable for storing intermediate results and final outputs in &lt;code&gt;R&lt;/code&gt;. Additionally, it is easy to use with built-in &lt;code&gt;R&lt;/code&gt; functions. However, it can result in larger file sizes compared to some compressed formats due to lack of advanced compression. Moreover, RDS has limited compatibility with other software or programming languages, limiting its use for data sharing outside of &lt;code&gt;R&lt;/code&gt; environments.&lt;/p&gt;
&lt;h2 id=&#34;qs&#34;&gt;QS&lt;/h2&gt;
&lt;p&gt;QS (Quick Serialization of &lt;code&gt;R&lt;/code&gt; objects) is a binary format for &lt;code&gt;R&lt;/code&gt; objects that emphasizes speed and compression. It uses advanced compression algorithms to achieve high performance. QS provides extremely fast read and write operations, even for large data sets, making it suitable for high-performance computing tasks. It uses advanced compression techniques to reduce file size significantly without compromising on speed. Additionally, QS preserves &lt;code&gt;R&lt;/code&gt; object metadata, ensuring that data types and structures are maintained. However, QS is mainly designed for use within &lt;code&gt;R&lt;/code&gt;, with limited support in other programming environments. It also requires familiarity with additional functions and parameters to fully leverage its capabilities, which might be a barrier for new users. The goal of this package is to provide a lightning-fast and complete replacement for the &lt;code&gt;saveRDS&lt;/code&gt; and &lt;code&gt;readRDS&lt;/code&gt; functions in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;csv-using-datatable-and-readr&#34;&gt;CSV (using &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;readr&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;CSV (Comma-Separated Values) is a plain text format widely used for data storage and transfer. &lt;code&gt;R&lt;/code&gt; offers multiple ways to handle CSV files, with &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;readr&lt;/code&gt; being popular for their speed and efficiency. The universality of CSV makes it readable across different software and programming languages. Its simple, plain text format can be easily inspected and edited manually, facilitating easy debugging and quick checks. Moreover, &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;readr&lt;/code&gt; provide efficient functions for reading and writing CSV files in &lt;code&gt;R&lt;/code&gt;, enhancing performance over base &lt;code&gt;R&lt;/code&gt; functions. However, CSV does not retain data types and structure information, requiring manual handling and potential data type issues when reading back. It is generally slower for large data sets compared to binary formats due to the need to parse text. The text format also results in larger file sizes, especially for large data sets with many rows and columns.&lt;/p&gt;
&lt;h2 id=&#34;columnar-storage-formats&#34;&gt;Columnar storage formats&lt;/h2&gt;
&lt;p&gt;Columnar storage is a method of storing data tables that improves the efficiency of reading and writing operations, especially for large data sets. Instead of storing data row by row, columnar storage organizes data by columns. This allows for more efficient data compression and faster retrieval of specific columns, which is particularly beneficial for analytical queries that often access a subset of columns from a large data set.&lt;/p&gt;
&lt;h3 id=&#34;fst&#34;&gt;FST&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;fst&lt;/code&gt; package is a fast, binary data format for &lt;code&gt;R&lt;/code&gt; data frames. It uses a highly efficient columnar storage format and supports multiple compression algorithms. FST offers extremely fast read and write operations, especially for large data sets, making it ideal for scenarios requiring frequent data access and manipulation. It efficiently compresses data, reducing file size significantly without sacrificing speed. The columnar storage format is optimized for columnar data access, enhancing speed for specific column retrieval and partial data loading. While FST is primarily designed for use within &lt;code&gt;R&lt;/code&gt;, it is not limited to it. The &lt;code&gt;pyfst&lt;/code&gt; library, for example, provides an interface to the &lt;code&gt;OpenFst&lt;/code&gt; library, enabling similar fast storage and retrieval functionalities in Python. However, within &lt;code&gt;R&lt;/code&gt;, FST can be slightly more complex to use compared to native formats due to additional options and parameters for compression and storage.&lt;/p&gt;
&lt;h3 id=&#34;feather&#34;&gt;Feather&lt;/h3&gt;
&lt;p&gt;Feather is a binary columnar data format that is part of the Apache Arrow project. It provides efficient storage for data frames and supports interchangeability between different languages. Feather supports interchangeability between &lt;code&gt;R&lt;/code&gt;, Python, and other languages, making it a great choice for multi-language data workflows. It offers fast read and write speeds due to columnar storage, suitable for handling large data sets efficiently. It is also easy to use with the &lt;code&gt;arrow&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;, allowing straightforward data exchange and manipulation. However, file sizes are generally larger than highly compressed formats like FST or Parquet, which might be a concern for very large data sets. Additionally, Feather is continually evolving, which might affect long-term stability and require frequent updates.&lt;/p&gt;
&lt;h3 id=&#34;parquet&#34;&gt;Parquet&lt;/h3&gt;
&lt;p&gt;Parquet is a columnar storage file format optimized for large-scale data processing and is part of the Apache Hadoop ecosystem. Parquet is widely supported across different programming languages and big data tools, including &lt;code&gt;R&lt;/code&gt;, Python, Spark, and more, facilitating seamless data exchange. It uses highly efficient data compression and encoding schemes to reduce file sizes significantly while maintaining performance. The columnar storage format is optimized for read performance on large data sets, especially when querying specific columns, making it ideal for big data applications. However, Parquet is more complex to handle due to advanced features and options for compression, encoding, and storage management.&lt;/p&gt;
&lt;h1 id=&#34;data-simulation&#34;&gt;Data Simulation&lt;/h1&gt;
&lt;p&gt;To evaluate the performance of various data formats, we generated sample data sets of varying sizes using a custom data generation function. This function creates data frames with three columns: an &lt;code&gt;ID&lt;/code&gt; column with sequential integers, a &lt;code&gt;Value&lt;/code&gt; column with random normal values, and a &lt;code&gt;Group&lt;/code&gt; column with randomly selected letters. The sizes of the data sets range from 100 to 10 million rows, allowing us to assess the performance across different scales. The random seed is set to $123$ to ensure reproducibility of the generated data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Function to generate sample data sets
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;generate_sample_data &amp;lt;- function(n) {
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  set.seed(123)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  data.frame(
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    ID = 1:n,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Value = rnorm(n),
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Group = sample(letters, n, replace = TRUE)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  )
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Define dataset sizes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;dataset_sizes &amp;lt;- c(1e2, 1e3, 1e4, 1e5, 1e6, 1e7)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;benchmarking-read-and-write-performance&#34;&gt;Benchmarking read and write performance&lt;/h2&gt;
&lt;p&gt;The benchmarking process involves measuring the time taken to read from and write to different data formats. We used the &lt;code&gt;microbenchmark&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; to perform these measurements, ensuring that each operation is repeated 30 times to obtain reliable statistics.&lt;/p&gt;
&lt;p&gt;For writing operations, we utilized the following functions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;`saveRDS()` for RDS files&lt;/li&gt;
  &lt;li&gt;`fwrite()` from the `data.table` package for CSV files&lt;/li&gt;
  &lt;li&gt;`write_fst()` from the `fst` package for FST files&lt;/li&gt;
  &lt;li&gt;`write_feather()` from the `arrow` package for Feather files&lt;/li&gt;
  &lt;li&gt;`qsave()` from the `qs` package for QS files&lt;/li&gt;
  &lt;li&gt;`write_csv()` from the `readr` package for CSV files&lt;/li&gt;
  &lt;li&gt;`write_parquet()` from the `arrow` package for Parquet files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For reading operations, we used the following functions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;`readRDS()` for RDS files&lt;/li&gt;
  &lt;li&gt;`fread()` from the `data.table` package for CSV files&lt;/li&gt;
  &lt;li&gt;`read_fst()` from the `fst` package for FST files&lt;/li&gt;
  &lt;li&gt;`read_feather()` from the `arrow` package for Feather files&lt;/li&gt;
  &lt;li&gt;`qread()` from the `qs` package for QS files&lt;/li&gt;
  &lt;li&gt;`read_csv()` from the `readr` package for CSV files&lt;/li&gt;
  &lt;li&gt;`read_parquet()` from the `arrow` package for Parquet files&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;performance-metrics&#34;&gt;Performance metrics&lt;/h2&gt;
&lt;p&gt;The performance metrics considered in our analysis are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt; The time taken to read from and write to each data format. This metric is crucial for understanding the efficiency of each format in different scenarios.&lt;/li&gt;
  &lt;li&gt;The disk space required to store the data in each format. Smaller file sizes are generally preferable, especially when dealing with large data sets.&lt;/li&gt;
  &lt;li&gt;The amount of memory consumed during the read operations. Efficient memory usage is important for handling large data sets without running into memory constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;results-and-analysis&#34;&gt;Results and analysis&lt;/h1&gt;
&lt;p&gt;The results of the benchmarking study on various data formats in &lt;code&gt;R&lt;/code&gt; include read and write time, file size, and memory usage. The results are visualized in graphs and tables, which show the performance trends as the data set size increases.&lt;/p&gt;
&lt;h2 id=&#34;file-sizes&#34;&gt;File sizes&lt;/h2&gt;
&lt;p&gt;The first graph illustrates the file sizes for each data format across different data set sizes. Smaller file sizes are generally preferable as they require less disk space and can be more efficient for data transfer. The formats compared include RDS, CSV (using &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;readr&lt;/code&gt;), FST, Feather, Parquet, and QS.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;## &amp;lt;img src=&amp;#34;p_file_sizes.png&amp;#34; style=&amp;#34;width:650px; cursor: pointer;&amp;#34; onclick=&amp;#34;this.style.width = this.style.width === &amp;amp;#39;650px&amp;amp;#39; ? &amp;amp;#39;100%&amp;amp;#39; : &amp;amp;#39;650px&amp;amp;#39;;&amp;#34;/&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From the graph, we can observe:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;QS consistently produces the smallest file sizes across all data set sizes.&lt;/li&gt;
  &lt;li&gt;RDS and FST also perform well, producing relatively small file sizes.&lt;/li&gt;
  &lt;li&gt;CSV formats (both `data.table` and `readr`) tend to produce the largest file sizes, particularly for larger data sets.&lt;/li&gt;
  &lt;li&gt;Feather and Parquet offer a balance between file size and compatibility, but their sizes increase significantly with very large data sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recommendations&#34;&gt;Recommendations&lt;/h3&gt;
&lt;p&gt;For small to medium-sized data sets where disk space and data transfer efficiency are critical, QS and RDS formats are recommended. For larger data sets, Feather and Parquet are preferable due to their balance of file size and compatibility, especially when interoperability with other tools is necessary.&lt;/p&gt;
&lt;h2 id=&#34;write-performance&#34;&gt;Write performance&lt;/h2&gt;
&lt;p&gt;The second graph shows the write performance for each data format. Faster write times are beneficial for efficiently saving data, particularly in workflows involving frequent data saving operations.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;
&lt;img src=&#34;p_write_benchmark.png&#34; alt=&#34;Write performance by format and dataset size&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Write performance by format and dataset size&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Key observations from the graph include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;data.table (`dt`) exhibits the fastest write times for smaller data sets, maintaining good performance even as dataset size increases.&lt;/li&gt;
  &lt;li&gt;QS, Feather, and FST follow closely with good write performance across various data set sizes.&lt;/li&gt;
  &lt;li&gt;Feather is quite close to `dt` in write performance for larger data sets.&lt;/li&gt;
  &lt;li&gt;RDS is relatively fast for small data sets, but its performance degrades more than `dt`, QS, and FST with larger data sets.&lt;/li&gt;
  &lt;li&gt;CSV formats, especially `readr`, show significantly slower write times, making them less suitable for large-scale data operations. Notably, `readr` has an interesting curvature, performing worst as data size increases but improving relative to Parquet and QS from data sizes of 1,000 onward.&lt;/li&gt;
  &lt;li&gt;Parquet provides moderate write performance, balancing speed and compatibility with other data analysis tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recommendations-1&#34;&gt;Recommendations&lt;/h3&gt;
&lt;p&gt;For small to medium-sized data sets, &lt;code&gt;dt&lt;/code&gt;, QS, Feather, and FST are recommended due to their fast write times. For larger data sets, Feather is preferable for its close performance to &lt;code&gt;dt&lt;/code&gt;, while Parquet can also be considered for its balance of write speed and compatibility. &lt;code&gt;readr&lt;/code&gt; should be avoided for large-scale data operations due to its slower write times, although it shows improved performance for data sets from sizes of 1,000 onward.&lt;/p&gt;
&lt;h2 id=&#34;read-performance&#34;&gt;Read performance&lt;/h2&gt;
&lt;p&gt;The third graph displays the read performance for each data format. Fast read times are critical for efficient data analysis, especially when working with large data sets that need to be frequently loaded into memory.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;
&lt;img src=&#34;p_read_benchmark.png&#34; alt=&#34;Read performance by format and dataset size&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Read performance by format and dataset size&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Key observations from the graph include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For data sets ranging from 100 to 10,000 rows, QS performs the best, offering the fastest read times.&lt;/li&gt;
  &lt;li&gt;For data sets larger than 10,000 rows, Feather outperforms all other formats, followed by Parquet, data.table (`dt`), and FST.&lt;/li&gt;
  &lt;li&gt;RDS is only effective for very small data sets, with its performance significantly degrading for larger data sets.&lt;/li&gt;
  &lt;li&gt;readr is the worst performer overall, exhibiting the slowest read times across all dataset sizes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;recommendations-2&#34;&gt;Recommendations&lt;/h3&gt;
&lt;p&gt;For small to medium-sized data sets (up to 10,000 rows), QS is recommended due to its superior read performance. For larger data sets, Feather is the preferred format for its fastest read times, followed by Parquet, data.table (&lt;code&gt;dt&lt;/code&gt;), and FST. RDS should only be used for very small data sets, while readr should be avoided due to its consistently poor performance.&lt;/p&gt;
&lt;h2 id=&#34;memory-usage&#34;&gt;Memory usage&lt;/h2&gt;
&lt;p&gt;The final graph compares the memory usage of each data format during read operations. Efficient memory usage is important for handling large data sets without encountering memory constraints.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;
&lt;img src=&#34;p_memory_usage.png&#34; alt=&#34;Memory usage by format and dataset size&#34; width=&#34;650px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Memory usage by format and dataset size&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Key points from the graph include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Feather and Parquet perform the best in terms of memory usage, consuming less than 50 MB for data sizes of 1e+07.&lt;/li&gt;
  &lt;li&gt;QS and RDS exhibit relatively low memory usage, making them suitable for environments with limited memory resources.&lt;/li&gt;
  &lt;li&gt;FST, data.table (`dt`), and readr have higher memory usage, with readr being the highest among all formats.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;final-considerations&#34;&gt;Final Considerations&lt;/h1&gt;
&lt;p&gt;When selecting a data format in &lt;code&gt;R&lt;/code&gt;, it is crucial to consider the specific requirements of your analysis. The following points summarize the key considerations for each format based on the benchmarking results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;QS:&lt;/strong&gt; excellent overall performance with the smallest file sizes, fastest read times, and lowest memory usage for small to medium-sized data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; small file sizes, fast read times, low memory usage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best For:&lt;/strong&gt; small to medium-sized data operations and environments with limited resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RDS:&lt;/strong&gt; performs well with moderate file sizes, fast read times, and low memory usage, making it a strong general-purpose format within &lt;code&gt;R&lt;/code&gt; for small to medium-sized data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; moderate file sizes, fast read times, low memory usage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best For:&lt;/strong&gt; small to medium-sized data sets, general-purpose use with a good balance of performance and compatibility within &lt;code&gt;R&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;FST:&lt;/strong&gt; offers excellent read and write times and efficient memory usage, suitable for high-performance data operations, especially with small to moderately large data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; fast read and write times, efficient memory usage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best For:&lt;/strong&gt; high-performance data operations with small to moderately large data sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feather:&lt;/strong&gt; provides the fastest read speeds for large data sets, close write performance to &lt;code&gt;dt&lt;/code&gt; for large data, and good memory efficiency.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; fastest read speeds for large data sets, good memory efficiency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best For:&lt;/strong&gt; interoperability across different programming languages and tools, handling large data sets efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Parquet:&lt;/strong&gt; balances performance and interoperability, with efficient memory usage for very large data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; balance between performance and interoperability, low memory usage for large data sets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best For:&lt;/strong&gt; compatibility with big data tools and frameworks, handling large data sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CSV (data.table and readr):&lt;/strong&gt; offers wide compatibility with various tools and platforms, but suffers from larger file sizes, slower read and write times, and higher memory usage, making it less ideal for large-scale data sets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Strengths:&lt;/strong&gt; wide compatibility with various tools and platforms.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best For:&lt;/strong&gt; small data sets or scenarios where maximum compatibility is needed, despite slower performance and higher memory usage for large data sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These results provide a comprehensive comparison of different data formats in &lt;code&gt;R&lt;/code&gt;, highlighting their respective strengths and weaknesses in terms of file size, read and write performance, and memory usage.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
