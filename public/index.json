[{"authors":null,"categories":null,"content":"I am a highly skilled and accomplished statistician with over 10 years of experience in experimental statistics and statistical modelling. I hold a PhD in Statistics from the University of S√£o Paulo, Brazil, and during my PhD studies, I gained expertise in statistical modelling in agricultural data while working as a visiting scholar at the National University of Ireland Galway, Ireland. After completing my PhD, I worked as a lecturer at ESALQ/University of S√£o Paulo from 2017 to 2019.\rIn 2019, I joined the Insight Centre for Data Analytics in partnership with Orreco, School of Mathematics, Statistics, and Applied Mathematics, and NUI Galway as a Researcher Biostatistician. I contributed significantly to the development of statistical methods applied to athlete performance and predictive models for COVID-19, where I worked on developing statistical methods in longitudinal concordance correlation, multilevel model (hierarchical model), generalized linear mixed-effects model, state-space models, experimental design, and longitudinal data.\rI am not only an accomplished statistician but also an advocate for using dashboard apps to create interactive data visualization. I believe that apps are an efficient tool to make visual representations of large scale data sets and enable users to explore the complex reality of the database, or even handle multiple data locations in a single visualization.\rIn 2020, I was awarded a Marie Sk≈Çodowska-Curie Actions (MSCA) COFOUND Fellowship (Train@Ed) to work at The Roslin Institute, University of Edinburgh where I worked on developing statistical models applied to quantitative genetics genomics of plant and animal breeding, and I am excited to bring my expertise to the AbacusBio team.\rApart from my professional achievements, I also have an extensive publication record in the field of statistics, showcasing my expertise in the area. In my free time, I enjoy photography, which I see as a way to capture the beauty of the world around me.\r","date":1705622400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705652154,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://prof-thiagooliveira.netlify.com/author/thiago-de-paula-oliveira/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/thiago-de-paula-oliveira/","section":"authors","summary":"I am a highly skilled and accomplished statistician with over 10 years of experience in experimental statistics and statistical modelling. I hold a PhD in Statistics from the University of S√£o Paulo, Brazil, and during my PhD studies, I gained expertise in statistical modelling in agricultural data while working as a visiting scholar at the National University of Ireland Galway, Ireland.","tags":null,"title":"Thiago de Paula Oliveira","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://prof-thiagooliveira.netlify.com/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"Table of Contents\r Summary  \r\rSummary I contributed with two talks related to the tidyverse world. In the first talk, I covered the grammar of graphics using ggplot2 and, in the second one, I showed how to use tidyverse functions to prepare tidy data.\n","date":1631955600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631955600,"objectID":"56ca024d2ed51f518606266ad8e5658a","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/","publishdate":"2021-09-18T09:00:00Z","relpermalink":"/courses/2021_ddbg/","section":"courses","summary":"Overview of the basics experimental designs","tags":null,"title":"Breeding Programme Modelling with AlphaSimR","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents\r Summary  \r\rSummary Planning observational studies, and experiment are one of the essential steps in scientific methodology. Unfortunately, in some cases, the statistical planning is neglected, leading to an analysis, sometimes, that does not answer properly the questions of researcher interest. When we are planning an experiment, we should account for non-biased samples, at same time that we consider the availability of experimental material. Moreover, we should to establish the experimental design with or without randomization restrictions.\n","date":1535360400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1535475600,"objectID":"06e45f2789bdef92c34686e706f56260","permalink":"https://prof-thiagooliveira.netlify.com/courses/2019_workshop/","publishdate":"2018-08-27T09:00:00Z","relpermalink":"/courses/2019_workshop/","section":"courses","summary":"Overview of the basics experimental designs","tags":null,"title":"I Workshop in introduction of experimental design","type":"docs"},{"authors":null,"categories":null,"content":"Did you find this page helpful? Consider sharing it üôå ","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"246300c7e70a1142a658f22bdbb04f77","permalink":"https://prof-thiagooliveira.netlify.com/post/experimental_design/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/post/experimental_design/","section":"post","summary":"Did you find this page helpful? Consider sharing it üôå ","tags":null,"title":"Main elements and questions of a good experimental plan.","type":"post"},{"authors":null,"categories":null,"content":"R code containing the basics on how to randomize an experiment.\nTip 1 Example of a completely randomized design.\n############################################################################\r#################### Completely Randomized Design ##########################\r############################################################################\r# Type of Rootstock\rTrat \u0026lt;- gl(5,1,labels = c(\u0026quot;P1\u0026quot;, \u0026quot;P2\u0026quot;, \u0026quot;P3\u0026quot;, \u0026quot;P4\u0026quot;, \u0026quot;Controle\u0026quot;)) # Repetitions\rRep \u0026lt;- 5 # Draw of treatments to plots\rDIC \u0026lt;- function(Trat,Rep){\rTrat \u0026lt;- rep(Trat,Rep)\rN \u0026lt;- length(levels(Trat))*Rep # N√∫mero total de parcelas\rPlan\u0026lt;-as.data.frame(\rmatrix(\rsample(Trat,N), ncol = length(levels(Trat)), nrow=Rep\r)\r)\rcolnames(Plan)\u0026lt;-paste(\u0026quot;Coluna\u0026quot;, c(seq(1:length(levels(Trat)))))\rrownames(Plan)\u0026lt;-paste(\u0026quot;Linha\u0026quot;, c(seq(1:Rep)))\rreturn(Plan)\r}\r# Experiment Sketch\rDIC(Trat,Rep)\r Tip 2 Example of a Randomized Block Design.\n############################################################################\r################### Randomized Block Design ################################\r############################################################################\r# A researcher wishes to evaluate the color, odor and consistency of ruminal\r# juice samples from cattle of same breed, who are treated with 3 types of # feeds. As a restriction ofexperiment implementation, we can confine up to # 4 cattle per sector and the maximum number of sectors available is 5. # In addition, those animals were classified into three groups of carcasses: # i) light (226-228 kg), ii) medium (241-243 kg) and iii) higher (259-261 Kg).\r# Feed\rTrat \u0026lt;- gl(3,1,labels = c(\u0026quot;Ra√ß√£o 1\u0026quot;, \u0026quot;Ra√ß√£o 2\u0026quot;, \u0026quot;Ra√ß√£o 3\u0026quot;)) # Number of Blocks - carcass groups\rBloco \u0026lt;- gl(5,1,labels = c(\u0026quot;Bloco I\u0026quot;, \u0026quot;Bloco II\u0026quot;, \u0026quot;Bloco III\u0026quot;, \u0026quot;Bloco IV\u0026quot;, \u0026quot;Bloco V\u0026quot;))\r# Total number of plots\rN \u0026lt;- length(levels(Trat))*length(levels(Bloco))\r# Draw of treatments to plots within blocks\rDCB \u0026lt;- function(Trat,Bloco){\rTrat_Bloco \u0026lt;- list(NA)\rfor(i in 1:length(levels(Bloco))){\rTrat_Bloco[[i]]\u0026lt;-matrix(\rsample(Trat,length(levels(Trat))))\r}\rPlan \u0026lt;- do.call(cbind.data.frame, Trat_Bloco)\rcolnames(Plan) \u0026lt;- c(levels(Bloco))\rrownames(Plan) \u0026lt;- paste(\u0026quot;Linha\u0026quot;, c(1:length(levels(Trat))))\rreturn(Plan)\r}\r# Experiment Sketch\rDCB(Trat,Bloco)\r Tip 3 Example of a completely randomized designs for factorial structure\n############################################################################\r################## Completely randomized designs ###########################\r####################### Factorial structure ################################\r############################################################################\r# Five soy cultivar\rCultivar \u0026lt;- gl(5,1,labels=c(\u0026quot;BRS 1003IPro\u0026quot;, \u0026quot;BRS 1007IPro\u0026quot;, \u0026quot;BRS 1010IPro\u0026quot;, \u0026quot;BRS 1074IPro\u0026quot;, \u0026quot;BRS 706IPro\u0026quot;))\r# Four nutrient solution with levels of 0.1.2, and 4 mg / liter of manganese.\rSolucao \u0026lt;- gl(4,1,labels = c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;4\u0026quot;))\r# Treatments\rTrat \u0026lt;- expand.grid(Cultivar = Cultivar, Solu√ß√£o = Solucao)\rTrat$Tratamento \u0026lt;- as.factor(paste(Trat$Cultivar,Trat$Solu√ß√£o))\r# Repetition\rRep \u0026lt;- 10 # Draw of treatments to plots\rDIC \u0026lt;- function(Trat,Rep){\rN \u0026lt;- length(levels(Trat))*Rep # Total number of plots\rTrat \u0026lt;- rep(Trat,Rep)\rPlan \u0026lt;- as.data.frame(\rmatrix(\rsample(Trat,N), ncol = Rep, nrow=length(levels(Trat))\r)\r)\rcolnames(Plan) \u0026lt;- paste(\u0026quot;Coluna\u0026quot;, c(seq(1:Rep)))\rrownames(Plan) \u0026lt;- paste(\u0026quot;Linha\u0026quot;, c(seq(1:length(levels(Trat)))))\rreturn(Plan)\r}\r# Experiment Sketch\rDIC(Trat$Tratamento,Rep)\r Tip 4 Example of a Latin Square Design\n############################################################################\r######################## Latin Square Design ###############################\r############################################################################\rL1 \u0026lt;- gl(4,1,labels = c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;))\rL2 \u0026lt;- gl(4,1,labels = c(\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;A\u0026quot;))\rL3 \u0026lt;- gl(4,1,labels = c(\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;))\rL4 \u0026lt;- gl(4,1,labels = c(\u0026quot;D\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;))\rTrat \u0026lt;- rbind(paste0(L1),paste0(L2),paste0(L3),paste0(L4))\rQL \u0026lt;- function(Trat){\r#Sorteando as linhas\rLinha \u0026lt;- Trat[sample(nrow(Trat),size=ncol(Trat)),]\rColuna \u0026lt;- Linha[, sample(nrow(Linha),size=ncol(Linha))]\rreturn(Coluna)\r}\rQL(Trat)\r ","date":1535360400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535360400,"objectID":"8b34388f306b41795f09acd044fa1ee8","permalink":"https://prof-thiagooliveira.netlify.com/courses/2019_workshop/visualization/","publishdate":"2018-08-27T09:00:00Z","relpermalink":"/courses/2019_workshop/visualization/","section":"courses","summary":"R code containing the basics on how to randomize an experiment.","tags":null,"title":"R Code","type":"book"},{"authors":null,"categories":null,"content":"\r\r2020\rMarie Sk≈Çodowska-Curie COFUND Fellowship under the project ``Quantitative genetics and\rgenomics of plant breeding‚Äô‚Äô\rRunner-up Poster on Young-ISA Twitter Poster Conference promoted by the Irish Statistical\rAssociation. Poster Title: Global short-term forecasting of Covid-19 cases. Authors: Oliveira,\rT.P.; Moral, R.A., July, 2020\r\r\r\r2010\rHonorable Mention at the 18th USP International Symposium of Undergraduate Research,\rUniversity of S√£o Paulo.\r\r\r","date":1696723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696723200,"objectID":"805562ed9a54842fc1dfe5bfd3d2235d","permalink":"https://prof-thiagooliveira.netlify.com/accomplishments/awards/","publishdate":"2023-10-08T00:00:00Z","relpermalink":"/accomplishments/awards/","section":"accomplishments","summary":"A showcase of distinguished personal and scientific milestones","tags":null,"title":"Awards and Achievements","type":"book"},{"authors":null,"categories":null,"content":"\r\r2022\rAlphaPart - Partition of Breeding Values by Paths of Information\rSummary:\rThe partitioning method is described in Garcia-Cortes et al.¬†(2008). The package includes the main function AlphaPart for partitioning breeding values and auxiliary functions for manipulating data and summarizing, visualizing, and saving results.\nPublications:\nA method for partitioning trends in genetic mean and variance to understand breeding practices\r\rR Package:\nAlphaPart: Partition/Decomposition of Breeding Values by Paths of Information\r\r\n\rGenomic strategies for optimal crossbreeding in African dairy cattle\rSummary:\rDevelop genomic strategies to optimise crossbreeding in livestock breeding programmes with focus on East African crossbred dairy cattle\n\n\r\r2021\rThe use of biostatistics for optimizing athletes performance\rSummary:\rUniquely blending Data Science and Sports Science to generate customized strategies by athlete\nPublications:\nModelling menstrual cycle length in athletes using state-space models, 2021\r\r\n\rDevelopment of predictive models and analytics techniques to forecast historical data-driven outcomes\rSummary:\rPredictive modelling is a commonly used statistical technique to predict future behavior. Predictive modelling solutions are a form of data-mining technology that analyses historical and current data and generates a model to help predict future outcomes. In predictive modelling, data is collected, a statistical model is formulated, predictions are made, and the model is validated as additional data becomes available.\nPublications:\nGlobal short-term forecasting of COVID-19 cases, 2021\rModelling menstrual cycle length in athletes using state-space models, 2021\r\r\n\r\r\r2020\rThe lcc Package\rSummary:\rEstimating longitudinal concordance correlation function\nPublications:\nlcc: Longitudinal Concordance Correlation, 2019\rlcc: an R package to estimate the concordance correlation, Pearson correlation and accuracy over time, 2020\r\rR Package:\nlcc: Longitudinal Concordance Correlation\r\r\n\rTransmission efficiency of xylella fastidiosa\rSummary:\rXylella fastidiosa is genetically diverse and has many vector species. However, there is limited information on vector specificity and efficiency for different sequence types (STs) Both STs of X. fastidiosa and vectors differ in their associations with plants\nPublications:\nTransmission efficiency of xylella fastidiosa subsp. Pauca sequence types by sharpshooter vectors after in vitro acquisition, 2018\rSettling and feeding behavior of sharpshooter vectors on plum genotypes with different susceptibility levels to leaf scald disease (Xylella fastidiosa), 2020\r\r\r\r\r2019\rSugarcane straw management for bioenergy\rSummary:\rGlobal warming can intensify the soil organic matter (SOM) turnover, damaging soil health. Crop residues left on the soil are important to maintain a positive SOM budget and nutrient cycling. But, sugarcane (Saccharum officinarum) straw has been removed from the field for bioenergy purposes. We hypothesize that global warming, together with straw removal, will negatively impact Brazil‚Äôs ethanol carbon footprint.\nPublications:\nSugarcane straw management for bioenergy: effects of global warming on greenhouse gas emissions and soil carbon storage\r\r\n\rMeasuring color using image analysis\rSummary:\rPromote the usage of image analysis as well as development of statistical methodologies for that purpose.\nPublications:\nMeasuring color hue in ‚ÄòSunrise Solo‚Äô papaya using a flatbed scanner (2017)\rLongitudinal concordance correlation function based on variance components: an application in fruit color analysis, 2018\r\r\r\r","date":1696723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696723200,"objectID":"bbe6f7f8edd6c243ffb12118947c72e6","permalink":"https://prof-thiagooliveira.netlify.com/accomplishments/projects/","publishdate":"2023-10-08T00:00:00Z","relpermalink":"/accomplishments/projects/","section":"accomplishments","summary":"A compilation of projects that have reached completion or were terminated.","tags":null,"title":"Completed \u0026 Discontinued Projects","type":"book"},{"authors":null,"categories":null,"content":"\r\rSummary\r\r\r2021\rWorkflows with Nextflow, University of Edinburgh\rIntroduction to Bash Shell Scripting, Coursera Project Network\rGenome-wide prediction of complex traits in humans, plants and animals\rUKRI-BBSRC Workshop on Computing in the Biosciences\rWorld Meeting of the International Society for Bayesian Analysis\r\r\r\r2020\rProgramming Fundamentals, Coursera, Duke University, USA.\r\r\r\r2019\rSurvival Analysis in R. DataCamp, USA.\rBuilding Web Applications in R with Shiny: Case Studies Course. DataCamp, USA.\rBuilding Dashboards with shinydashboard. DataCamp, USA.\rBuilding Web Applications in R with Shiny. DataCamp, USA.\rIntroduction to Python. DataCamp, USA.\rStatistical Modeling in R (Part 1). DataCamp, USA\rIntermediate R. DataCamp, USA\rIntroduction to R. DataCamp, USA\r\r\r\r2018\rMachine Learning Toolbox. DataCamp, USA\r\r\r\r2016\rLongitudinal and Incomplete Data ‚Äì USP\r\r\r\r2015\rRegression Models ‚Äì Coursera, MOOC, USA\rShort curse on Dimensionality Reduction ‚Äì USP\rAdditive Generalized Models with P-splines ‚Äì RBras\rExploring interactive graphical interfaces in R ‚Äì RBras\rExploring the Flexibility of Linear Mixed Models ‚Äì RBras\rSpecial Topics in Multivariate Analysis ‚Äì RBras\r\r\r\r2014\rGeneralized Additive Models with P-splines ‚Äì USP\r\r\r\r2013\rStatistics: Making Sense of Data ‚Äì Coursera, MOOC, USA\rMathematical Biostatistics Boot Camp ‚Äì Coursera, MOOC, USA\rIntroduction to Categorical Data Analysis ‚Äì USP\rStructural Equations Models ‚Äì USP\rSome Important Topics of Asymptotic Theory ‚Äì USP\r\r\r","date":1696723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696723200,"objectID":"d54c26bac14317e687b42b74dba30877","permalink":"https://prof-thiagooliveira.netlify.com/accomplishments/online_courses/","publishdate":"2023-10-08T00:00:00Z","relpermalink":"/accomplishments/online_courses/","section":"accomplishments","summary":"A chronicle of formal courses and specialized training sessions","tags":null,"title":"Educational Pursuits","type":"book"},{"authors":null,"categories":null,"content":"Visualization and Data Structure What is the grammar of graphics? It is a tool that enables us to concisely describe the components of a graphic and provides a strong foundation for understanding a diverse range of graphics.\nPart I: \rPart II: \rSlides \rR Code Exercises #=======================================================================\r# Packages\r#=======================================================================\rif (!require(\u0026quot;pacman\u0026quot;)) {\rinstall.packages(\u0026quot;pacman\u0026quot;)\r}\r# Include all packages here\rpacman::p_load(\rknitr,\rtidyverse,\rkableExtra, prettycode,\rformattable,\rDT,\rAlphaSimR,\rpatchwork, # ggplot design\rtufte, # quotes\rggridges,\rpathwork,\rggpmisc,\regg,\rdatarium,\rtools\r)\rprettycode::prettycode()\r If you\u0026rsquo;re at all familiar with ggplot, you may know the basic structure of a call to the ggplot() function. For an introduction to the ggplot package, you can check out the visualization and data structure lecture. When you call ggplot, you should provide a data source, usually a data frame or tibble. Afterward, you can build a ggplot by mapping different variables in your data source to different aesthetics. For example, there are colour, shape, size of points, the position of the x or y-axis, ans so on. To demonstrate this and even more procedures using ggplot, we will work with different data sets such as:\n Growth of Orange Trees: the Orange data frame has 35 rows and 3 columns of records of the growth of orange trees.  glimpse(Orange)\r  Storm tracks data: This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.  glimpse(storms)\r  Carbon Dioxide Uptake in Grass Plants: The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.  glimpse(CO2)\r  Simulated cattle breeding program (cbp): the cbp data frame has 10,000 rows and 7 columns of data from a simulation using the AlphaSimR package. The data is comprised of founders and phenotyped individuals (on the Milk Yield), where for each one, we have information of parents, sex, and herd.  cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;)\rglimpse(cbp)\r Mapping p \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex))\rp + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE))\r Practice Use the data CO2 (Biochemical Oxygen Demand) to build a layer with geom = \u0026quot;point\u0026quot; using conc as the response variable, uptake as an explanatory variable, and colour the points by Treatment.\ndata(CO2)\rpEx1 \u0026lt;- ggplot(data = , aes(x = , y = , color = ))\rpEx1 + layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE))\r Facets # facet_wrap\rp + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rfacet_wrap(facet = \u0026quot;sex\u0026quot;)\r# facet_grid\rp + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rfacet_grid(rows = vars(sex), cols = vars(herd))\r Practice  Using the previous exercise as a starting point, add a facet_wrap() using Plant as a subgroup.  pEx1 + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rfacet_wrap()  Make a plot using the storms database considering ts_diameter as response, hu_diameter as explanatory variable, and add a facet_grid() using rows as status and columns as category.  ggplot(data = , aes(y= , x=)) +\rlayer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rfacet_grid(rows = , cols = )\r Scale p2 \u0026lt;- p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rfacet_grid(rows = vars(sex), cols = vars(herd)) +\rscale_colour_manual(values = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#FC4E07\u0026quot;))\rp2\rp2 + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;)\r Practice Make a plot using the storms database using ts_diameter as a response, pressure as an explanatory variable, colored by  status. Also add a facet_wrap() by category and do a scale transformation on the x and y axis using a log10() function.\nggplot(data = storms, aes(y= , x= )) +\rlayer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rfacet_wrap(...) +\rscale_y_*() +\rscale_x_*() +\rylab(expression(log[10](\u0026quot;ts_diameter\u0026quot;))) +\rxlab(expression(log[10](\u0026quot;pressure\u0026quot;)))\r Statistics # facet_wrap\rdata(Orange)\rggplot(data = Orange, aes(x = age, y = circumference)) +\rlayer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rlayer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;,\rparams = list(method = \u0026quot;lm\u0026quot;, se = FALSE))\rggplot(data = cbp, aes(x = year, y = phenotype, group = year)) + layer(geom = \u0026quot;boxplot\u0026quot;, stat = \u0026quot;boxplot\u0026quot;, position = \u0026quot;identity\u0026quot;,\rparams = list(outlier.colour=\u0026quot;red\u0026quot;, outlier.shape = 1,\rna.rm = FALSE)) +\rfacet_grid(rows = vars(sex), cols = vars(herd))\r Practice The main difference between the regression line and LOESS is that while the regression line is a straight line representing the relationship between the x and y, a LOESS line is mainly used to identify trends in the data. Faceting allows you to split the data into subgroups, build a plot adding points, a regression line, and a LOESS curve for each Tree level in the Orange data set. For this, you can use the formula circumference ~ age.\nggplot(data = Orange, aes(x = , y = )) +\rlayer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) +\rlayer(geom = , stat = , position = \u0026quot;identity\u0026quot;,\rparams = list(method = , se = FALSE)) +\rlayer(geom = , stat = , position = \u0026quot;identity\u0026quot;,\rparams = list(method = , se = FALSE, colour = \u0026quot;red\u0026quot;)) +\rfacet_wrap(...)\r Geometries ggplot(data = Orange, aes(x = age, y = circumference)) +\rlayer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE, width = 0.1, height = 0.1)) +\rlayer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;,\rparams = list(method = \u0026quot;lm\u0026quot;, se = FALSE)) +\rfacet_wrap(facet = \u0026quot;Tree\u0026quot;)\r Examples # Density plot\rp \u0026lt;- ggplot(data = cbp)\rp + geom_density(aes(x=phenotype))\r# Density plot per year\rp + geom_density_ridges(aes(x = phenotype, y = year, fill = year))\r# Histogram - absolute frequency of storms per month and year\rggplot(data = storms, aes(x = year)) +\rfacet_wrap(~month)+\rgeom_histogram(fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+\rylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Histogram - relative frequency of storms per month\rggplot(data = storms, aes(x = year)) +\rgeom_histogram(aes(y=stat(count)/sum(..count..)),\rfill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+\rscale_y_continuous(labels=scales::percent) +\rylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Histogram - absolute frequency of storms per year (stack by month)\rggplot(data = storms, aes(x = year, fill = factor(month))) +\rgeom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;stack\u0026quot;)+\rylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rlabs(fill=\u0026quot;Month\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Histogram - proportion of storms per month by year\rggplot(data = storms, aes(x = year, fill = factor(month))) +\rgeom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+\rylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rlabs(fill=\u0026quot;Month\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Exercise: build a histogram with relative frequency per month within year\r# Ribbon and stat_summary - Plot of a time-series\rggplot(data = Orange, aes(x = age, y = circumference)) +\rstat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,\ralpha = 0.3, fun.max = function(x) mean(x) + 2*sd(x),\rfun.min = function(x) mean(x) - 2*sd(x), fill = \u0026quot;#00AFBB\u0026quot;) +\rstat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +\rstat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,\rsize = 2) +\rgeom_point(shape=1)\r Practice A violin plot is a method of plotting numeric data, and it is similar to a box plot but with a rotated kernel density plot on each side. Violin plots can show the probability density of the data at different values, usually smoothed by a kernel density estimator. We can also combine both stories into only one. In this exercise, your task is to build a violin plot adding a box plot internally using the cbp data. Also try to modify the arguments (like alpha, coef etc) to see how they change the final plot.\nggplot(data = cbp, aes(x = year, y = phenotype, group = year)) +\rgeom_violin(aes(fill = ), size = 1, alpha = 0.5) +\rgeom_boxplot(outlier.alpha = 0, coef =0, colour = \u0026quot;black\u0026quot;, width = 0.2)\r Coordinates # Cartesian coordinate\rp \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(category))) +\rgeom_bar(aes(y = stat(count)/sum(..count..))) +\rscale_y_continuous(labels=scales::percent) +\rlabs(title = \u0026quot;Cartesian Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;)\rp\r# Polar coordinate\rp + coord_polar(theta=\u0026quot;y\u0026quot;) +\rlabs(title = \u0026quot;Polar Coordinate\u0026quot;)\r Themes p \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) +\rstat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,\ralpha = 0.3, fun.max = max,\rfun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) +\rstat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +\rstat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,\rsize = 2) +\rgeom_point(shape=1)\rp + theme_bw()\rp + theme_classic()\rp + theme_light()\rp + theme_grey()\rp + theme_minimal()\rp + theme_void()\r p + labs(title = \u0026quot;Modifing themes\u0026quot;) +\rtheme(axis.title = element_text(size = 13, colour = \u0026quot;blue\u0026quot;, family=\u0026quot;serif\u0026quot;,\rface = \u0026quot;bold\u0026quot;),\raxis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;))\rp + labs(title = \u0026quot;Modifing themes\u0026quot;) +\rtheme(text = element_text(size = 13, colour = \u0026quot;blue\u0026quot;, family=\u0026quot;serif\u0026quot;,\rface = \u0026quot;bold\u0026quot;))\rp +\rtheme(panel.grid = element_blank(),\rpanel.grid.minor = element_blank(),\rpanel.border = element_rect(fill=NA, color=\u0026quot;black\u0026quot;, size=0.5, linetype=\u0026quot;dashed\u0026quot;),\raxis.line = element_line(colour = \u0026quot;darkblue\u0026quot;, size = 1, linetype = \u0026quot;solid\u0026quot;),\raxis.ticks = element_line(color=\u0026quot;black\u0026quot;, size=1.2),\rpanel.background = element_rect(fill = \u0026quot;white\u0026quot;),\raxis.title = element_text(size = 13, colour = \u0026quot;darkblue\u0026quot;, family=\u0026quot;serif\u0026quot;,\rface = \u0026quot;bold\u0026quot;),\raxis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;))\rp \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex)) +\rgeom_point()\rp + theme_classic() +\rtheme(legend.position=\u0026quot;top\u0026quot;,\rlegend.background = element_rect(fill=NULL,\rsize=0.5, linetype=\u0026quot;solid\u0026quot;, colour =\u0026quot;darkblue\u0026quot;))\r Practice Let we start with this initial ggplot object:\ncbp$year2 \u0026lt;- as.numeric(cbp$year)\rp \u0026lt;- ggplot(data = cbp, aes(x = year2, y = phenotype)) +\rgeom_smooth(aes(linetype = herd, color = herd), method = \u0026quot;loess\u0026quot;, se = FALSE) +\rstat_summary(aes(group = herd,colour = herd),fun = mean, geom =\u0026quot;point\u0026quot;,\rsize = 1.3, shape = 1) +\rfacet_wrap(~sex)\rp\r Now we have some tasks to make this plot looks more professional.\n Add a new standard theme:  p2 \u0026lt;- p + theme_*() # suggestion to classic\r Change the legend from right to top, increase text (title and axis) size to 13, increase the axis ticks to 1.3, increase legend key width to 1.3 cm, and include a legend key with fill = \u0026quot;white\u0026quot; and colour = \u0026quot;gray40\u0026quot;:  p3 \u0026lt;- p2 +\rtheme(legend.position = \u0026quot;top\u0026quot;,\raxis.* = element_*(size = ),\raxis.* = element_*(size= ),\raxis.* = element_*(size = ),\rlegend.* = element_*(fill = \u0026quot;white\u0026quot;, colour = \u0026quot;gray40\u0026quot;),\rlegend.key.* = unit( , \u0026quot;cm\u0026quot;))\r Change the y label to Phenotype, x label to Year, and legend title from herd to Herd.  p3 + labs(...)\r Quick plot # Using qplot\rqplot( year, phenotype, data = cbp, facets = sex ~ herd)\r# using ggplot\rggplot(data = cbp, aes(x = year, y = phenotype)) +\rgeom_point() +\rfacet_grid(rows = vars(sex), cols = vars(herd))\r Annotate # Example 1\rggplot(storms, aes(x = wind, y = pressure, colour = status)) +\rgeom_point(shape=1) + annotate(\u0026quot;rect\u0026quot;, xmin = 63, xmax = 161, ymin = 880, ymax = 1010,\ralpha = .2) +\rannotate(\u0026quot;text\u0026quot;, x = 110, y = 888, label = \u0026quot;Gilbert (1988)\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = 110, y = 882, label = \u0026quot;Wilma (2005)\u0026quot;) +\rgeom_segment(aes(x = 135, y = 888, xend = 159, yend = 888),\rarrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)),\rshow.legend = FALSE)+\rgeom_segment(aes(x = 135, y = 882, xend = 159, yend = 882),\rarrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)),\rshow.legend = FALSE)\r### Example 2\rcolnames(Orange)[2:3] \u0026lt;- c(\u0026quot;x\u0026quot;,\u0026quot;y\u0026quot;)\rp1 \u0026lt;- ggplot(data = Orange, aes(x = x, y = y)) +\rgeom_point(shape=1) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) +\rggpmisc::stat_poly_eq(\raes(label = paste(after_stat(eq.label),\rafter_stat(adj.rr.label),\rsep = \u0026quot;*\\\u0026quot;, \\\u0026quot;*\u0026quot;)),\rformula = y ~ poly(x, 1, raw = TRUE))\rp1\r# Using the package ggpmisc with facets\rp2 \u0026lt;- p1 + facet_wrap(~Tree)\rtag_facet(p2, x = 1300, y = -Inf, vjust = -1,\ropen = \u0026quot;\u0026quot;, close = \u0026quot;)\u0026quot;,\rtag_pool = LETTERS)\r# Example 3\rex1 \u0026lt;- data.frame(y = 1:3, family = c(\u0026quot;sans\u0026quot;, \u0026quot;serif\u0026quot;, \u0026quot;Roboto\u0026quot;))\rggplot(data=ex1) + geom_text(aes(x=1, y=y, label = family, colour = family, family = family),\rshow.legend = FALSE, size = 6)+ geom_label(aes(x=0, y=y, label = family, family = family,\rfontface = c(1:3)))+\rxlim(c(-0.5,1.5)) +\rtheme_classic(base_size = 12)\r Practice Use the function lmNote to add the regression equation and $R^2$ to the plot using a label geometry.\n# Function to extract coefficients lm(y~x)\rlmNote \u0026lt;- function(y, x, digits=2) {\rp \u0026lt;- lm(y~x)\rz \u0026lt;- list(beta0 = format(as.numeric(coef(p)[1]), digits = digits),\rbeta1 = format(abs(as.numeric(coef(p)[2])), digits = digits),\rr2 = format(summary(p)$r.squared, digits = digits));\rif (coef(p)[2] \u0026gt;= 0) {\rtmp \u0026lt;- substitute(hat(y) == beta0 + beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z)\r} else {\rtmp \u0026lt;- substitute(hat(y) == beta0 - beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z) }\ras.character(as.expression(tmp))\r}\rdata(\u0026quot;Orange\u0026quot;)\rggplot(data = Orange, aes(x = age, y = circumference)) +\rgeom_point(shape=1) +\rgeom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) +\rgeom_*(x=400, y=200, label = lmNote(y = Orange$, x = Orange$),\rparse = TRUE)\r Plot Composition p1 \u0026lt;- ggplot(data = storms, aes(x = year, fill = factor(month))) +\rgeom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+\rylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rlabs(fill=\u0026quot;Month\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\rp2 \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(status):factor(category))) +\rgeom_bar(aes(y = stat(count)/sum(..count..))) +\rscale_y_continuous(labels=scales::percent) +\rlabs(title = \u0026quot;Polar Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;) + coord_polar(theta=\u0026quot;y\u0026quot;)\rp3 \u0026lt;- ggplot(data = storms, aes(x = year)) +\rgeom_histogram(aes(y=stat(count)/sum(..count..)),\rfill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+\rscale_y_continuous(labels=scales::percent) +\rylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) +\rxlab(\u0026quot;Year\u0026quot;) +\rtheme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r(p3 | p2) +\rplot_annotation(tag_levels = \u0026quot;A\u0026quot;)\rp1|(p3/p2)\r data(\u0026quot;Orange\u0026quot;)\rp1 \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) +\rstat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,\ralpha = 0.3, fun.max = max,\rfun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) +\rstat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +\rstat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,\rsize = 2) +\rgeom_point(shape=1)\rOrangeSummary \u0026lt;- Orange %\u0026gt;%\rgroup_by(age) %\u0026gt;%\rsummarise(\rAge = unique(age),\rMean = mean(circumference),\rMedian = median(circumference),\rSd = sd(circumference)\r) %\u0026gt;%\rround(1)\rp1 | gridExtra::tableGrob(OrangeSummary)\rtext \u0026lt;- paste(\u0026quot;The Orange data frame has 35 rows and 3 columns\u0026quot;, \u0026quot;of records of the growth of orange trees.\u0026quot;, sep = \u0026quot;\\n\u0026quot;)\rp4 \u0026lt;- wrap_elements(ggpubr::text_grob(text, face=\u0026quot;bold\u0026quot;,color = \u0026quot;blue\u0026quot;))\rp4/( p1 | gridExtra::tableGrob(OrangeSummary))\r ","date":1631923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631923200,"objectID":"b2ced46b92d7cbbcd1979336a0d519d9","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/ggplot2/","publishdate":"2021-09-18T00:00:00Z","relpermalink":"/courses/2021_ddbg/ggplot2/","section":"courses","summary":"Visualization and Data Structure What is the grammar of graphics? It is a tool that enables us to concisely describe the components of a graphic and provides a strong foundation for understanding a diverse range of graphics.","tags":null,"title":"The grammar of graphics","type":"book"},{"authors":null,"categories":null,"content":"\r\rpre  code.sourceCode { white-space: pre; position: relative; }\rpre  code.sourceCode  span { display: inline-block; line-height: 1.25; }\rpre  code.sourceCode  span:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode  span { color: inherit; text-decoration: inherit; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rpre  code.sourceCode { white-space: pre-wrap; }\rpre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; }\r}\rpre.numberSource code\r{ counter-reset: source-line 0; }\rpre.numberSource code  span\r{ position: relative; left: -4em; counter-increment: source-line; }\rpre.numberSource code  span  a:first-child::before\r{ content: counter(source-line);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rbackground-color: #ffffff;\rcolor: #a0a0a0;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0; padding-left: 4px; }\rdiv.sourceCode\r{ color: #1f1c1b; background-color: #ffffff; }\r@media screen {\rpre  code.sourceCode  span  a:first-child::before { text-decoration: underline; }\r}\rcode span { color: #1f1c1b; } /* Normal */\rcode span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */\rcode span.an { color: #ca60ca; } /* Annotation */\rcode span.at { color: #0057ae; } /* Attribute */\rcode span.bn { color: #b08000; } /* BaseN */\rcode span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */\rcode span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #924c9d; } /* Char */\rcode span.cn { color: #aa5500; } /* Constant */\rcode span.co { color: #898887; } /* Comment */\rcode span.cv { color: #0095ff; } /* CommentVar */\rcode span.do { color: #607880; } /* Documentation */\rcode span.dt { color: #0057ae; } /* DataType */\rcode span.dv { color: #b08000; } /* DecVal */\rcode span.er { color: #bf0303; text-decoration: underline; } /* Error */\rcode span.ex { color: #0095ff; font-weight: bold; } /* Extension */\rcode span.fl { color: #b08000; } /* Float */\rcode span.fu { color: #644a9b; } /* Function */\rcode span.im { color: #ff5500; } /* Import */\rcode span.in { color: #b08000; } /* Information */\rcode span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */\rcode span.op { color: #1f1c1b; } /* Operator */\rcode span.ot { color: #006e28; } /* Other */\rcode span.pp { color: #006e28; } /* Preprocessor */\rcode span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */\rcode span.sc { color: #3daee9; } /* SpecialChar */\rcode span.ss { color: #ff5500; } /* SpecialString */\rcode span.st { color: #bf0303; } /* String */\rcode span.va { color: #0057ae; } /* Variable */\rcode span.vs { color: #bf0303; } /* VerbatimString */\rcode span.wa { color: #bf0303; } /* Warning */\r\r\rMapping\r\rPractice\r\rFacets\r\rPractice\r\rScale\r\rPractice\r\rStatistics\r\rPractice\r\rGeometries\r\rExamples\rPractice\r\rCoordinates\rThemes\r\rPractice\r\rQuick plot\rAnnotate\r\rPractice\r\rPlot Composition\r\r\rIf you‚Äôre at all familiar with ggplot, you may know the basic structure of a call to the ggplot() function. For an introduction to the ggplot package, you can check out the visualization and data structure lecture. When you call ggplot, you should provide a data source, usually a data frame or tibble. Afterward, you can build a ggplot by mapping different variables in your data source to different aesthetics. For example, there are colour, shape, size of points, the position of the x or y-axis, ans so on. To demonstrate this and even more procedures using ggplot, we will work with different data sets such as:\n\rGrowth of Orange Trees: the Orange data frame has 35 rows and 3 columns of records of the growth of orange trees.\r\rglimpse(Orange)\r## Rows: 35\r## Columns: 3\r## $ Tree \u0026lt;ord\u0026gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,‚Ä¶\r## $ age \u0026lt;dbl\u0026gt; 118, 484, 664, 1004, 1231, 1372, 1582, 118, 484, 664, 10‚Ä¶\r## $ circumference \u0026lt;dbl\u0026gt; 30, 58, 87, 115, 120, 142, 145, 33, 69, 111, 156, 172, 2‚Ä¶\r\rStorm tracks data: This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.\r\rglimpse(storms)\r## Rows: 10,010\r## Columns: 13\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;A‚Ä¶\r## $ year \u0026lt;dbl\u0026gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975‚Ä¶\r## $ month \u0026lt;dbl\u0026gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7‚Ä¶\r## $ day \u0026lt;int\u0026gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30‚Ä¶\r## $ hour \u0026lt;dbl\u0026gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18, 0,‚Ä¶\r## $ lat \u0026lt;dbl\u0026gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3, 34.0, 34.4, 34.0‚Ä¶\r## $ long \u0026lt;dbl\u0026gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7, -78.0, -77.0, -7‚Ä¶\r## $ status \u0026lt;chr\u0026gt; \u0026quot;tropical depression\u0026quot;, \u0026quot;tropical depression\u0026quot;, \u0026quot;tropical de‚Ä¶\r## $ category \u0026lt;ord\u0026gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\r## $ wind \u0026lt;int\u0026gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 45, 50, 50, 55, 60‚Ä¶\r## $ pressure \u0026lt;int\u0026gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011, 1006, 1004, 1002‚Ä¶\r## $ ts_diameter \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\r## $ hu_diameter \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\r\rCarbon Dioxide Uptake in Grass Plants: The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.\r\rglimpse(CO2)\r## Rows: 84\r## Columns: 5\r## $ Plant \u0026lt;ord\u0026gt; Qn1, Qn1, Qn1, Qn1, Qn1, Qn1, Qn1, Qn2, Qn2, Qn2, Qn2, Qn2, ‚Ä¶\r## $ Type \u0026lt;fct\u0026gt; Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Queb‚Ä¶\r## $ Treatment \u0026lt;fct\u0026gt; nonchilled, nonchilled, nonchilled, nonchilled, nonchilled, ‚Ä¶\r## $ conc \u0026lt;dbl\u0026gt; 95, 175, 250, 350, 500, 675, 1000, 95, 175, 250, 350, 500, 6‚Ä¶\r## $ uptake \u0026lt;dbl\u0026gt; 16.0, 30.4, 34.8, 37.2, 35.3, 39.2, 39.7, 13.6, 27.3, 37.1, ‚Ä¶\r\rSimulated cattle breeding program (cbp): the cbp data frame has 10,000 rows and 7 columns of data from a simulation using the AlphaSimR package. The data is comprised of founders and phenotyped individuals (on the Milk Yield), where for each one, we have information of parents, sex, and herd.\r\rcbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;)\rglimpse(cbp)\r## Rows: 10,000\r## Columns: 7\r## $ ind \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1‚Ä¶\r## $ father \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\r## $ mother \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶\r## $ year \u0026lt;fct\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\r## $ sex \u0026lt;fct\u0026gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, ‚Ä¶\r## $ phenotype \u0026lt;dbl\u0026gt; 37.66915, 35.79590, 28.42813, 33.62726, 32.86620, 31.63590, ‚Ä¶\r## $ herd \u0026lt;fct\u0026gt; E, B, A, D, A, A, A, E, E, C, A, B, B, A, C, C, E, C, A, A, ‚Ä¶\rMapping\rp \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex))\rp + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE))\rPractice\rUse the data CO2 (Biochemical Oxygen Demand) to build a layer with geom = \"point\" using conc as the response variable, uptake as an explanatory variable, and colour the points by Treatment.\ndata(CO2)\rpEx1 \u0026lt;- ggplot(data = , aes(x = , y = , color = ))\rpEx1 + layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE))\r\r\rFacets\r# facet_wrap\rp + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r facet_wrap(facet = \u0026quot;sex\u0026quot;)\r# facet_grid\rp + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r facet_grid(rows = vars(sex), cols = vars(herd))\rPractice\rUsing the previous exercise as a starting point, add a facet_wrap() using Plant as a subgroup.\r\rpEx1 + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r facet_wrap() \rMake a plot using the storms database considering ts_diameter as response, hu_diameter as explanatory variable, and add a facet_grid() using rows as status and columns as category.\r\rggplot(data = , aes(y= , x=)) +\r layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r facet_grid(rows = , cols = )\r\r\rScale\rp2 \u0026lt;- p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r facet_grid(rows = vars(sex), cols = vars(herd)) +\r scale_colour_manual(values = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#FC4E07\u0026quot;))\rp2\rp2 + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;)\rPractice\rMake a plot using the storms database using ts_diameter as a response, pressure as an explanatory variable, colored by status. Also add a facet_wrap() by category and do a scale transformation on the x and y axis using a log10() function.\nggplot(data = storms, aes(y= , x= )) +\r layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r facet_wrap(...) +\r scale_y_*() +\r scale_x_*() +\r ylab(expression(log[10](\u0026quot;ts_diameter\u0026quot;))) +\r xlab(expression(log[10](\u0026quot;pressure\u0026quot;)))\r\r\rStatistics\r# facet_wrap\rdata(Orange)\rggplot(data = Orange, aes(x = age, y = circumference)) +\r layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r layer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;,\r params = list(method = \u0026quot;lm\u0026quot;, se = FALSE))\rggplot(data = cbp, aes(x = year, y = phenotype, group = year)) + \r layer(geom = \u0026quot;boxplot\u0026quot;, stat = \u0026quot;boxplot\u0026quot;, position = \u0026quot;identity\u0026quot;,\r params = list(outlier.colour=\u0026quot;red\u0026quot;, outlier.shape = 1,\r na.rm = FALSE)) +\r facet_grid(rows = vars(sex), cols = vars(herd))\rPractice\rThe main difference between the regression line and LOESS is that while the regression line is a straight line representing the relationship between the x and y, a LOESS line is mainly used to identify trends in the data. Faceting allows you to split the data into subgroups, build a plot adding points, a regression line, and a LOESS curve for each Tree level in the Orange data set. For this, you can use the formula circumference ~ age.\nggplot(data = Orange, aes(x = , y = )) +\r layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE)) +\r layer(geom = , stat = , position = \u0026quot;identity\u0026quot;,\r params = list(method = , se = FALSE)) +\r layer(geom = , stat = , position = \u0026quot;identity\u0026quot;,\r params = list(method = , se = FALSE, colour = \u0026quot;red\u0026quot;)) +\r facet_wrap(...)\r\r\rGeometries\rggplot(data = Orange, aes(x = age, y = circumference)) +\r layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, \r params = list(shape=1, na.rm = FALSE, width = 0.1, height = 0.1)) +\r layer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;,\r params = list(method = \u0026quot;lm\u0026quot;, se = FALSE)) +\r facet_wrap(facet = \u0026quot;Tree\u0026quot;)\rExamples\r# Density plot\rp \u0026lt;- ggplot(data = cbp)\rp + geom_density(aes(x=phenotype))\r# Density plot per year\rp + geom_density_ridges(aes(x = phenotype, y = year, fill = year))\r# Histogram - absolute frequency of storms per month and year\rggplot(data = storms, aes(x = year)) +\r facet_wrap(~month)+\r geom_histogram(fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+\r ylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) +\r xlab(\u0026quot;Year\u0026quot;) +\r theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Histogram - relative frequency of storms per month\rggplot(data = storms, aes(x = year)) +\r geom_histogram(aes(y=stat(count)/sum(..count..)),\r fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+\r scale_y_continuous(labels=scales::percent) +\r ylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) +\r xlab(\u0026quot;Year\u0026quot;) +\r theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Histogram - absolute frequency of storms per year (stack by month)\rggplot(data = storms, aes(x = year, fill = factor(month))) +\r geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;stack\u0026quot;)+\r ylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) +\r xlab(\u0026quot;Year\u0026quot;) +\r labs(fill=\u0026quot;Month\u0026quot;) +\r theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Histogram - proportion of storms per month by year\rggplot(data = storms, aes(x = year, fill = factor(month))) +\r geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+\r ylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) +\r xlab(\u0026quot;Year\u0026quot;) +\r labs(fill=\u0026quot;Month\u0026quot;) +\r theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r# Exercise: build a histogram with relative frequency per month within year\r\r# Ribbon and stat_summary - Plot of a time-series\rggplot(data = Orange, aes(x = age, y = circumference)) +\r stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,\r alpha = 0.3, fun.max = function(x) mean(x) + 2*sd(x),\r fun.min = function(x) mean(x) - 2*sd(x), fill = \u0026quot;#00AFBB\u0026quot;) +\r stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +\r stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,\r size = 2) +\r geom_point(shape=1)\r\rPractice\rA violin plot is a method of plotting numeric data, and it is similar to a box plot but with a rotated kernel density plot on each side. Violin plots can show the probability density of the data at different values, usually smoothed by a kernel density estimator. We can also combine both stories into only one. In this exercise, your task is to build a violin plot adding a box plot internally using the cbp data. Also try to modify the arguments (like alpha, coef etc) to see how they change the final plot.\nggplot(data = cbp, aes(x = year, y = phenotype, group = year)) +\r geom_violin(aes(fill = ), size = 1, alpha = 0.5) +\r geom_boxplot(outlier.alpha = 0, coef =0, \r colour = \u0026quot;black\u0026quot;, width = 0.2)\r\r\rCoordinates\r# Cartesian coordinate\rp \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(category))) +\r geom_bar(aes(y = stat(count)/sum(..count..))) +\r scale_y_continuous(labels=scales::percent) +\r labs(title = \u0026quot;Cartesian Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;)\rp\r# Polar coordinate\rp + coord_polar(theta=\u0026quot;y\u0026quot;) +\r labs(title = \u0026quot;Polar Coordinate\u0026quot;)\r\rThemes\rp \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) +\r stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,\r alpha = 0.3, fun.max = max,\r fun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) +\r stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +\r stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,\r size = 2) +\r geom_point(shape=1)\r\rp + theme_bw()\rp + theme_classic()\rp + theme_light()\rp + theme_grey()\rp + theme_minimal()\rp + theme_void()\rp + labs(title = \u0026quot;Modifing themes\u0026quot;) +\r theme(axis.title = element_text(size = 13, colour = \u0026quot;blue\u0026quot;, \r family=\u0026quot;serif\u0026quot;,\r face = \u0026quot;bold\u0026quot;),\r axis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;))\rp + labs(title = \u0026quot;Modifing themes\u0026quot;) +\r theme(text = element_text(size = 13, colour = \u0026quot;blue\u0026quot;, \r family=\u0026quot;serif\u0026quot;,\r face = \u0026quot;bold\u0026quot;))\rp +\r theme(panel.grid = element_blank(),\r panel.grid.minor = element_blank(),\r panel.border = element_rect(fill=NA, color=\u0026quot;black\u0026quot;, \r size=0.5, linetype=\u0026quot;dashed\u0026quot;),\r axis.line = element_line(colour = \u0026quot;darkblue\u0026quot;, \r size = 1, linetype = \u0026quot;solid\u0026quot;),\r axis.ticks = element_line(color=\u0026quot;black\u0026quot;, size=1.2),\r panel.background = element_rect(fill = \u0026quot;white\u0026quot;),\r axis.title = element_text(size = 13, colour = \u0026quot;darkblue\u0026quot;, \r family=\u0026quot;serif\u0026quot;,\r face = \u0026quot;bold\u0026quot;),\r axis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;))\rp \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex)) +\r geom_point()\r\rp + theme_classic() +\r theme(legend.position=\u0026quot;top\u0026quot;,\r legend.background = element_rect(fill=NULL,\r size=0.5, linetype=\u0026quot;solid\u0026quot;, \r colour =\u0026quot;darkblue\u0026quot;))\rPractice\rLet we start with this initial ggplot object:\ncbp$year2 \u0026lt;- as.numeric(cbp$year)\rp \u0026lt;- ggplot(data = cbp, aes(x = year2, y = phenotype)) +\r geom_smooth(aes(linetype = herd, color = herd), \r method = \u0026quot;loess\u0026quot;, se = FALSE) +\r stat_summary(aes(group = herd,colour = herd),fun = mean, geom =\u0026quot;point\u0026quot;,\r size = 1.3, shape = 1) +\r facet_wrap(~sex)\rp\rNow we have some tasks to make this plot looks more professional.\nAdd a new standard theme:\r\rp2 \u0026lt;- p + theme_*() # suggestion to classic\rChange the legend from right to top, increase text (title and axis) size to 13, increase the axis ticks to 1.3, increase legend key width to 1.3 cm, and include a legend key with fill = \"white\" and colour = \"gray40\":\r\rp3 \u0026lt;- p2 +\r theme(legend.position = \u0026quot;top\u0026quot;,\r axis.* = element_*(size = ),\r axis.* = element_*(size= ),\r axis.* = element_*(size = ),\r legend.* = element_*(fill = \u0026quot;white\u0026quot;, colour = \u0026quot;gray40\u0026quot;),\r legend.key.* = unit( , \u0026quot;cm\u0026quot;))\rChange the y label to Phenotype, x label to Year, and legend title from herd to Herd.\r\rp3 + \r labs(...)\r\r\rQuick plot\r# Using qplot\rqplot( year, phenotype, data = cbp, facets = sex ~ herd)\r# using ggplot\rggplot(data = cbp, aes(x = year, y = phenotype)) +\r geom_point() +\r facet_grid(rows = vars(sex), cols = vars(herd))\r\rAnnotate\r# Example 1\rggplot(storms, aes(x = wind, y = pressure, colour = status)) +\r geom_point(shape=1) + \r annotate(\u0026quot;rect\u0026quot;, xmin = 63, xmax = 161, ymin = 880, ymax = 1010,\r alpha = .2) +\r annotate(\u0026quot;text\u0026quot;, x = 110, y = 888, label = \u0026quot;Gilbert (1988)\u0026quot;) +\r annotate(\u0026quot;text\u0026quot;, x = 110, y = 882, label = \u0026quot;Wilma (2005)\u0026quot;) +\r geom_segment(aes(x = 135, y = 888, xend = 159, yend = 888),\r arrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)),\r show.legend = FALSE)+\r geom_segment(aes(x = 135, y = 882, xend = 159, yend = 882),\r arrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)),\r show.legend = FALSE)\r# Example 2\rcolnames(Orange)[2:3] \u0026lt;- c(\u0026quot;x\u0026quot;,\u0026quot;y\u0026quot;)\rp1 \u0026lt;- ggplot(data = Orange, aes(x = x, y = y)) +\r geom_point(shape=1) +\r geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) +\r ggpmisc::stat_poly_eq(\r aes(label = paste(after_stat(eq.label),\r after_stat(adj.rr.label),\r sep = \u0026quot;*\\\u0026quot;, \\\u0026quot;*\u0026quot;)),\r formula = y ~ poly(x, 1, raw = TRUE))\rp1\r# Using the package ggpmisc with facets\rp2 \u0026lt;- p1 + facet_wrap(~Tree)\r\rtag_facet(p2, \r x = 1300, y = -Inf, \r vjust = -1,\r open = \u0026quot;\u0026quot;, close = \u0026quot;)\u0026quot;,\r tag_pool = LETTERS)\r# Example 3\rex1 \u0026lt;- data.frame(y = 1:3, family = c(\u0026quot;sans\u0026quot;, \u0026quot;serif\u0026quot;, \u0026quot;Roboto\u0026quot;))\rggplot(data=ex1) + \r geom_text(aes(x=1, y=y, label = family, \r colour = family, family = family),\r show.legend = FALSE, size = 6)+ \r geom_label(aes(x=0, y=y, label = family, family = family,\r fontface = c(1:3)))+\r xlim(c(-0.5,1.5)) +\r theme_classic(base_size = 12)\rPractice\rUse the function lmNote to add the regression equation and \\(R^2\\) to the plot using a label geometry.\n# Function to extract coefficients lm(y~x)\rlmNote \u0026lt;- function(y, x, digits=2) {\r p \u0026lt;- lm(y~x)\r z \u0026lt;- list(beta0 = format(as.numeric(coef(p)[1]), digits = digits),\r beta1 = format(abs(as.numeric(coef(p)[2])), digits = digits),\r r2 = format(summary(p)$r.squared, digits = digits));\r if (coef(p)[2] \u0026gt;= 0) {\r tmp \u0026lt;- substitute(hat(y) == beta0 + beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z)\r } else {\r tmp \u0026lt;- substitute(hat(y) == beta0 - beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z) \r }\r as.character(as.expression(tmp))\r}\r\rdata(\u0026quot;Orange\u0026quot;)\rggplot(data = Orange, aes(x = age, y = circumference)) +\r geom_point(shape=1) +\r geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) +\r geom_*(x=400, y=200, label = lmNote(y = Orange$, x = Orange$),\r parse = TRUE)\r\r\rPlot Composition\rp1 \u0026lt;- ggplot(data = storms, aes(x = year, fill = factor(month))) +\r geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+\r ylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) +\r xlab(\u0026quot;Year\u0026quot;) +\r labs(fill=\u0026quot;Month\u0026quot;) +\r theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\rp2 \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(status):factor(category))) +\r geom_bar(aes(y = stat(count)/sum(..count..))) +\r scale_y_continuous(labels=scales::percent) +\r labs(title = \u0026quot;Polar Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;) + \r coord_polar(theta=\u0026quot;y\u0026quot;)\r\rp3 \u0026lt;- ggplot(data = storms, aes(x = year)) +\r geom_histogram(aes(y=stat(count)/sum(..count..)),\r fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+\r scale_y_continuous(labels=scales::percent) +\r ylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) +\r xlab(\u0026quot;Year\u0026quot;) +\r theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\r\r(p3 | p2) +\r plot_annotation(tag_levels = \u0026quot;A\u0026quot;)\rp1|(p3/p2)\rdata(\u0026quot;Orange\u0026quot;)\rp1 \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) +\r stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,\r alpha = 0.3, fun.max = max,\r fun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) +\r stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +\r stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,\r size = 2) +\r geom_point(shape=1)\r\rOrangeSummary \u0026lt;- Orange %\u0026gt;%\r group_by(age) %\u0026gt;%\r summarise(\r Age = unique(age),\r Mean = mean(circumference),\r Median = median(circumference),\r Sd = sd(circumference)\r ) %\u0026gt;%\r round(1)\r\rp1 | gridExtra::tableGrob(OrangeSummary)\rtext \u0026lt;- paste(\u0026quot;The Orange data frame has 35 rows and 3 columns\u0026quot;, \r \u0026quot;of records of the growth of orange trees.\u0026quot;, sep = \u0026quot;\\n\u0026quot;)\rp4 \u0026lt;- wrap_elements(ggpubr::text_grob(text, face=\u0026quot;bold\u0026quot;,color = \u0026quot;blue\u0026quot;))\r\rp4/( p1 | gridExtra::tableGrob(OrangeSummary))\r\r","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"9e7de2e4c6300da0d6ca3fc574148f53","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/oliveiratp_ggplot2/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/courses/2021_ddbg/oliveiratp_ggplot2/","section":"courses","summary":"pre  code.sourceCode { white-space: pre; position: relative; }\rpre  code.sourceCode  span { display: inline-block; line-height: 1.25; }\rpre  code.sourceCode  span:empty { height: 1.2em; }\r.","tags":null,"title":"The grammar of graphics: R code","type":"courses"},{"authors":null,"categories":null,"content":"Visualization and Data Structure Tidy data refers to data arranged to make data processing, analysis, and visualization simpler. Remember that in a tidy data set we should consider:\n Each variable must have its column. Each observation must have its row. Each value must have its cell.  Video \rSlides \rExercises Exercise 1 Let‚Äôs say we want to organize the data anscombe. Below I show how this data looks like:\nanscombe\r  Organize this data set to obtain tidy data. Remember here we have two response variables been measured four times.  ex1 \u0026lt;- anscombe %\u0026gt;%\r Filter the data set to get replications 2 and 4, and summarise it to get the maximum, minimum, and mean values.  ex2 %\u0026gt;%\rfilter() %\u0026gt;%\rsummarise(\r)\r Exercise 2 Often you do not need the entire data set, but just part of it.\n Here, you should make the data mtcars tidy before making any selection.  (dataEx3 \u0026lt;- readRDS(\u0026quot;./data/dataEx3.rds\u0026quot;))\r As you can see, some columns are not variable names but values. Create two new variables calling mpg (for observations) and gear (with column values).\ndataEx3 \u0026lt;- dataEx3 %\u0026gt;%\rpivot_longer(\r)\r Select the columns mpg, hp, gear, and carb, and then make a plot using ggplot2 where  mpg is the response variable, and hp is the co-variate in the x-axis. Also include different shapes and colours for gear, and facets for carb.  dataEx3 %\u0026gt;%\rselect() %\u0026gt;%\rggplot() %\u0026gt;%\rgeom_point() %\u0026gt;%\rfacet_wrap() %\u0026gt;%\rtheme_bw()\r Exercise 3 The following data represents song rankings for Billboard top 100 in the year 2000. The rank of the song is displayed in each week after it entered.\nbillboard\r A slightly more complex case where columns have a common prefix and missing missings are structural, so should be dropped. So, make this data tidy.\nbillboard %\u0026gt;%\r Data Structure Exercise 1  Make this data tidy by including tmin and tmax as variable. Remember that here type is carrying to variables names rather than factors.  (dataEx2 \u0026lt;- as.tibble(readRDS(\u0026quot;./data/dataEx2.RDS\u0026quot;)))\r dataEx2 \u0026lt;- dataEx2 %\u0026gt;%\rpivot_wider()\r Now, build a new variable called tdiff, which is the difference between tmax and tmin. Moreover, display a ggplot2 graph that shows tdiff over time.\ndataEx2 %\u0026gt;%\r Exercise 2 Our cattle data data is already in a tidy format.\n(cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;))\r For this exercise, complete the following tasks with that data set:\n Calculate the average phenotype per year by sex and herd using the summarise() function in the dplyr package. Add two columns to cattle data using the mutate() function:  Column 1: Phenotype should be rescaled to have a mean of zero and a standard deviation of one. You can call this new variable as PhenoStd. Column 2: Rank the PhenoStd using the function min_rank(). The output data frame should have only PhenoStd \u0026gt; 0.    cbp %\u0026gt;%\rsummarise(\r) %\u0026gt;%\r Exercise 3 Excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country. This data has 142 countries observed from the year 1952 to 2007 in increments of 5 years. The response variable observed was the life expectancy at birth (in years), population size, and Per capita gross domestic product (GDP).\nPer capita gross domestic product (GDP) measures a country\u0026rsquo;s economic response per person and is calculated by dividing its GDP by its population. It is a global measure for gauging the prosperity of nations as we can analyze the worth of a country based on its economic growth. Thus, countries that have the highest per capita GDP tend to be more developed.\ngapminder\r Questions:\n What are the ten highest gpdPercap values?  gapminder %\u0026gt;%\r Find both the median life expectancy (lifeExp) and the median and maximum GDP per capita (gdpPercap) in 1957, 1982, and 2007, by country and continent. Call them medianLifeExp, medianGdpPercap, and maxGdpPercap, respectively.  dat \u0026lt;- gapminder %\u0026gt;%\r Use a scatter plot to compare the median GDP and median life expectancy. Use the variables continent and year to produce this plot.  dat %\u0026gt;%\r ","date":1631923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631923200,"objectID":"d98bf4fb1b6752d8c7b4e1d23bcf457a","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/tidyverse/","publishdate":"2021-09-18T00:00:00Z","relpermalink":"/courses/2021_ddbg/tidyverse/","section":"courses","summary":"Visualization and Data Structure Tidy data refers to data arranged to make data processing, analysis, and visualization simpler. Remember that in a tidy data set we should consider:\n Each variable must have its column.","tags":null,"title":"Tidyverse","type":"book"},{"authors":null,"categories":null,"content":"\r\rpre  code.sourceCode { white-space: pre; position: relative; }\rpre  code.sourceCode  span { display: inline-block; line-height: 1.25; }\rpre  code.sourceCode  span:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode  span { color: inherit; text-decoration: inherit; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rpre  code.sourceCode { white-space: pre-wrap; }\rpre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; }\r}\rpre.numberSource code\r{ counter-reset: source-line 0; }\rpre.numberSource code  span\r{ position: relative; left: -4em; counter-increment: source-line; }\rpre.numberSource code  span  a:first-child::before\r{ content: counter(source-line);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rbackground-color: #ffffff;\rcolor: #a0a0a0;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0; padding-left: 4px; }\rdiv.sourceCode\r{ color: #1f1c1b; background-color: #ffffff; }\r@media screen {\rpre  code.sourceCode  span  a:first-child::before { text-decoration: underline; }\r}\rcode span { color: #1f1c1b; } /* Normal */\rcode span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */\rcode span.an { color: #ca60ca; } /* Annotation */\rcode span.at { color: #0057ae; } /* Attribute */\rcode span.bn { color: #b08000; } /* BaseN */\rcode span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */\rcode span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #924c9d; } /* Char */\rcode span.cn { color: #aa5500; } /* Constant */\rcode span.co { color: #898887; } /* Comment */\rcode span.cv { color: #0095ff; } /* CommentVar */\rcode span.do { color: #607880; } /* Documentation */\rcode span.dt { color: #0057ae; } /* DataType */\rcode span.dv { color: #b08000; } /* DecVal */\rcode span.er { color: #bf0303; text-decoration: underline; } /* Error */\rcode span.ex { color: #0095ff; font-weight: bold; } /* Extension */\rcode span.fl { color: #b08000; } /* Float */\rcode span.fu { color: #644a9b; } /* Function */\rcode span.im { color: #ff5500; } /* Import */\rcode span.in { color: #b08000; } /* Information */\rcode span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */\rcode span.op { color: #1f1c1b; } /* Operator */\rcode span.ot { color: #006e28; } /* Other */\rcode span.pp { color: #006e28; } /* Preprocessor */\rcode span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */\rcode span.sc { color: #3daee9; } /* SpecialChar */\rcode span.ss { color: #ff5500; } /* SpecialString */\rcode span.st { color: #bf0303; } /* String */\rcode span.va { color: #0057ae; } /* Variable */\rcode span.vs { color: #bf0303; } /* VerbatimString */\rcode span.wa { color: #bf0303; } /* Warning */\r\r\rData Structure\r\rPivoting: ‚Äòlong form‚Äô\rPivoting: ‚Äòwide form‚Äô\rFilter\rDistinct\rSlice\rArrange\rAdd row\rPull\rSelect\rRelocate\rMutate\rTransmute\rAcross\rRename\rGroup by\r\rStatistics Summary\r\rSummarise\r\r\r\rAll code I used to build the slides can be found here. Making data and code available as supplementary material promotes transparency and reproducibility, enabling anyone to reproduce the methodology discussed during the lecture.\nData Structure\rPivoting: ‚Äòlong form‚Äô\rdataEx1 \u0026lt;- readRDS(\u0026quot;./data/dataEx1.RDS\u0026quot;)\r\r\rdataEx1.1 \u0026lt;- dataEx1 %\u0026gt;%\r rownames_to_column(var = \u0026quot;Farm\u0026quot;) %\u0026gt;%\r pivot_longer(cols = 2:4, names_to = \u0026quot;Income\u0026quot;, values_to = \u0026quot;Freq\u0026quot;)\r\rPivoting: ‚Äòwide form‚Äô\rdataEx2 \u0026lt;- as_tibble(readRDS(\u0026quot;./data/dataEx2.RDS\u0026quot;))\r\rdataEx2.2 \u0026lt;- dataEx2 %\u0026gt;%\r pivot_wider(values_from = value, names_from = type)\r\rI \u0026lt;- dataEx2 %\u0026gt;%\r pivot_wider(values_from = value, names_from = type) %\u0026gt;%\r transform(id = str_replace(id,\u0026quot;Ind\u0026quot;,\u0026quot;\u0026quot;))\r\rFilter\rcbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;)\r\rthreshold \u0026lt;- with(cbp, mean(phenotype) + 2*sd(phenotype))\rcbp %\u0026gt;%\r filter(herd %in% c(\u0026quot;A\u0026quot;, \u0026quot;E\u0026quot;)) %\u0026gt;%\r filter(year == \u0026quot;9\u0026quot; \u0026amp; phenotype \u0026gt; threshold) %\u0026gt;%\r ggplot(aes(y = phenotype, x = sex)) +\r geom_violin() +\r geom_jitter(shape = 1, width = 0.15)\rcbp %\u0026gt;%\r filter(herd %in% c(\u0026quot;A\u0026quot;, \u0026quot;E\u0026quot;)) %\u0026gt;%\r filter(year == \u0026quot;9\u0026quot; \u0026amp; (phenotype \u0026gt; threshold | sex == \u0026quot;F\u0026quot;)) %\u0026gt;%\r ggplot(aes(y = phenotype, x = sex)) +\r geom_violin() +\r geom_jitter(shape = 1, width = 0.15)\rggsave(\u0026quot;filterData2.pdf\u0026quot;, width = 4, height = 4)\r\rDistinct\rcbp %\u0026gt;%\r distinct(dplyr::across(contains(\u0026quot;r\u0026quot;)))\r## # A tibble: 8,186 √ó 4\r## father mother year herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 NA NA 0 E ## 2 NA NA 0 B ## 3 NA NA 0 A ## 4 NA NA 0 D ## 5 NA NA 0 C ## 6 418 692 1 E ## 7 461 614 1 B ## 8 195 524 1 A ## 9 198 768 1 D ## 10 122 537 1 A ## # ‚Ä¶ with 8,176 more rows\rcbp %\u0026gt;%\r distinct(sex)\r## # A tibble: 2 √ó 1\r## sex ## \u0026lt;fct\u0026gt;\r## 1 M ## 2 F\r\rSlice\rcbp %\u0026gt;% slice(1:3)\r## # A tibble: 3 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A\rcbp %\u0026gt;% slice((n()-5L):n())\r## # A tibble: 6 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 9995 8463 8849 9 F 71.3 E ## 2 9996 8013 8628 9 F 61.9 E ## 3 9997 8342 8677 9 F 58.4 A ## 4 9998 8088 8510 9 F 58.2 E ## 5 9999 8449 8685 9 F 56.7 D ## 6 10000 8296 8854 9 F 67.1 B\rcbp %\u0026gt;% slice_sample(n = 5)\r## # A tibble: 5 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 9516 8410 8794 9 F 58.3 C ## 2 6592 5280 5501 6 F 47.2 A ## 3 3973 2372 2794 3 F 35.6 E ## 4 5629 4404 4834 5 F 45.7 B ## 5 6441 5318 5840 6 M 56.7 B\rcbp %\u0026gt;% slice_min(phenotype, n = 3)\r## # A tibble: 3 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1331 148 840 1 M 15.3 B ## 2 1316 62 551 1 M 20.2 D ## 3 1992 354 523 1 F 20.3 C\r\rArrange\rcbp %\u0026gt;% arrange(desc(phenotype)) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 9069 8366 8725 9 M 82.1 C ## 2 9979 8305 8994 9 F 80.0 B ## 3 9492 8346 8985 9 M 79.1 B ## 4 9934 8470 8776 9 F 78.9 A ## 5 8650 7117 7726 8 F 78.6 B\rcbp %\u0026gt;% arrange(desc(herd)) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 0 M 37.7 E ## 2 8 NA NA 0 M 33.3 E ## 3 9 NA NA 0 M 39.0 E ## 4 17 NA NA 0 M 42.5 E ## 5 23 NA NA 0 M 51.9 E\rcbp %\u0026gt;% arrange(sex, phenotype, herd) %\u0026gt;% slice_head(n=7)\r## # A tibble: 7 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1992 354 523 1 F 20.3 C ## 2 618 NA NA 0 F 20.8 C ## 3 1830 378 900 1 F 20.8 A ## 4 557 NA NA 0 F 21.5 A ## 5 1663 102 983 1 F 21.9 B ## 6 800 NA NA 0 F 22.2 A ## 7 1645 47 541 1 F 22.9 D\rcbp %\u0026gt;% arrange(herd, phenotype, sex) %\u0026gt;% slice_head(n=7)\r## # A tibble: 7 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1830 378 900 1 F 20.8 A ## 2 557 NA NA 0 F 21.5 A ## 3 95 NA NA 0 M 22.2 A ## 4 800 NA NA 0 F 22.2 A ## 5 1592 440 667 1 F 23.0 A ## 6 1192 355 642 1 M 24.7 A ## 7 1671 448 953 1 F 25.0 A\r\rAdd row\rdf \u0026lt;- tibble(x = 1:4, y = 20:23)\rdf %\u0026gt;% add_row(x = 3, y =21)\r## # A tibble: 5 √ó 2\r## x y\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 20\r## 2 2 21\r## 3 3 22\r## 4 4 23\r## 5 3 21\rdf %\u0026gt;% add_row(x = 3, y =21, .before = 4)\r## # A tibble: 5 √ó 2\r## x y\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 20\r## 2 2 21\r## 3 3 22\r## 4 3 21\r## 5 4 23\rdf %\u0026gt;% add_row(x = 3, .before = 4)\r## # A tibble: 5 √ó 2\r## x y\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 1 20\r## 2 2 21\r## 3 3 22\r## 4 3 NA\r## 5 4 23\r\rPull\rdf %\u0026gt;% pull(var = x)\r## [1] 1 2 3 4\rdf %\u0026gt;% pull(var = x) %\u0026gt;% sum()\r## [1] 10\r\rSelect\rcbp %\u0026gt;%\r select(ind:mother) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 3\r## ind father mother\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 NA NA\r## 2 2 NA NA\r## 3 3 NA NA\r## 4 4 NA NA\r## 5 5 NA NA\rcbp %\u0026gt;%\r select(starts_with(c(\u0026quot;p\u0026quot;,\u0026quot;m\u0026quot;))| ends_with(\u0026quot;r\u0026quot;)) %\u0026gt;% \r slice_head(n=5)\r## # A tibble: 5 √ó 4\r## phenotype mother father year ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 37.7 NA NA 0 ## 2 35.8 NA NA 0 ## 3 28.4 NA NA 0 ## 4 33.6 NA NA 0 ## 5 32.9 NA NA 0\rcbp %\u0026gt;%\r select(starts_with(c(\u0026quot;p\u0026quot;,\u0026quot;m\u0026quot;)) \u0026amp; !ends_with(\u0026quot;r\u0026quot;)) %\u0026gt;% \r slice_head(n=5)\r## # A tibble: 5 √ó 1\r## phenotype\r## \u0026lt;dbl\u0026gt;\r## 1 37.7\r## 2 35.8\r## 3 28.4\r## 4 33.6\r## 5 32.9\rcbp %\u0026gt;% select(contains(\u0026quot;a\u0026quot;)) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 2\r## father year ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 NA 0 ## 2 NA 0 ## 3 NA 0 ## 4 NA 0 ## 5 NA 0\r\rRelocate\rcbp %\u0026gt;%\r relocate(mother, .before = father) %\u0026gt;%\r relocate(year, herd, .after = sex)\r## # A tibble: 10,000 √ó 7\r## ind mother father sex year herd phenotype\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 NA NA M 0 E 37.7\r## 2 2 NA NA M 0 B 35.8\r## 3 3 NA NA M 0 A 28.4\r## 4 4 NA NA M 0 D 33.6\r## 5 5 NA NA M 0 A 32.9\r## 6 6 NA NA M 0 A 31.6\r## 7 7 NA NA M 0 A 38.8\r## 8 8 NA NA M 0 E 33.3\r## 9 9 NA NA M 0 E 39.0\r## 10 10 NA NA M 0 C 46.0\r## # ‚Ä¶ with 9,990 more rows\rcbp %\u0026gt;%\r relocate(where(is.factor), \r .after = last_col())\r## # A tibble: 10,000 √ó 7\r## ind father mother phenotype year sex herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 37.7 0 M E ## 2 2 NA NA 35.8 0 M B ## 3 3 NA NA 28.4 0 M A ## 4 4 NA NA 33.6 0 M D ## 5 5 NA NA 32.9 0 M A ## 6 6 NA NA 31.6 0 M A ## 7 7 NA NA 38.8 0 M A ## 8 8 NA NA 33.3 0 M E ## 9 9 NA NA 39.0 0 M E ## 10 10 NA NA 46.0 0 M C ## # ‚Ä¶ with 9,990 more rows\r\rMutate\rcbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%\r dplyr::mutate(logPheno = log10(phenotype)) %\u0026gt;% \r slice_head(n=5)\r## # A tibble: 5 √ó 3\r## sex phenotype logPheno\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 M 37.7 1.58\r## 2 M 35.8 1.55\r## 3 M 28.4 1.45\r## 4 M 33.6 1.53\r## 5 M 32.9 1.52\rcbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%\r dplyr::mutate(rankPheno = min_rank(desc(phenotype))) %\u0026gt;%\r dplyr::arrange(rankPheno) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 3\r## sex phenotype rankPheno\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 M 82.1 1\r## 2 F 80.0 2\r## 3 M 79.1 3\r## 4 F 78.9 4\r## 5 F 78.6 5\rcbp %\u0026gt;%\r dplyr::mutate(dplyr::across(!phenotype, as.factor))%\u0026gt;% \r dplyr::mutate(phenotype = NULL) %\u0026gt;%\r slice_head(n=5)\r## # A tibble: 5 √ó 6\r## ind father mother year sex herd ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M E ## 2 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M B ## 3 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A ## 4 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M D ## 5 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A\rcbp %\u0026gt;% select(sex, herd, phenotype) %\u0026gt;%\r dplyr::mutate(herdSex = sex:herd) %\u0026gt;%\r relocate(herdSex, .before = phenotype) %\u0026gt;% \r slice_head(n=5)\r## # A tibble: 5 √ó 4\r## sex herd herdSex phenotype\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 M E M:E 37.7\r## 2 M B M:B 35.8\r## 3 M A M:A 28.4\r## 4 M D M:D 33.6\r## 5 M A M:A 32.9\r\rTransmute\rcbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%\r dplyr::transmute(logPheno = log10(phenotype)) %\u0026gt;% \r slice_head(n=5)\r## # A tibble: 5 √ó 1\r## logPheno\r## \u0026lt;dbl\u0026gt;\r## 1 1.58\r## 2 1.55\r## 3 1.45\r## 4 1.53\r## 5 1.52\rcbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%\r dplyr::transmute(rankPheno = min_rank(desc(phenotype))) %\u0026gt;%\r dplyr::arrange(rankPheno) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 1\r## rankPheno\r## \u0026lt;int\u0026gt;\r## 1 1\r## 2 2\r## 3 3\r## 4 4\r## 5 5\rcbp %\u0026gt;%\r dplyr::transmute(dplyr::across(!phenotype, as.factor))%\u0026gt;% \r dplyr::mutate(phenotype = NULL) %\u0026gt;%\r slice_head(n=5)\r## # A tibble: 5 √ó 6\r## ind father mother year sex herd ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M E ## 2 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M B ## 3 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A ## 4 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M D ## 5 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A\rcbp %\u0026gt;% select(sex, herd, phenotype) %\u0026gt;%\r dplyr::transmute(herdSex = sex:herd) %\u0026gt;%\r slice_head(n=5)\r## # A tibble: 5 √ó 1\r## herdSex\r## \u0026lt;fct\u0026gt; ## 1 M:E ## 2 M:B ## 3 M:A ## 4 M:D ## 5 M:A\r\rAcross\rcbp %\u0026gt;%\r dplyr::mutate(across(phenotype, round, 2))\r## # A tibble: 10,000 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A ## 6 6 NA NA 0 M 31.6 A ## 7 7 NA NA 0 M 38.8 A ## 8 8 NA NA 0 M 33.3 E ## 9 9 NA NA 0 M 39.0 E ## 10 10 NA NA 0 M 46.0 C ## # ‚Ä¶ with 9,990 more rows\r# Centering variable\rfun \u0026lt;- function(x, na.rm = TRUE){\r x - mean(x, na.rm = na.rm)\r}\rcbp %\u0026gt;%\r dplyr::mutate(year2 = as.numeric(year)-1) %\u0026gt;%\r dplyr::mutate(across(c(year2, phenotype), fun))\r## # A tibble: 10,000 √ó 8\r## ind father mother year sex phenotype herd year2\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 NA NA 0 M -12.3 E -4.5\r## 2 2 NA NA 0 M -14.2 B -4.5\r## 3 3 NA NA 0 M -21.6 A -4.5\r## 4 4 NA NA 0 M -16.4 D -4.5\r## 5 5 NA NA 0 M -17.1 A -4.5\r## 6 6 NA NA 0 M -18.4 A -4.5\r## 7 7 NA NA 0 M -11.2 A -4.5\r## 8 8 NA NA 0 M -16.7 E -4.5\r## 9 9 NA NA 0 M -11.0 E -4.5\r## 10 10 NA NA 0 M -4.02 C -4.5\r## # ‚Ä¶ with 9,990 more rows\r\rRename\rcbp %\u0026gt;%\r rename(Pheno = phenotype) %\u0026gt;% slice_head(n=5)\r## # A tibble: 5 √ó 7\r## ind father mother year sex Pheno herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A\rcbp %\u0026gt;%\r rename_with(~(toupper(gsub(\u0026quot;r\u0026quot;, \u0026quot;r2\u0026quot;, .x, fixed = TRUE)))) %\u0026gt;%\r slice_head(n=5)\r## # A tibble: 5 √ó 7\r## IND FATHER2 MOTHER2 YEAR2 SEX PHENOTYPE HER2D\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A\r\rGroup by\r# Centering variable\rfun \u0026lt;- function(x, na.rm = TRUE){\r x - mean(x, na.rm = na.rm)\r}\rcbp %\u0026gt;%\r group_by(herd) %\u0026gt;%\r dplyr::mutate(year2 = as.numeric(year)-1) %\u0026gt;%\r dplyr::mutate(across(c(year2, phenotype), fun))\r## # A tibble: 10,000 √ó 8\r## # Groups: herd [5]\r## ind father mother year sex phenotype herd year2\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 NA NA 0 M -12.4 E -4.5\r## 2 2 NA NA 0 M -14.2 B -4.5\r## 3 3 NA NA 0 M -21.6 A -4.5\r## 4 4 NA NA 0 M -16.2 D -4.5\r## 5 5 NA NA 0 M -17.2 A -4.5\r## 6 6 NA NA 0 M -18.4 A -4.5\r## 7 7 NA NA 0 M -11.2 A -4.5\r## 8 8 NA NA 0 M -16.8 E -4.5\r## 9 9 NA NA 0 M -11.1 E -4.5\r## 10 10 NA NA 0 M -4.11 C -4.5\r## # ‚Ä¶ with 9,990 more rows\r\r\rStatistics Summary\rSummarise\rs1 \u0026lt;- cbp %\u0026gt;%\r group_by(year, sex, herd) %\u0026gt;%\r summarise(\r mean = mean(phenotype),\r median = median(phenotype),\r min = min(phenotype),\r max = max(phenotype),\r IQR = IQR(phenotype),\r sd = sd(phenotype),\r var = sd^2,\r n = n()\r ) %\u0026gt;%\r dplyr::mutate(across(where(is.numeric), round,1))\r\rs1 %\u0026gt;%\r ggplot(aes(y=mean, x =year)) +\r geom_point(aes(shape =herd, colour = herd)) +\r facet_wrap(~sex) +\r labs(x = \u0026quot;Generation\u0026quot;, y = \u0026quot;Milk Yield (kg/day\u0026quot;,\r colour = \u0026quot;Herd\u0026quot;, shape = \u0026quot;Herd\u0026quot;) +\r theme_bw(base_size = 13)\rggsave(\u0026quot;milk.pdf\u0026quot;, width = 5, height = 3)\rs1 %\u0026gt;%\r filter(year %in% c(\u0026quot;8\u0026quot;,\u0026quot;9\u0026quot;) \u0026amp; \r max \u0026gt; 76 \u0026amp; min \u0026gt;40)\r## # A tibble: 7 √ó 11\r## # Groups: year, sex [4]\r## year sex herd mean median min max IQR sd var n\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 8 F B 58.3 57.9 41.7 78.6 8.9 6.6 43.2 114\r## 2 8 M C 59.7 59.9 45.3 77.6 7.3 6 35.8 108\r## 3 8 M D 59.2 59.1 45.1 76.7 7.7 6.3 39.5 106\r## 4 9 F E 59.6 60.1 42.9 77.4 7.5 6.3 40.3 99\r## 5 9 M B 61.1 60.7 45 79.1 11.1 7.2 51.9 86\r## 6 9 M C 61.2 61 42.4 82.1 8.5 6.5 41.8 108\r## 7 9 M E 60.8 59.6 45.1 77.7 8.4 6.3 39.9 101\rs1 %\u0026gt;%\r filter(year %in% c(\u0026quot;8\u0026quot;,\u0026quot;9\u0026quot;) \u0026amp; \r max \u0026gt; 76 \u0026amp; min \u0026gt;40) %\u0026gt;%\r transmute(CV = sd / mean, \r CV = scales::percent(CV)) %\u0026gt;%\r dplyr::mutate(across(where(is.numeric), round,1))\r## # A tibble: 7 √ó 3\r## # Groups: year, sex [4]\r## year sex CV ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 8 F 11% ## 2 8 M 10.05%\r## 3 8 M 10.64%\r## 4 9 F 11% ## 5 9 M 11.78%\r## 6 9 M 10.62%\r## 7 9 M 10.36%\r\r\r","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"b25b1e0ebe7029da45a741031abf4603","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/oliveiratp_tidyverse_slidescode/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/courses/2021_ddbg/oliveiratp_tidyverse_slidescode/","section":"courses","summary":"pre  code.sourceCode { white-space: pre; position: relative; }\rpre  code.sourceCode  span { display: inline-block; line-height: 1.25; }\rpre  code.sourceCode  span:empty { height: 1.2em; }\r.","tags":null,"title":"Tidyverse: R code","type":"courses"},{"authors":null,"categories":null,"content":"\r\rpre  code.sourceCode { white-space: pre; position: relative; }\rpre  code.sourceCode  span { display: inline-block; line-height: 1.25; }\rpre  code.sourceCode  span:empty { height: 1.2em; }\r.sourceCode { overflow: visible; }\rcode.sourceCode  span { color: inherit; text-decoration: inherit; }\rdiv.sourceCode { margin: 1em 0; }\rpre.sourceCode { margin: 0; }\r@media screen {\rdiv.sourceCode { overflow: auto; }\r}\r@media print {\rpre  code.sourceCode { white-space: pre-wrap; }\rpre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; }\r}\rpre.numberSource code\r{ counter-reset: source-line 0; }\rpre.numberSource code  span\r{ position: relative; left: -4em; counter-increment: source-line; }\rpre.numberSource code  span  a:first-child::before\r{ content: counter(source-line);\rposition: relative; left: -1em; text-align: right; vertical-align: baseline;\rborder: none; display: inline-block;\r-webkit-touch-callout: none; -webkit-user-select: none;\r-khtml-user-select: none; -moz-user-select: none;\r-ms-user-select: none; user-select: none;\rpadding: 0 4px; width: 4em;\rbackground-color: #ffffff;\rcolor: #a0a0a0;\r}\rpre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0; padding-left: 4px; }\rdiv.sourceCode\r{ color: #1f1c1b; background-color: #ffffff; }\r@media screen {\rpre  code.sourceCode  span  a:first-child::before { text-decoration: underline; }\r}\rcode span { color: #1f1c1b; } /* Normal */\rcode span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */\rcode span.an { color: #ca60ca; } /* Annotation */\rcode span.at { color: #0057ae; } /* Attribute */\rcode span.bn { color: #b08000; } /* BaseN */\rcode span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */\rcode span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */\rcode span.ch { color: #924c9d; } /* Char */\rcode span.cn { color: #aa5500; } /* Constant */\rcode span.co { color: #898887; } /* Comment */\rcode span.cv { color: #0095ff; } /* CommentVar */\rcode span.do { color: #607880; } /* Documentation */\rcode span.dt { color: #0057ae; } /* DataType */\rcode span.dv { color: #b08000; } /* DecVal */\rcode span.er { color: #bf0303; text-decoration: underline; } /* Error */\rcode span.ex { color: #0095ff; font-weight: bold; } /* Extension */\rcode span.fl { color: #b08000; } /* Float */\rcode span.fu { color: #644a9b; } /* Function */\rcode span.im { color: #ff5500; } /* Import */\rcode span.in { color: #b08000; } /* Information */\rcode span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */\rcode span.op { color: #1f1c1b; } /* Operator */\rcode span.ot { color: #006e28; } /* Other */\rcode span.pp { color: #006e28; } /* Preprocessor */\rcode span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */\rcode span.sc { color: #3daee9; } /* SpecialChar */\rcode span.ss { color: #ff5500; } /* SpecialString */\rcode span.st { color: #bf0303; } /* String */\rcode span.va { color: #0057ae; } /* Variable */\rcode span.vs { color: #bf0303; } /* VerbatimString */\rcode span.wa { color: #bf0303; } /* Warning */\r\r\rTidy data\r\rExercise 1\rExercise 2\r\rExercise 3\rData Structure\r\rExercise 1\rExercise 2\rExercise 3\r\r\r\rTidy data\rTidy data refers to data arranged to make data processing, analysis, and visualization simpler. Remember that in a tidy data set we should consider:\n\rEach variable must have its column.\rEach observation must have its row.\rEach value must have its cell.\r\rExercise 1\rLet‚Äôs say we want to organize the data anscombe. Below I shpw how this data looks like:\nanscombe\r## x1 x2 x3 x4 y1 y2 y3 y4\r## 1 10 10 10 8 8.04 9.14 7.46 6.58\r## 2 8 8 8 8 6.95 8.14 6.77 5.76\r## 3 13 13 13 8 7.58 8.74 12.74 7.71\r## 4 9 9 9 8 8.81 8.77 7.11 8.84\r## 5 11 11 11 8 8.33 9.26 7.81 8.47\r## 6 14 14 14 8 9.96 8.10 8.84 7.04\r## 7 6 6 6 8 7.24 6.13 6.08 5.25\r## 8 4 4 4 19 4.26 3.10 5.39 12.50\r## 9 12 12 12 8 10.84 9.13 8.15 5.56\r## 10 7 7 7 8 4.82 7.26 6.42 7.91\r## 11 5 5 5 8 5.68 4.74 5.73 6.89\rOrganize this data set to obtain tidy data. Remember here we have two response variables been measured four times.\r\rMost of the selecting, separating, mutating and renaming is taking place within the pivot function calls.\n(ex1 \u0026lt;- anscombe %\u0026gt;%\r pivot_longer(everything(),\r names_to = c(\u0026quot;.value\u0026quot;, \u0026quot;rep\u0026quot;),\r names_pattern = \u0026quot;(.)([0-9])\u0026quot;\r ))\r## # A tibble: 44 √ó 3\r## rep x y\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 10 8.04\r## 2 2 10 9.14\r## 3 3 10 7.46\r## 4 4 8 6.58\r## 5 1 8 6.95\r## 6 2 8 8.14\r## 7 3 8 6.77\r## 8 4 8 5.76\r## 9 1 13 7.58\r## 10 2 13 8.74\r## # ‚Ä¶ with 34 more rows\rFilter the data set to get replications 2 and 4, and summarise it to\rget the maximum, minimum, and mean values.\r\rex1 %\u0026gt;%\r filter(rep %in% c(2,4)) %\u0026gt;%\r summarise(\r across(c(x,y), list(mean = mean, min = min, max = max),\r .names = \u0026quot;{.col}.{.fn}\u0026quot;\r ))\r## # A tibble: 1 √ó 6\r## x.mean x.min x.max y.mean y.min y.max\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 9 4 19 7.50 3.1 12.5\r\rExercise 2\rOften you do not need the entire data set, but just part of it.\nHere, you should make the data mtcars tidy before making any selection.\r\r(dataEx3 \u0026lt;- readRDS(\u0026quot;./data/dataEx3.rds\u0026quot;))\r## # A tibble: 32 √ó 12\r## cyl disp hp drat wt qsec vs am carb `4` `3` `5`\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 6 160 110 3.9 2.62 16.5 0 1 4 21 NA NA\r## 2 6 160 110 3.9 2.88 17.0 0 1 4 21 NA NA\r## 3 4 108 93 3.85 2.32 18.6 1 1 1 22.8 NA NA\r## 4 6 258 110 3.08 3.22 19.4 1 0 1 NA 21.4 NA\r## 5 8 360 175 3.15 3.44 17.0 0 0 2 NA 18.7 NA\r## 6 6 225 105 2.76 3.46 20.2 1 0 1 NA 18.1 NA\r## 7 8 360 245 3.21 3.57 15.8 0 0 4 NA 14.3 NA\r## 8 4 147. 62 3.69 3.19 20 1 0 2 24.4 NA NA\r## 9 4 141. 95 3.92 3.15 22.9 1 0 2 22.8 NA NA\r## 10 6 168. 123 3.92 3.44 18.3 1 0 4 19.2 NA NA\r## # ‚Ä¶ with 22 more rows\rAs you can see, some columns are not variable names but values. Create two new variables calling mpg (for observations) and gear (with column values).\ndataEx3 \u0026lt;- dataEx3 %\u0026gt;%\r pivot_longer(\r cols = matches(\u0026quot;([1-9])\u0026quot;),\r names_to = \u0026quot;gear\u0026quot;,\r values_to = \u0026quot;mpg\u0026quot;,\r values_drop_na = TRUE\r )\rSelect the columns mpg, hp, gear, and carb, and then make a plot using ggplot2 where mpg is the response variable, and hp is the co-variate in the x-axis. Also include different shapes and colours for gear, and facets for carb.\r\rdataEx3 %\u0026gt;%\r select(mpg, hp, gear, carb) %\u0026gt;%\r ggplot(aes(y=mpg, x = hp, shape = gear,\r colour = gear)) +\r geom_point() +\r facet_wrap(~carb) +\r theme_bw()\r\r\rExercise 3\rThe following data represents song rankings for Billboard top 100 in the year 2000. The rank of the song is displayed in each week after it entered.\nbillboard\r## # A tibble: 317 √ó 79\r## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2 Pac Baby D‚Ä¶ 2000-02-26 87 82 72 77 87 94 99 NA\r## 2 2Ge+her The Ha‚Ä¶ 2000-09-02 91 87 92 NA NA NA NA NA\r## 3 3 Doors‚Ä¶ Krypto‚Ä¶ 2000-04-08 81 70 68 67 66 57 54 53\r## 4 3 Doors‚Ä¶ Loser 2000-10-21 76 76 72 69 67 65 55 59\r## 5 504 Boyz Wobble‚Ä¶ 2000-04-15 57 34 25 17 17 31 36 49\r## 6 98^0 Give M‚Ä¶ 2000-08-19 51 39 34 26 26 19 2 2\r## 7 A*Teens Dancin‚Ä¶ 2000-07-08 97 97 96 95 100 NA NA NA\r## 8 Aaliyah I Don\u0026#39;‚Ä¶ 2000-01-29 84 62 51 41 38 35 35 38\r## 9 Aaliyah Try Ag‚Ä¶ 2000-03-18 59 53 38 28 21 18 16 14\r## 10 Adams, ‚Ä¶ Open M‚Ä¶ 2000-08-26 76 76 74 69 68 67 61 58\r## # ‚Ä¶ with 307 more rows, and 68 more variables: wk9 \u0026lt;dbl\u0026gt;, wk10 \u0026lt;dbl\u0026gt;,\r## # wk11 \u0026lt;dbl\u0026gt;, wk12 \u0026lt;dbl\u0026gt;, wk13 \u0026lt;dbl\u0026gt;, wk14 \u0026lt;dbl\u0026gt;, wk15 \u0026lt;dbl\u0026gt;, wk16 \u0026lt;dbl\u0026gt;,\r## # wk17 \u0026lt;dbl\u0026gt;, wk18 \u0026lt;dbl\u0026gt;, wk19 \u0026lt;dbl\u0026gt;, wk20 \u0026lt;dbl\u0026gt;, wk21 \u0026lt;dbl\u0026gt;, wk22 \u0026lt;dbl\u0026gt;,\r## # wk23 \u0026lt;dbl\u0026gt;, wk24 \u0026lt;dbl\u0026gt;, wk25 \u0026lt;dbl\u0026gt;, wk26 \u0026lt;dbl\u0026gt;, wk27 \u0026lt;dbl\u0026gt;, wk28 \u0026lt;dbl\u0026gt;,\r## # wk29 \u0026lt;dbl\u0026gt;, wk30 \u0026lt;dbl\u0026gt;, wk31 \u0026lt;dbl\u0026gt;, wk32 \u0026lt;dbl\u0026gt;, wk33 \u0026lt;dbl\u0026gt;, wk34 \u0026lt;dbl\u0026gt;,\r## # wk35 \u0026lt;dbl\u0026gt;, wk36 \u0026lt;dbl\u0026gt;, wk37 \u0026lt;dbl\u0026gt;, wk38 \u0026lt;dbl\u0026gt;, wk39 \u0026lt;dbl\u0026gt;, wk40 \u0026lt;dbl\u0026gt;,\r## # wk41 \u0026lt;dbl\u0026gt;, wk42 \u0026lt;dbl\u0026gt;, wk43 \u0026lt;dbl\u0026gt;, wk44 \u0026lt;dbl\u0026gt;, wk45 \u0026lt;dbl\u0026gt;, wk46 \u0026lt;dbl\u0026gt;, ‚Ä¶\rA slightly more complex case where columns have a common prefix and missing missings are structural, so should be dropped. So, make this data tidy.\nbillboard %\u0026gt;%\r pivot_longer(\r cols = starts_with(\u0026quot;wk\u0026quot;),\r names_to = \u0026quot;week\u0026quot;,\r names_prefix = \u0026quot;wk\u0026quot;,\r values_to = \u0026quot;rank\u0026quot;,\r values_drop_na = TRUE\r )\r\rData Structure\rExercise 1\rMake this data tidy by including tmin and tmax as variable. Remember that here type is carrying to variables names rather than factors.\r\r(dataEx2 \u0026lt;- as_tibble(readRDS(\u0026quot;./data/dataEx2.RDS\u0026quot;)))\r## # A tibble: 20 √ó 4\r## id date type value\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Ind 1 2020-01-01 tmin 35.8\r## 2 Ind 1 2020-01-01 tmax 36.7\r## 3 Ind 1 2020-01-02 tmin 35.9\r## 4 Ind 1 2020-01-02 tmax 36.8\r## 5 Ind 1 2020-01-03 tmin 36.0\r## 6 Ind 1 2020-01-03 tmax 36.8\r## 7 Ind 1 2020-01-04 tmin 36.1\r## 8 Ind 1 2020-01-04 tmax 36.9\r## 9 Ind 1 2020-01-05 tmin 36.1\r## 10 Ind 1 2020-01-05 tmax 36.9\r## 11 Ind 1 2020-01-06 tmin 36.2\r## 12 Ind 1 2020-01-06 tmax 37.0\r## 13 Ind 1 2020-01-07 tmin 36.3\r## 14 Ind 1 2020-01-07 tmax 37.1\r## 15 Ind 1 2020-01-08 tmin 36.3\r## 16 Ind 1 2020-01-08 tmax 37.2\r## 17 Ind 1 2020-01-09 tmin 36.6\r## 18 Ind 1 2020-01-09 tmax 37.3\r## 19 Ind 1 2020-01-10 tmin 36.6\r## 20 Ind 1 2020-01-10 tmax 38.0\r(dataEx2 \u0026lt;- dataEx2 %\u0026gt;%\r pivot_wider(values_from = value, names_from = type))\r## # A tibble: 10 √ó 4\r## id date tmin tmax\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Ind 1 2020-01-01 35.8 36.7\r## 2 Ind 1 2020-01-02 35.9 36.8\r## 3 Ind 1 2020-01-03 36.0 36.8\r## 4 Ind 1 2020-01-04 36.1 36.9\r## 5 Ind 1 2020-01-05 36.1 36.9\r## 6 Ind 1 2020-01-06 36.2 37.0\r## 7 Ind 1 2020-01-07 36.3 37.1\r## 8 Ind 1 2020-01-08 36.3 37.2\r## 9 Ind 1 2020-01-09 36.6 37.3\r## 10 Ind 1 2020-01-10 36.6 38.0\rNow, build a new variable called tdiff, which is the difference between tmax and tmin. Moreover, display a ggplot2 graph that shows tdiff over time.\ndataEx2 %\u0026gt;%\r dplyr::mutate(tdiff = tmax - tmin) %\u0026gt;%\r ggplot(aes(y = tdiff, x = date)) +\r geom_point() +\r geom_smooth(se = FALSE) +\r ylab(\u0026quot;tmax - tmin\u0026quot;) + xlab (\u0026quot;Date\u0026quot;) +\r theme_classic()\r\rExercise 2\rOur cattle data data is already in a tidy format.\n(cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;))\r## # A tibble: 10,000 √ó 7\r## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A ## 6 6 NA NA 0 M 31.6 A ## 7 7 NA NA 0 M 38.8 A ## 8 8 NA NA 0 M 33.3 E ## 9 9 NA NA 0 M 39.0 E ## 10 10 NA NA 0 M 46.0 C ## # ‚Ä¶ with 9,990 more rows\rFor this exercise, complete the following tasks with that data set:\nCalculate the average phenotype per year by sex and herd using the summarise() function in the dplyr package.\rAdd two columns to cattle data using the mutate() function:\rColumn 1: Phenotype should be rescaled to have a mean of zero and a standard deviation of one. You can call this new variable as PhenoStd.\rColumn 2: Rank the PhenoStd using the function min_rank().\rThe output data frame should have only PhenoStd \u0026gt; 0.\r\r\rcbp %\u0026gt;%\r group_by(year, sex, herd) %\u0026gt;%\r summarise(\r mean = mean(phenotype)\r )\r## # A tibble: 100 √ó 4\r## # Groups: year, sex [20]\r## year sex herd mean\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 F A 40.3\r## 2 0 F B 40.6\r## 3 0 F C 40.3\r## 4 0 F D 39.1\r## 5 0 F E 40.1\r## 6 0 M A 41.0\r## 7 0 M B 40.9\r## 8 0 M C 40.7\r## 9 0 M D 39.9\r## 10 0 M E 40.4\r## # ‚Ä¶ with 90 more rows\r.scale \u0026lt;- function(x){\r (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)\r}\r\rcbp %\u0026gt;%\r dplyr::mutate(PhenoStd = .scale(phenotype)) %\u0026gt;%\r dplyr::mutate(RankPhenoStd = min_rank(PhenoStd)) %\u0026gt;%\r filter(PhenoStd \u0026gt; 0)\r## # A tibble: 5,057 √ó 9\r## ind father mother year sex phenotype herd PhenoStd RankPhenoStd\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 11 NA NA 0 M 51.0 A 0.103 5338\r## 2 23 NA NA 0 M 51.9 E 0.191 5661\r## 3 29 NA NA 0 M 53.5 C 0.355 6258\r## 4 34 NA NA 0 M 50.1 D 0.00987 4981\r## 5 61 NA NA 0 M 53.6 C 0.366 6306\r## 6 65 NA NA 0 M 55.2 A 0.529 6883\r## 7 69 NA NA 0 M 51.3 C 0.135 5461\r## 8 84 NA NA 0 M 52.0 A 0.207 5726\r## 9 90 NA NA 0 M 50.5 B 0.0506 5144\r## 10 106 NA NA 0 M 52.1 C 0.209 5731\r## # ‚Ä¶ with 5,047 more rows\r\rExercise 3\rExcerpt of the Gapminder data on life expectancy, GDP per capita, and population by country. This data has 142 countries observed from the year 1952 to 2007 in increments of 5 years. The response variable observed was the life expectancy at birth (in years), population size, and Per capita gross domestic product (GDP).\nPer capita gross domestic product (GDP) measures a country‚Äôs economic response per person and is calculated by dividing its GDP by its population. It is a global measure for gauging the prosperity of nations as we can analyze the worth of a country based on its economic growth. Thus, countries that have the highest per capita GDP tend to be more developed.\ngapminder\r## # A tibble: 1,704 √ó 6\r## country continent year lifeExp pop gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan Asia 1952 28.8 8425333 779.\r## 2 Afghanistan Asia 1957 30.3 9240934 821.\r## 3 Afghanistan Asia 1962 32.0 10267083 853.\r## 4 Afghanistan Asia 1967 34.0 11537966 836.\r## 5 Afghanistan Asia 1972 36.1 13079460 740.\r## 6 Afghanistan Asia 1977 38.4 14880372 786.\r## 7 Afghanistan Asia 1982 39.9 12881816 978.\r## 8 Afghanistan Asia 1987 40.8 13867957 852.\r## 9 Afghanistan Asia 1992 41.7 16317921 649.\r## 10 Afghanistan Asia 1997 41.8 22227415 635.\r## # ‚Ä¶ with 1,694 more rows\rQuestions:\nWhat are the ten highest gdpPercap values?\r\rgapminder %\u0026gt;%\r slice_max(gdpPercap, n = 10)\r## # A tibble: 10 √ó 6\r## country continent year lifeExp pop gdpPercap\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Kuwait Asia 1957 58.0 212846 113523.\r## 2 Kuwait Asia 1972 67.7 841934 109348.\r## 3 Kuwait Asia 1952 55.6 160000 108382.\r## 4 Kuwait Asia 1962 60.5 358266 95458.\r## 5 Kuwait Asia 1967 64.6 575003 80895.\r## 6 Kuwait Asia 1977 69.3 1140357 59265.\r## 7 Norway Europe 2007 80.2 4627926 49357.\r## 8 Kuwait Asia 2007 77.6 2505559 47307.\r## 9 Singapore Asia 2007 80.0 4553009 47143.\r## 10 Norway Europe 2002 79.0 4535591 44684.\rFind both the median life expectancy (lifeExp) and the median and maximum GDP per capita (gdpPercap) in 1957, 1982, and 2007, by country and continent. Call them medianLifeExp, medianGdpPercap, and maxGdpPercap, respectively.\r\r(dat \u0026lt;- gapminder %\u0026gt;%\r filter(year %in% c(\u0026quot;1957\u0026quot;,\u0026quot;1982\u0026quot;,\u0026quot;2007\u0026quot;)) %\u0026gt;%\r group_by(year, country, continent) %\u0026gt;%\r summarise(\r medianlifeExp = median(lifeExp),\r medianGdpPercap = median(gdpPercap),\r maxGdpPercap = max(gdpPercap)\r ))\r## # A tibble: 426 √ó 6\r## # Groups: year, country [426]\r## year country continent medianlifeExp medianGdpPercap maxGdpPercap\r## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1957 Afghanistan Asia 30.3 821. 821.\r## 2 1957 Albania Europe 59.3 1942. 1942.\r## 3 1957 Algeria Africa 45.7 3014. 3014.\r## 4 1957 Angola Africa 32.0 3828. 3828.\r## 5 1957 Argentina Americas 64.4 6857. 6857.\r## 6 1957 Australia Oceania 70.3 10950. 10950.\r## 7 1957 Austria Europe 67.5 8843. 8843.\r## 8 1957 Bahrain Asia 53.8 11636. 11636.\r## 9 1957 Bangladesh Asia 39.3 662. 662.\r## 10 1957 Belgium Europe 69.2 9715. 9715.\r## # ‚Ä¶ with 416 more rows\rUse a scatter plot to compare the median GDP and median life expectancy. Use the variables continent and year to produce this plot.\r\rdat %\u0026gt;%\r ggplot(aes(x = medianGdpPercap, y = medianlifeExp)) +\r facet_wrap(~ continent) +\r geom_point(shape = 1)\r\r\r","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"4550a78f519351e225018b836c431d9c","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/oliveiratp_tidyverse_answers/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/courses/2021_ddbg/oliveiratp_tidyverse_answers/","section":"courses","summary":"pre  code.sourceCode { white-space: pre; position: relative; }\rpre  code.sourceCode  span { display: inline-block; line-height: 1.25; }\rpre  code.sourceCode  span:empty { height: 1.2em; }\r.","tags":null,"title":"Tidyverse: answers","type":"courses"},{"authors":["Thiago de Paula Oliveira"],"categories":["Statistics","Statistical Models"],"content":"\rThe ability to accurately model and interpret complex data sets is paramount. This technical exploration delves into three sophisticated modelling techniques:\r\rPolynomial Models,\rFractional Polynomials, and\rSpline Models.\r\rEach of these models serves as a fundamental tool in the statistical toolkit, enabling us to capture and understand the intricacies of linear and non-linear relationships inherent in real-world data.\rAs a bio-statistician entrenched in the technical aspects of data analysis, I recognize the critical importance of these models. We will commence with an examination of Polynomial Models, discussing their mathematical underpinnings and practical applications in capturing curvilinear trends. Next, we will navigate through the Fractional Polynomials, a more flexible extension of traditional polynomials, adept at modelling asymmetric patterns. Lastly, we will explore Spline Models, one of the most flexible approach in data fitting, capable of adapting to complex and segmented patterns in data.\rThis post is designed not just to inform but to provide a technical understanding of these models with examples, illustrating their relevance and application in contemporary data analysis. Whether you are a data scientist, a statistician, or a researcher grappling with complex data sets, this exploration aims to enhance the modelling arsenal, offering insights into when and how to apply these models effectively.\rPolynomial Models\rPolynomial Models, represented by functions of the form \\[y = a_n x^n + a_{n-1} x^{n-1} + \\ldots + a_1 x + a_0,\\] are foundational in modelling curvilinear relationships. In this formulation, each \\(a_i\\) (where \\(i = 0, 1, \\ldots, n\\)) denotes the coefficient corresponding to the \\(i\\)-th term of the polynomial, and \\(x\\) is the independent or exploratory variable. The degree \\(n\\) of the polynomial determines the model‚Äôs complexity, with higher degrees allowing for more intricate curve shapes.\rThese models are particularly useful in capturing the non-linear dynamics often observed in real-world data. For instance, a quadratic model (where \\(n = 2\\)) can describe simple parabolic trends, while higher-degree models, such as cubic (\\(n = 3\\)) or quartic (\\(n = 4\\)), enable the representation of more complex and varied behaviours (Figure 1).\r\rFigure 1: Comparative Visualization of Polynomial Fits: Degrees 1 to 4\r\rHowever, increasing the degree \\(n\\) also increases the risk of overfitting, a phenomenon where the model adapts too closely to the specificities of the training data, including noise, at the expense of generalizability to new data (Figure 2). Overfitting leads to models that perform poorly in predictive scenarios, failing to capture the true underlying trend.\r\rFigure 2: Visualizing Model Complexity: Parsimonious vs Overfit Polynomial Fits\r\rPolynomial models are extensively applied across various disciplines. In physics, they are instrumental in modelling motion under uniform acceleration, among other phenomena. In economics, polynomial trends are fitted to time series data to understand market dynamics. In biological sciences, these models aid in interpreting growth rates and simple gene expression patterns. The interpretation of the coefficients \\(a_i\\) can provide significant insights; for instance, in the quadratic model \\(y = ax^2 + bx + c\\), the sign of \\(a\\) determines the direction in which the parabola opens, offering crucial information about the nature of the relationship being modelled (Figure 3).\r\rFigure 3: Parabola Opening Upwards and Downwards\r\rIn practical applications, the selection of the polynomial degree \\(n\\) is critical. It is a balance between capturing the complexity of the data and avoiding overfitting (Figure 2). Techniques such as cross-validation, where the data is divided into training and testing sets, can be used to determine the optimal degree of the polynomial. Additionally, statistical measures like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) are often employed to select the most appropriate model by balancing model fit and complexity.\rIn summary, Polynomial Models are a versatile and powerful tool in statistical modelling. Their ability to approximate complex functions with a relatively straightforward mathematical formulation makes them a fundamental component in various fields of data analysis.\rWhile traditional polynomial models are highly effective, they sometimes lack the flexibility required for certain types of data, particularly those exhibiting asymmetric trends. This limitation led to the development of fractional polynomial models, which extend the concept of polynomial models by allowing for fractional exponents. This advancement provides a greater ability to fit a wider range of curves and is especially useful in cases where the relationship between variables is not adequately captured by integer exponents.\r\rFractional Polynomial Models\rFractional Polynomials represent an advanced evolution of traditional polynomial models, marked by their use of non-integer, real-number exponents in the independent variable. Mathematically, they are expressed as \\[y = \\beta_0 + \\beta_1 x^{p_1} + \\beta_2 x^{p_2} + \\ldots + \\beta_n x^{p_n},\\] where \\(x\\) is the independent variable, \\(\\beta_i\\) are coefficients, and \\(p_i\\) are the variable powers. These powers, unlike the integer-only powers in traditional polynomials, can include any real number like \\(0.5\\), \\(-1\\), or \\(2.3\\). This flexibility significantly broadens the modeling capability of polynomials, allowing for more precise fitting to complex and asymmetric data patterns. Terms such as \\(x^{-1}\\) and \\(x^{0.5}\\), representing the reciprocal and square root of \\(x\\) respectively, enable the modeling of relationships that exhibit dramatic changes over different ranges of \\(x\\), a task challenging for standard polynomial models with integer exponents.\r\rFigure 4: Comparative Visualization of Fractional Polynomial Fits\r\rThe utility of fractional polynomials extends to various fields, notably in medical statistics and biological data analysis, where data patterns often defy symmetry. They are particularly adept in modeling phenomena like dose-response curves in pharmacokinetics and progression rates of diseases, where the response changes in a non-linear fashion. Such flexibility makes them invaluable in scenarios where data exhibit complex, non-standard behaviors.\rHowever, the complexity of fractional polynomials can pose interpretational challenges, and like their traditional counterparts, they are susceptible to overfitting. This risk is heightened with the inclusion of multiple fractional terms or higher degrees, necessitating careful model selection and validation processes. Methods such as cross-validation or the use of information criteria like AIC or BIC are often employed to balance model fit against the risk of overfitting.\rFinding Optimal Power Values in Fractional Polynomials\rThe process of finding the best values for \\(p_1, p_2, \\ldots, p_n\\) is inherently iterative and may require a combination of statistical testing, validation techniques, and expert judgement. The goal is to have a balance between a model that fits the data well, is not overly complex, and is robust to variations in model parameters.\rIn Figure 5, I exemplify this process by fitting various fractional polynomial models to the dataset. Through the application of the Bayesian Information Criterion (BIC), I identified the most parsimonious model, which is succinctly expressed mathematically as \\[y = \\beta_0 + \\beta_1 x^{1.13},\\] and the corresponding BIC value for this model is denoted as \\(‚àí144.34\\).\r\rFigure 5: Comparative Visualization of Fractional Polynomial Fits\r\rBelow I have described some possible approaches to achieve this objective:\rInitial Power Selection: Start with a set of candidate powers, often including a mix of positive, negative, and fractional values. Common choices are \\(-2, -1, -0.5, 0, 0.5, 1, 2, 3\\). The selection of these initial powers is guided by prior knowledge about the data, theoretical considerations, or exploratory analysis.\rModel Fitting and Comparison: Fit fractional polynomial models using different combinations of these candidate powers. This fitting can be done using least squares regression or other suitable methods depending on the nature of the data. For each model, compute a goodness-of-fit statistic, such as the residual sum of squares (RSS) or the Akaike Information Criterion (AIC).\rIterative Testing: Employ an iterative approach to test various combinations of powers. This might involve starting with a simple model and gradually adding complexity (increasing the number of terms) while monitoring the improvement in the fit.\rCross-Validation: To guard against overfitting, especially in models with higher degrees or more terms, use cross-validation. Divide your data into training and testing sets. Fit the model to the training set and evaluate its performance on the testing set. This step helps in assessing the model‚Äôs predictive accuracy.\rStatistical Significance: Assess the statistical significance of the coefficients associated with each term in the model. Non-significant terms might suggest that certain powers do not contribute meaningfully to the model and could be excluded.\rModel Selection Criteria: Use model selection criteria like AIC or BIC to compare models with different combinations of powers. These criteria balance the goodness of fit with the complexity of the model, helping to choose a model that is both accurate and parsimonious.\rSensitivity Analysis: Conduct sensitivity analyses by varying the powers slightly to see how robust the model is to changes in these parameters. This step is crucial to understand the stability and reliability of the model.\rWhile fractional polynomials offer enhanced modelling flexibility over traditional polynomials, they sometimes fall short in handling data with distinct behavioural changes across different segments. This limitation summed to the difficulty in determine \\(p_i\\) are where spline models come into play. Spline models, constructed as piecewise polynomials, provide localized fitting capabilities, adapting seamlessly to variations within different data segments. Such adaptability is particularly useful in datasets with distinct phases or regimes. Thus, spline models emerge as a natural progression when fractional polynomials alone are insufficient to model the intricate patterns present in the data\r\r\rSpline Models\r\rReferences\rCarrasco, J. L., King, T. S., \u0026amp; Chinchilli, V. M. (2009). The Concordance Correlation Coefficient for Repeated Measures Estimated by Variance Components. Journal of Biopharmaceutical Statistics, 19, 90-105. DOI: 10.1080/10543400802527890\n\rCitation\rFor attribution, please cite this work as:\r\rOliveira T.P. (2024, Jan.¬†25). Exploring the Technicalities of Data Fitting - Polynomial, Fractional Polynomial, and Spline Models\n\rBibTeX citation\r\r@misc{oliveira2024polynomials,\rauthor = {Oliveira, Thiago},\rtitle = {Exploring the Technicalities of Data Fitting - Polynomial, Fractional Polynomial, and Spline Models},\rurl = {https://prof-thiagooliveira.netlify.app/post/exploring-the-technicalities-of-data-fitting-polynomial-fractional-polynomial-and-spline-models/},\ryear = {2024}\r}\rDid you find this page helpful? Consider sharing it üôå\n\r","date":1705622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705652154,"objectID":"709e88fa8a6bf95f1598f8b2f0b14bd2","permalink":"https://prof-thiagooliveira.netlify.com/post/statistics/","publishdate":"2024-01-19T00:00:00Z","relpermalink":"/post/statistics/","section":"post","summary":"The ability to accurately model and interpret complex data sets is paramount. This technical exploration delves into three sophisticated modelling techniques:\r\rPolynomial Models,\rFractional Polynomials, and\rSpline Models.\r\rEach of these models serves as a fundamental tool in the statistical toolkit, enabling us to capture and understand the intricacies of linear and non-linear relationships inherent in real-world data.","tags":["Splines","Polynomials","Statistical Modelling","Statistics"],"title":"Exploring the Technicalities of Data Fitting","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["Statistics","Concordance"],"content":"\rThe Concordance Correlation Coefficient (CCC) is a statistical measure designed to evaluate the agreement between two sets of measurements, such as those represented by two random variables, \\(X\\) and \\(Y\\). Mathematically, the CCC is defined as:\r\\[\\rho_c = \\frac{2\\sigma_{x,y}}{\\sigma_x^2 + \\sigma_y^2 + (\\mu_x - \\mu_y)^2} = \\frac{2\\rho\\sigma_x\\sigma_y}{\\sigma_x^2 + \\sigma_y^2 + (\\mu_x - \\mu_y)^2} = \\rho \\times C_b.\\]\nIn this formula, \\(\\sigma_x\\) and \\(\\sigma_y\\) are the standard deviations of \\(X\\) and \\(Y\\), respectively, showing the variability within each set of measurements. The means, \\(\\mu_x\\) and \\(\\mu_y\\), represent the central tendency of each dataset. The term \\(\\rho\\) is Pearson‚Äôs correlation coefficient, expressing the linear association between \\(X\\) and \\(Y\\), while \\(C_b\\), the bias correction factor, quantifies the deviation of the best-fit line from the 45-degree line through the origin‚Äîthe line of perfect agreement. A \\(C_b\\) of 1 indicates no bias, and values close to 1 signify an accurate agreement.\rUnderstanding the concepts of precision and accuracy is crucial for grasping the essence of CCC (Concordance Correlation Coefficient). Let‚Äôs break down these components:\rPrecision: This aspect refers to the consistency or repeatability of measurements (Figure 1). Imagine a target with a bullseye:\r\rIf you shoot a series of arrows, and they all land very close to each other but not necessarily near the bullseye, this demonstrates high precision. The arrows are consistently hitting the same spot, showing that your measurements (in this case, arrow shots) are repeatable and reliable.\r\rHowever, high precision does not guarantee that you are hitting the target accurately. Your consistent shots might be clustered in a corner of the target, far from the bullseye.\r\r\rAccuracy: This term refers to how close the measurements are to the ‚Äòtrue‚Äô or accepted value (Figure 1). Continuing with the target analogy:\r\rIf your arrows hit or are very close to the bullseye, this indicates high accuracy. You are hitting the correct or intended spot.\r\rIt‚Äôs possible to be accurate without being precise if your arrows are scattered all around the bullseye, each hitting close but not in a consistent pattern.\r\r\r\r\rFigure 1: Example of precision and accuracy concept. The red dot in the center represents the bullseye.\r\rNow, applying these concepts to CCC. The CCC measures both the precision and accuracy of a set of measurements. It assesses the strength of the relationship between two variables (precision) and how closely these measurements agree with the ‚Äòtrue‚Äô or accepted values (accuracy). A high CCC indicates that not only are the measurements consistent with each other (precision), but they also closely match the true values (accuracy). In our analogy, this would be like consistently hitting the bullseye with every arrow.\rFurthermore, in the context of CCC, the concept of a ‚Äútrue‚Äù value is central. One variable is considered the standard or ‚Äútrue‚Äù measurement against which the other is compared. This distinction is vital because CCC is not merely a correlation but a measure of agreement. The notion of a ‚Äútrue‚Äù value in scientific research is intricate, often defined by theoretical constructs or consensus standards, and the assumption of truth can be challenged or refined with advancing knowledge and technology. Thus, CCC offers a way to quantitatively assess how well our measurements reflect what we accept as true, acknowledging that our understanding of ‚Äútrue‚Äù can evolve.\rTo illustrate, in Scenario 1, even with a Pearson correlation near 1, the CCC‚Äôs modest value signals a discrepancy from the true value, reflecting either a systematic bias or scale differences between the measurements. The dashed red line represents the line of perfect agreement, where the true values would ideally lie. This scenario underscores the necessity of considering both precision and accuracy‚Äîwhere precision alone can mislead, and accuracy is pivotal for measurements to be meaningful and trustworthy.\r\rFigure 2: Scenario 1 - High Pearson correlation with modest CCC indicating possible systematic bias or scale differences. The dashed line represents perfect agreement\r\rScenario 2 presents a contrasting picture with a CCC value of 0.95, indicating not only a strong linear relationship but also a high degree of concordance. In this instance, the measurements not only follow a consistent pattern (precision) but also align closely with the line of perfect agreement (accuracy), suggesting that one set of measurements can be reliably used as a surrogate for the true values.\r\rFigure 3: Scenario 2 - Strong linear relationship and high concordance with CCC at 0.95. The dashed line indicates the line of perfect agreement\r\rIn Scenario 3, we explore a scenario characterized by a negative Pearson correlation coefficient of approximately -0.44 and a Concordance Correlation Coefficient (CCC) of -0.43. This specific case demands a tailored approach to understanding concordance within inverse relationships. Instead of using the \\(Y = X\\) line commonly associated with positive correlations, we introduce a new reference line represented by \\(Y = 100 - X\\) (depicted as the black dashed line in the plot). This reference line possesses a slope of -1 and serves as the benchmark for perfect inverse agreement.\r\rFigure 4: Scenario 3: Inverse relationship with both Pearson correlation and CCC around -0.44. The red dashed line represents \\(Y = X\\), the black dashed line is the \\(Y = 100 - X\\), and the blue solid line is the best fit line\r\rThe noteworthy aspect of this scenario lies in the high \\(C_b\\) value, which is approximately 0.98 (\\(C_b = \\frac{\\rho_c}{\\rho} = \\frac{-0.43}{-0.44}\\)). This value is calculated as the ratio of CCC to the Pearson correlation coefficient. While the negative Pearson correlation may not indicate a high precision, the elevated \\(C_b\\) value implies that \\(Y\\) and \\(X\\) are almost perfectly accurate.\rNow, let‚Äôs take a closer look at an intriguing scenario. If we push the Pearson correlation to the extreme, reaching -1, we witness a perfect inverse agreement. In this scenario, every data point converges precisely onto the black dashed line, forming an impeccable alignment. Notably, even the best fit line mirrors this black dashed line in perfect harmony, showcasing the unparalleled concordance in this exceptional case.\r\rFigure 5: Scenario 4: Inverse relationship with both Pearson correlation varying from -0.43 to -1. The red dashed line represents \\(Y = X\\), the black dashed line is the \\(Y = 100 - X\\), and the blue solid line is the best fit line\r\rIn Scenario 5, the CCC value of 0 denotes absolute non-concordance, indicating that the variability between the measurements does not correspond to the variability expected by chance alone. This suggests that the measurements lack any systematic agreement and cannot be used interchangeably or as reliable estimates of one another. The complete absence of concordance highlights the importance of accuracy in measurement, underscoring that precision without accuracy does not yield valid or useful data in reflecting the assumed ‚Äútrue‚Äù values.\r\rFigure 6: Scenario 5 - No agreement with CCC at 0\r\rThese scenarios vividly demonstrate the nuanced interplay between precision and accuracy within the framework of CCC. They bring to light the importance of accuracy in scientific measurement and the limitations of relying solely on correlation coefficients for assessing the validity and reliability of data.\rTo further strengthen the discussion, we must consider the assumptions and limitations of CCC. It assumes that the data scales are continuous and that the relationship between the measures is linear. The presence of outliers can unduly influence the CCC, and it may not be suitable for all data types. Additionally, the CCC does not account for random error, which can affect measurements variably.\rWhen considering alternative measures, such as Bland-Altman plots, we can address scenarios where CCC is less suitable, like non-normal data or comparisons of more than two measurement sets.\rIn practical terms, CCC can be used to validate new measurement methods against established gold standards, underscoring its application in method comparison studies. Real-world examples where CCC has been pivotal could include its use in clinical settings for comparing measurement techniques, methods of colour measurements in agriculture, and model diagnostic in statistics.\rFor computational purposes, the statistical environment R has functions and libraries dedicated to calculating CCC, making it accessible for researchers and practitioners to apply this measure to their data. The epiR package in R, for instance, provides a function epi.ccc specifically for calculating Lin‚Äôs CCC.\rIf one prefers to write a custom function in R for educational or analytical purposes, the following function can be used to compute a point estimate for Lin‚Äôs Concordance Correlation Coefficient (CCC):\r#\u0026#39; Calculate Lin\u0026#39;s Concordance Correlation Coefficient (CCC)\r#\u0026#39;\r#\u0026#39; This function computes Lin\u0026#39;s Concordance Correlation Coefficient to evaluate the agreement\r#\u0026#39; between two sets of measurements. It returns the CCC which combines measures of precision\r#\u0026#39; and accuracy to determine how well the data from the two sets conform to the line of\r#\u0026#39; perfect concordance.\r#\u0026#39;\r#\u0026#39; @param x A numeric vector of measurements.\r#\u0026#39; @param y A numeric vector of measurements, where each element corresponds to the element in `x`.\r#\u0026#39;\r#\u0026#39; @return The Concordance Correlation Coefficient as a numeric value.\r#\u0026#39;\r#\u0026#39; @examples\r#\u0026#39; x \u0026lt;- c(1, 2, 3, 4, 5)\r#\u0026#39; y \u0026lt;- c(1.1, 1.9, 3.1, 4.2, 4.8)\r#\u0026#39; ccc(x, y)\r#\u0026#39;\r#\u0026#39; @export\rccc \u0026lt;- function(x, y) {\r# Check if inputs are numeric vectors\rstopifnot(is.numeric(x), is.numeric(y))\r# Calculate Pearson\u0026#39;s correlation coefficient\rrho \u0026lt;- cor(x, y)\r# Calculate means of x and y\rmean_x \u0026lt;- mean(x)\rmean_y \u0026lt;- mean(y)\r# Calculate variances of x and y\rvar_x \u0026lt;- var(x)\rvar_y \u0026lt;- var(y)\r# Calculate CCC based on the formula\rccc_value \u0026lt;- (2 * rho * sqrt(var_x) * sqrt(var_y)) / (var_x + var_y + (mean_x - mean_y)^2)\rreturn(ccc_value)\r}\rThis function begins by calculating Pearson‚Äôs correlation coefficient (\\(rho\\)) for the input vectors \\(x\\) and \\(y\\). It then computes the means (\\(mean_x\\) and \\(mean_y\\)) and variances (\\(var_x\\) and \\(var_y\\)) of the two sets of measurements. The CCC is calculated by combining these values according to the formula, thereby quantifying the agreement between the two measurements in terms of both precision and accuracy.\rUsing the custom ccc function provides a straightforward computation of Lin‚Äôs Concordance Correlation Coefficient (CCC) for research applications, particularly when a simple estimate of concordance is required. However, for a more comprehensive analysis, the epi.ccc function from the epiR package is advantageous as it not only computes CCC but also provides confidence intervals for the CCC value, offering a statistical range within which the true concordance lies with a certain probability. This is crucial for making inferences about the precision of the agreement between measurements in research.\rFor those looking to apply CCC specifically to Lin‚Äôs method, both the epiR package and the DescTools package on CRAN offer robust tools. The DescTools package provides a comprehensive collection of statistical functions, including methods for calculating CCC.\rFor studies involving repeated measures, where the same subjects are measured under different conditions or at different times, the lcc package offers functions for calculating CCC for longitudinal data, taking into account the within-subject correlation.\rAdditionally, the cccrm package provides functions for calculating the CCC for repeated (and non-repeated) measures, catering to a wide range of research designs and ensuring that the variability inherent in repeated measures is appropriately accounted for.\rThese packages are valuable additions to the toolkit of researchers, statisticians, and data analysts, enhancing the reliability and interpretability of concordance assessments in scientific studies.\rR code used to produce the data and plots\r# Load necessary libraries\rlibrary(MASS)\rlibrary(ggplot2)\r# Function to calculate Concordance Correlation Coefficient (CCC)\rccc \u0026lt;- function(x, y) {\r# Check if inputs are numeric vectors\rstopifnot(is.numeric(x), is.numeric(y))\r# Calculate Pearson\u0026#39;s correlation coefficient\rrho \u0026lt;- cor(x, y)\r# Calculate means of x and y\rmean_x \u0026lt;- mean(x)\rmean_y \u0026lt;- mean(y)\r# Calculate variances of x and y\rvar_x \u0026lt;- var(x)\rvar_y \u0026lt;- var(y)\r# Calculate CCC based on the formula\rccc_value \u0026lt;- (2 * rho * sqrt(var_x) * sqrt(var_y)) / (var_x + var_y + (mean_x - mean_y)^2)\rreturn(ccc_value)\r}\r# Set seed for reproducibility\rset.seed(123)\r# Function to generate and plot data for each scenario\rdata_plot \u0026lt;- function(mean, cov, scenario_number) {\rdata \u0026lt;- mvrnorm(100, mu = mean, Sigma = cov)\rdf \u0026lt;- data.frame(x = data[,1], y = data[,2])\rtitle \u0026lt;- ifelse(scenario_number == 3 || scenario_number == 4,\rpaste(\u0026quot;Scenario\u0026quot;, scenario_number, \u0026quot;: Pearson =\u0026quot;, round(cor(df$x, df$y), 2), \u0026quot;CCC =\u0026quot;, round(ccc(df$x, df$y), 2)),\rpaste(\u0026quot;Scenario\u0026quot;, scenario_number, \u0026quot;: CCC =\u0026quot;, round(ccc(df$x, df$y), 2)))\rp \u0026lt;- ggplot(df, aes(x = x, y = y)) + geom_point(color = \u0026quot;blue\u0026quot;, alpha = 0.6, size = 3) +\rggtitle(title) +\rxlab(\u0026quot;X values\u0026quot;) +\rylab(\u0026quot;Y values\u0026quot;) +\rtheme_minimal() +\rtheme(plot.title = element_text(hjust = 0.5, face = \u0026quot;bold\u0026quot;, size = 14),\raxis.title.x = element_text(face = \u0026quot;bold\u0026quot;, size = 12),\raxis.title.y = element_text(face = \u0026quot;bold\u0026quot;, size = 12),\raxis.text.x = element_text(size = 10),\raxis.text.y = element_text(size = 10))\rif (scenario_number == 3 || scenario_number == 4) {\rp \u0026lt;- p + geom_smooth(method = \u0026quot;lm\u0026quot;, color = \u0026quot;blue\u0026quot;, se = FALSE) +\rgeom_abline(slope = -1, intercept = 100, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;black\u0026quot;) +\rgeom_abline(slope = 1, intercept = 0, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;)\r} else {\rp \u0026lt;- p + geom_abline(slope = 1, intercept = 0, linetype = \u0026quot;dashed\u0026quot;, color = \u0026quot;red\u0026quot;)\r}\rprint(p)\r}\r# Scenario 1: High Pearson correlation, modest CCC\rdata_plot(c(50, 70), matrix(c(100, 0.99 * sqrt(100) * sqrt(150), 0.99 * sqrt(100) * sqrt(150), 150), 2), 1)\r# Scenario 2: High CCC\rdata_plot(c(50, 50), matrix(c(100, 0.95 * sqrt(100) * sqrt(100), 0.95 * sqrt(100) * sqrt(100), 100), 2), 2)\r# Scenario 3: Negative Pearson and CCC\rdata_plot(c(50, 50), matrix(c(100, -0.3 * sqrt(100) * sqrt(150), -0.3 * sqrt(100) * sqrt(150), 150), 2), 3)\r# Scenario 4 (previously 5): Perfect inverse agreement\rdata_plot(c(50, 50), matrix(c(100, -1 * sqrt(100) * sqrt(100), -1 * sqrt(100) * sqrt(100), 100), 2), 4)\r# Scenario 5 (previously 4): No correlation (CCC = 0)\rdata_plot(c(50, 50), matrix(c(100, 0, 0, 150), 2), 5)\r\rReferences\rCarrasco, J. L., King, T. S., \u0026amp; Chinchilli, V. M. (2009). The Concordance Correlation Coefficient for Repeated Measures Estimated by Variance Components. Journal of Biopharmaceutical Statistics, 19, 90-105. DOI: 10.1080/10543400802527890\rOliveira, T. de P., Hinde, J., \u0026amp; Zocchi, S. S. (2018). Longitudinal Concordance Correlation Function Based on Variance Components: An Application in Fruit Color Analysis. Journal of Agricultural, Biological, and Environmental Statistics, 23(2), 233-254. DOI: 10.1007/s13253-018-0321-1\rRathnayake, L. N., \u0026amp; Choudhary, P. K. (2017). Semiparametric Modeling and Analysis of Longitudinal Method Comparison Data. Statistics in Medicine, 36, 2003-2015. DOI: 10.1002/sim.7261\r\rCitation\rFor attribution, please cite this work as:\r\rOliveira T.P. (2024, Jan.¬†10). Precision \u0026amp; Accuracy - The Role of Concordance Correlation in Research\n\rBibTeX citation\r\r@misc{oliveira2024concordance,\rauthor = {Oliveira, Thiago},\rtitle = {Precision \u0026amp; Accuracy - The Role of Concordance Correlation in Research},\rurl = {https://prof-thiagooliveira.netlify.app/post/precision-and-accuracy-the-role-of-concordance-correlation-in-research/},\ryear = {2024}\r}\rDid you find this page helpful? Consider sharing it üôå\n\r","date":1705449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705479354,"objectID":"cf8331c6eb5b199d6eda6aa5ad907dcc","permalink":"https://prof-thiagooliveira.netlify.com/post/statistics/","publishdate":"2024-01-17T00:00:00Z","relpermalink":"/post/statistics/","section":"post","summary":"The Concordance Correlation Coefficient (CCC) is a statistical measure designed to evaluate the agreement between two sets of measurements, such as those represented by two random variables, \\(X\\) and \\(Y\\).","tags":["Concordance","Agreement","Statistics"],"title":"Precision \u0026 Accuracy - The Role of Concordance Correlation in Research","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["R"],"content":"\rIntroduction\rIn R programming, efficiency is key. Snippets, small reusable blocks of code, are a cornerstone in achieving this. This post delves into the world of snippets, offering both novice and seasoned R programmers insights into their power and versatility.\rWhat are Snippets?\rIn R programming, snippets are more than just pre-written bits of code; they are dynamic templates designed to streamline code writing and editing. Snippets in R can contain placeholders, which are special fields that can be easily tabbed through and filled in by the programmer. This feature allows for rapid customization of the snippet to fit specific coding needs. They can encapsulate complex coding patterns, data structures, and algorithms, making them particularly useful for tasks that require adherence to specific coding standards or methodologies.\rSnippets can be simple, such as a line to import a commonly used library, or complex, containing entire functions or control structures. They support variable interpolation, enabling the inclusion of dynamic content like dates, user names, or contextual code. Advanced snippets may even include scriptable transformations of the inserted text, allowing for sophisticated code generation based on the user‚Äôs input.\r\rAdvantages of Using Snippets\r\rEnhanced Productivity: Snippets go beyond automating repetitive code insertion. They serve as a framework for implementing best practices and methodologies, significantly reducing the cognitive load on the programmer. By providing ready-to-use code templates, they allow programmers to focus on the unique aspects of their work, rather than the boilerplate code.\r\rError Reduction: The use of snippets minimizes syntax and logical errors not just through standardization, but also by embedding proven and tested code patterns. This is especially beneficial in complex programming tasks where the risk of introducing errors is high. It ensures that the fundamental building blocks of the code are sound, allowing programmers to concentrate on higher-level logic and functionality.\r\rCode Consistency: In collaborative projects, maintaining a consistent coding style and structure is vital for readability and maintainability. Snippets enforce a uniform coding convention, which is crucial when working in teams or when codebases are passed between different developers. They help in aligning the code with organizational or community standards, making the code more accessible and understandable to all team members.\r\rRapid Prototyping and Experimentation: Snippets enable quick assembly of code constructs, facilitating rapid prototyping and experimentation. This is particularly valuable in data science and statistical analysis, where various approaches and methods are often tested in quick succession.\r\rEducational Tool: For learners of R programming, snippets act as an educational tool, demonstrating best practices and exposing them to different coding styles and patterns. It accelerates the learning curve by providing examples of well-structured code.\r\r\rPotential Drawbacks of Using Snippets\rWhile snippets offer numerous advantages, there are some considerations to keep in mind:\r\rInflexibility in Complex Scenarios: Snippets are excellent for routine tasks, but they may not always suit more complex, unique programming challenges. Overusing snippets in such scenarios can lead to inefficient or convoluted code, especially if the snippet doesn‚Äôt align perfectly with the specific requirements of the task.\r\rMaintenance Challenges: Snippets, like any other code, require maintenance. As the R language and associated packages evolve, snippets might become outdated, leading to compatibility issues or deprecated practices. Keeping a library of snippets up-to-date can be a task in itself.\r\rStandardization vs.¬†Creativity: While standardization is an advantage, it can sometimes stifle creativity and innovation in coding. Relying heavily on snippets may discourage developers from exploring new or unconventional solutions to programming problems.\r\r\rThus, while snippets are a powerful tool in R programming, understanding and mitigating these potential drawbacks is crucial for effective and efficient use. It‚Äôs important to balance the convenience of snippets with the need for deep understanding, creativity, and code efficiency.\r\r\rIntegrating Snippets into R\rMost R Integrated Development Environments (IDEs), such as RStudio, have built-in support for snippets. They allow for easy creation, modification, and insertion of snippets into your code.\r\rExample of Snippets for R Programming\rFunction Declaration (advFun)\nsnippet advFun\r${1:function_name} \u0026lt;- function(${2:args}, ${3:optional_args = default_values}) {\rtryCatch({\r${4:body}\rreturn(${5:result})\r}, error = function(e) {\rstop(\u0026quot;Error in ${1:function_name}: \u0026quot;, e)\r})\r}\rConditional Execution (three_statements)\nsnippet three_statements\rif (${1:primary_condition}) {\r${2:primary_action}\r} else if (${3:secondary_condition}) {\r${4:secondary_action}\r} else {\r${5:alternative_action}\r}\rFor Loop (ForLoop)\nsnippet ForLoop\rfor (${1:var} in ${2:sequence}) {\rif (${3:break_condition}) {\rbreak\r} else if (${4:continue_condition}) {\rnext\r}\r${5:loop_body}\r}\rWhile Loop with Counter (whileLoopCounter)\nsnippet whileLoopCounter\r${1:counter} \u0026lt;- ${2:initial_value}\rwhile (${3:condition}) {\r${4:body}\r${1:counter} \u0026lt;- ${1:counter} + 1\rif (${1:counter} \u0026gt; ${5:max_iterations}) break\r}\rJoin and Transform Data with dplyr (dplyrJoinTransform)\nsnippet dplyrJoinTransform\r${1:result} \u0026lt;- ${2:dataset1} %\u0026gt;%\rinner_join(${3:dataset2}, by = \u0026quot;${4:key}\u0026quot;) %\u0026gt;%\rdplyr::mutate(${5:new_column} = ${6:transformation}) %\u0026gt;%\rarrange(${7:order_column})\rRobust Exception Handling (robustTryCatch)\nsnippet robustTryCatch\rtryCatch({\r${1:expr}\r}, warning = function(w) {\rwarning(\u0026quot;Warning in ${1:expr}: \u0026quot;, w)\r}, error = function(e) {\rstop(\u0026quot;Error in ${1:expr}: \u0026quot;, e)\r}, finally = {\rmessage(\u0026quot;Executed ${1:expr}\u0026quot;)\r})\rPlotting with ggplot2 (ggplot_wrap)\nsnippet ggplot_wrap\rggplot(${1:data}, aes(${2:aes_params})) +\r${3:geom_layer} +\rfacet_wrap(~ ${4:facet_var}) +\rtheme_minimal() +\rtheme(axis.text.x = element_text(angle = 90, hjust = 1)) +\rlabs(title = \u0026quot;${5:plot_title}\u0026quot;, x = \u0026quot;${6:x_label}\u0026quot;, y = \u0026quot;${7:y_label}\u0026quot;)\rData Reading (readData)\nsnippet readData\r${1:dataset} \u0026lt;- read.csv(\u0026quot;${2:file_path}\u0026quot;, header = ${3:TRUE}, na.strings = \u0026quot;${4:NA}\u0026quot;, stringsAsFactors = ${5:FALSE})\r\rThese snippets are formatted to be directly added to your R snippet library, making them easily accessible and usable within your R programming environment.\r\rEvaluating Code Performance with Snippet\rIn addition to the various functional and structural snippets, a key aspect of efficient programming is performance optimization. The measureCodeBottleneck snippet is an example of code for identifying performance bottlenecks in your R code. It helps you measure both execution time and memory usage, offering insights into how your code can be optimized for better performance.\rThe measureCodeBottleneck Snippet\rsnippet measureCodeBottleneck\rlibrary(microbenchmark)\rlibrary(pryr)\r# Memory and Time Measurement Function\rmeasureBottleneck \u0026lt;- function(expr) {\r# Measure execution time\rtime_result \u0026lt;- microbenchmark(expr, times = ${1:100})\rprint(summary(time_result))\r# Measure memory usage\rmem_usage \u0026lt;- object_size(expr)\rprint(paste(\u0026quot;Memory Usage: \u0026quot;, mem_usage))\r}\r# Example Usage\r# measureBottleneck({\r# # Place your code here\r# })\rThis snippet is particularly useful when working with large datasets or complex algorithms, where understanding and minimizing resource consumption is crucial.\r\r\rConclusion\rSnippets stand as a powerful asset in the toolkit of any R programmer, driving efficiency, reducing errors, and ensuring consistency across coding projects. Their integration into your daily workflow can be a game changer, significantly elevating both productivity and the quality of your code. However, it‚Äôs crucial to use snippets judiciously.\rWhile snippets are designed to save time and resources, their indiscriminate or inappropriate use can, paradoxically, lead to the opposite - a waste of time and a drain on resources. As you incorporate these snippets into your work, be mindful of their relevance and applicability to the task at hand. Choose and customize snippets that align closely with your specific coding needs and avoid the temptation to use a snippet when a more straightforward or tailored piece of code would be more efficient. This balanced approach to using snippets will ensure that you truly harness their potential to make your R programming more effective and streamlined.\rRemember, the goal is not just to code faster, but to code smarter. Snippets, when used thoughtfully, are a robust lever in achieving this goal.\r\r\rCitation\rFor attribution, please cite this work as:\r\rOliveira T.P. (2024, Jan.¬†06). R Programming with Efficient Snippets\n\rBibTeX citation\r\r@misc{oliveira2024snippets,\rauthor = {Oliveira, Thiago},\rtitle = {R Programming with Efficient Snippets},\rurl = {https://prof-thiagooliveira.netlify.app/post/r-programming-with-efficient-snippets/},\ryear = {2024}\r}\rDid you find this page helpful? Consider sharing it üôå\n\r","date":1704499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704543354,"objectID":"4015bfd944ce2034ef830fd1be64b82d","permalink":"https://prof-thiagooliveira.netlify.com/post/r-programming/","publishdate":"2024-01-06T00:00:00Z","relpermalink":"/post/r-programming/","section":"post","summary":"Introduction\rIn R programming, efficiency is key. Snippets, small reusable blocks of code, are a cornerstone in achieving this. This post delves into the world of snippets, offering both novice and seasoned R programmers insights into their power and versatility.","tags":["R","code","snippets"],"title":"R Programming with Efficient Snippets","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["R","shiny"],"content":"\rThe golem package\rIn the world of R programming, Shiny applications let us make interactive web apps using R code. The golem package (Fay et al.¬†2021) makes it easier to develop these apps. It brings new tools and methods to this area, helping developers handle complex tasks more simply.\rMaking Things with Structure\rThink of making a sculpture out of clay. At first, the big lump of clay can be hard to handle. golem helps developers, like sculptors, by giving them a clear framework. This means instead of dealing with a big, confusing bunch of code, developers have an organized way to work. It is like having lines drawn on the clay, showing where to shape and smooth it.\r\rModular Component\rWhen I first stumbled upon the golem package for R‚Äôs Shiny applications, it was like discovering a secret garden in the world of coding. The stand-out feature for me? Its emphasis on modular coding. Let me break down why this is such a big deal.\rThink of building a Shiny app like crafting a beautiful mosaic. Each piece (or module) is unique and serves a specific purpose. When you put them all together, they create a stunning picture - your final application. This modular approach is not just about aesthetics; it is about making your coding life a whole lot easier.\rWhy Modules Make All the Difference\rOrganization: Breaking down the app into modules is like having a well-organized toolbox. Everything has its place, and you know exactly where to find it. It is incredibly satisfying and efficient.\r\rTeamwork Made Simple: If you are working in a team, modules are a lifesaver. Imagine each team member painting their part of a large canvas. With modules, you can work independently on different features without stepping on each other‚Äôs toes.\r\rDebugging: We have all been there - something is broken, and we have no idea where to start looking. With modular coding, it is like having a map with a big ‚ÄúX‚Äù marking the spot of the problem. A big simplication!\r\rReuse and Recycle: I love this part. Created a nifty user authentication module? You can plug it into your next project without reinventing the wheel. It is like having a secret recipe you can use over and over with minor adaptations when needed.\r\rGrowth Made Easy: As your app grows, you can just add new modules. It is like adding new rooms to a house. This scalability is one of most helpful feature for any developer.\r\rTesting: Testing each module separately means you can be super confident that every part of your app works perfectly before you put it all together.\r\rNewbie-Friendly: If someone new joins your project, it is much easier for them to get up to speed with a modular structure. It is like giving them a well-detailed map instead of a single, overwhelming blueprint.\r\r\r\rA Developer‚Äôs Toolbox\rgolem is not just about keeping things tidy. It is like a multi-tool for Shiny developers. It helps with JavaScript and CSS, makes app settings simpler, and improves how you work. golem also manages updates in R, making sure your app stays stable even when other parts of R change.\r\rDeployment and Documentation\rDeploying a Shiny app should feel like a victory lap, not a hurdle race. golem ensures this by packaging Shiny apps in a deployment-ready format. Be it RStudio Connect, Shinyapps.io, or the containerized world of Docker, your app is prepared and primed to go live.\rNow, let‚Äôs talk about something that does not always get the spotlight but is super crucial: documentation. golem knows how important this is. It is not just about coding; it is about leaving a trail of breadcrumbs for those who will follow in your footsteps. golem encourages you to document your work thoroughly. Think of it as creating a treasure map for future developers and collaborators who will join your project. Moreover, golem aligns seamlessly with the roxygen2 style of documentation, familiar to many R developers. This integration means that while you are crafting your Shiny app, you can simultaneously create comprehensive, easy-to-understand documentation. It is like having a dual toolkit - one for building your app and another for creating a clear, helpful guide for any future developer or user who ventures into your code. This approach not only saves time but also ensures that your documentation is as robust and user-friendly as the app you are building.\r\rConclusion\rgolem truly revolutionizes the way we handle R and Shiny applications. It is like having a GPS for the often complex journey of app development, guiding you with a structured, modular approach. This not only simplifies the process but also injects a sense of fun and creativity, much like piecing together a Lego masterpiece.\rBeyond just coding, golem makes deploying apps feel like a victory lap and turns documentation into an integral, rewarding part of the development cycle. With the added bonus of a supportive community, golem is more than just a tool - it is a companion for any developer venturing into the exciting world of Shiny applications. üöÄüåü\r\r\rReferences\rFay, Colin, Vincent Guyader, S√©bastien Rochette, and Cervan Girard. 2021. Golem: A Framework for Robust Shiny Applications. https://github.com/ThinkR-open/golem.\n\rAdditional Material\r\rEngineering Production-Grade Shiny Apps\rgolem: A Framework for Building Robust Shiny Apps\rgolem R package\r\r\rCitation\rFor attribution, please cite this work as:\r\rOliveira T.P. (2023, Oct.¬†02). Navigating the Shiny Universe with Golem\n\rBibTeX citation\r\r@misc{oliveira2020golem,\rauthor = {Oliveira, Thiago},\rtitle = {Navigating the Shiny Universe with Golem},\rurl = {https://prof-thiagooliveira.netlify.app/post/golem-package/},\ryear = {2023}\r}\rDid you find this page helpful? Consider sharing it üôå\n\r","date":1696204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696248954,"objectID":"61ae4498bc07a00d09bcd41822d9797e","permalink":"https://prof-thiagooliveira.netlify.com/post/golem-package/","publishdate":"2023-10-02T00:00:00Z","relpermalink":"/post/golem-package/","section":"post","summary":"The golem package\rIn the world of R programming, Shiny applications let us make interactive web apps using R code. The golem package (Fay et al.¬†2021) makes it easier to develop these apps.","tags":["R","shiny","R package"],"title":"Navigating the Shiny Universe with Golem","type":"post"},{"authors":["Kishor Das","Thiago de Paula Oliveira","John Newell"],"categories":null,"content":"","date":1689379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689379200,"objectID":"45d1974a0b2105c74bda2da9bb76d40f","permalink":"https://prof-thiagooliveira.netlify.com/publication/2023-floa/","publishdate":"2023-07-15T00:00:00Z","relpermalink":"/publication/2023-floa/","section":"publication","summary":"Markerless motion capture challenges traditional systems. A mixed-effects model assesses their agreement for reliable human movement analysis.","tags":["R Package","Sports analytics","Hierarchical modelling","Athlete evaluation","Multilevel regression","Human movement"],"title":"Comparison of Markerless and Marker-based Motion Capture Systems using 95% Functional Limits of Agreement in a Linear Mixed-Effects Modelling Framework","type":"publication"},{"authors":["Thiago de Paula Oliveira","John Newell"],"categories":null,"content":"","date":1687651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687651200,"objectID":"4ebc6279c2aa4b5ee761eaa363a35cc8","permalink":"https://prof-thiagooliveira.netlify.com/publication/2023-onscore/","publishdate":"2023-06-25T00:00:00Z","relpermalink":"/publication/2023-onscore/","section":"publication","summary":"ON score is a comprehensive athlete rating using mixed-regression and PCA on 4-season NBA data.","tags":["R Package","Sports analytics","Hierarchical modelling","Athlete evaluation","Multilevel regression","Performance metrics"],"title":"A Hierarchical Approach for Evaluating Athlete Performance with an Application in Elite Basketball","type":"publication"},{"authors":["Thiago de Paula Oliveira","Ivan Pocrnic","Gregor Gorjanc"],"categories":null,"content":"","date":1686355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686355200,"objectID":"cfc5038b84423cccbad9cdc8fd5584df","permalink":"https://prof-thiagooliveira.netlify.com/publication/2023-dag/","publishdate":"2023-06-10T00:00:00Z","relpermalink":"/publication/2023-dag/","section":"publication","summary":"This paper introduces a versatile graphical model for animal models, adaptable to various animal model variants in JAGS and NIMBLE using the BUGS language. The proposed model represents the animal model as a directed acyclic graph (DAG), enabling a computationally efficient implementation that is conceptually equivalent to mixed model equations. The model offers several benefits, serving as a foundation for numerous animal model extensions and applicability in other environments. Quantitative geneticists can utilize this graphical model to clarify and discuss ideas about the underlying data generation process and propose novel statistical models or associations. The animal model, frequently employed in quantitative genetics, estimates individual breeding values based on observed phenotypic data and known relationships between individuals. It is especially useful in animal breeding programs aiming to enhance desirable traits, such as milk production in dairy cattle or meat quality in swine, which are often complex and influenced by multiple genes and environmental factors. By incorporating DAGs in the implementation of the animal model, the results become more reproducible, which is vital for constructing animal models and using DAGs to communicate outcomes or justify covariate selection. The model has the potential to improve the accuracy of breeding value estimates and inform breeding programs targeting desirable trait enhancement in animal populations. Additionally, its generic form can be used with MCMC methods to infer changes in additive genetic variance across generations due to the Bulmer effect, applicable in evolutionary studies. However, some initial programming experience is necessary for effective model utilization. In summary, this paper emphasizes the importance of reproducibility in constructing animal models and using DAGs for result communication, crucial for advancing quantitative genetic research. The proposed graphical model offers a flexible environment for quantitative genetics researchers, facilitating a deeper understanding of the underlying data structure and enabling informed decisions about covariate selection and new statistical model development. Moreover, the model has the potential to increase the accuracy of breeding value estimates and inform breeding programs focused on enhancing desirable traits in animal populations.\n","tags":["Quantitative Genetics","Animal Model","Bayesian","Effective Sample Size","BUGS Language","Directed acyclic graph (DAG)","MCMC methods","Graphical model"],"title":"Pedigree-based Animal Models Using Directed Acyclic Graphs","type":"publication"},{"authors":["Thiago de Paula Oliveira","Jana Obsteter","Ivan Pocrnic","Nicolas Heslot","Gregor Gorjanc"],"categories":null,"content":"","date":1675382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675382400,"objectID":"43e171c019bdfd92fe9f406dcd0aeb69","permalink":"https://prof-thiagooliveira.netlify.com/publication/2022-alphapartvariance/","publishdate":"2022-02-03T00:00:00Z","relpermalink":"/publication/2022-alphapartvariance/","section":"publication","summary":"Partitioning method to quantify the contribution of different groups to genetic variance","tags":["Genetic trend","Genetic mean","Genetic variance","Partitioning","Bayesian","MCMC","AlphaPart"],"title":"A method for partitioning trends in genetic mean and variance to understand breeding practices","type":"publication"},{"authors":["Cristiane Hayumi Taniguti","Lucas Mitsuo Taniguti","Rodrigo Rampazo Amadeu","Marcelo Mollinari","Guilherme da Silva Pereira","Oscar Riera-Lizarazu","Jeekin Lau","David Byrne","Gabriel de Siqueira Gesteira","Thiago de Paula Oliveira","Getulio Caixeta Ferreira","Antonio Augusto Franco Garcia"],"categories":null,"content":"","date":1669334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669334400,"objectID":"c1968a49b1662d0b5bd5189879c0f33e","permalink":"https://prof-thiagooliveira.netlify.com/publication/2022-genotyping-by-sequencing/","publishdate":"2022-11-25T00:00:00Z","relpermalink":"/publication/2022-genotyping-by-sequencing/","section":"publication","summary":"Reads2Map improves GBS data analysis by addressing genotyping errors, optimizing linkage maps, and offering dataset-specific solutions with a user-friendly app.","tags":["R Package","Genetic","Quantitative Genetic","Reads2Map Tools","Workflows"],"title":"Developing best practices for genotyping-by-sequencing analysis using linkage maps as benchmarks","type":"publication"},{"authors":["Thiago de Paula Oliveira"],"categories":null,"content":"","date":1663786800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663786800,"objectID":"468d294ff75aca0c4f5eba3c5596c418","permalink":"https://prof-thiagooliveira.netlify.com/talk/quantifying-the-drivers-of-genetic-change-in-plant-breeding/","publishdate":"2021-09-12T12:00:00Z","relpermalink":"/talk/quantifying-the-drivers-of-genetic-change-in-plant-breeding/","section":"event","summary":"Evaluate the contribution of germplasm origin given the heterotic pool in a maize breeding programme into contributions to additive genetic mean and variance summarized over the years","tags":["Breeding Programme","Maize","Genetic trend","Genetic mean","Genetic variance","Partitioning","Bayesian","MCMC","AlphaPart"],"title":"Quantifying the Drivers of Genetic Change in Plant Breeding","type":"event"},{"authors":["Thiago de Paula Oliveira"],"categories":null,"content":"","date":1658948400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658948400,"objectID":"155edccf49cb361c78f217886586d368","permalink":"https://prof-thiagooliveira.netlify.com/talk/a-method-for-partitioning-trends-in-genetic-mean-and-variance-to-understand-breeding-practices/","publishdate":"2021-09-20T12:00:00Z","relpermalink":"/talk/a-method-for-partitioning-trends-in-genetic-mean-and-variance-to-understand-breeding-practices/","section":"event","summary":"Partitioning method to quantify the contribution of different groups to genetic variance and its impact in breeding programme","tags":["Genetic trend","Genetic mean","Genetic variance","Partitioning","Bayesian","MCMC","AlphaPart"],"title":"A method for partitioning trends in genetic mean and variance to understand breeding practices","type":"event"},{"authors":null,"categories":null,"content":"This seeding project aims to build a proof of concept for extending our recently developed framework for temporal and genomic analysis of additive genetic variance in breeding programmes to non-additive genetic variance. The primary purpose of this extension is to quantify the magnitude of non-additive genetic variance and relatedy to quantify the contribution of inbreeding depression and heterosis to genetic gain in breeding programmes.\r","date":1644451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644451200,"objectID":"0e8b6a293ecfe2e37188924cfbadc2fe","permalink":"https://prof-thiagooliveira.netlify.com/project/non-additive/","publishdate":"2022-02-10T00:00:00Z","relpermalink":"/project/non-additive/","section":"project","summary":"Developed framework for temporal and genomic analysis of additive and dominance genetic variance","tags":["Statistics","Statistical modelling","Genetic","Genomics","Plant breeding"],"title":"Analysis of non-additive genetic variance in breeding programmes","type":"project"},{"authors":["Leticia A. P. Lara","Ivan Pocrnic","Thiago de Paula Oliveira","Chris Gaynor","Gregor Gorjanc"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"3e3c911af88919b6c71b633c4c220cbc","permalink":"https://prof-thiagooliveira.netlify.com/publication/2021-llara_gen_var_plants/","publishdate":"2021-12-15T00:00:00Z","relpermalink":"/publication/2021-llara_gen_var_plants/","section":"publication","summary":"Evaluate the temporal and genomic analysis of additive genetic variance in different stages of a breeding programme","tags":["Genomic analysis","Genetic variance","Breeding programme","Simulation","Genetic","Variance components","Linkage-disequilibrium"],"title":"Temporal and genomic analysis of additive genetic variance in breeding programmes","type":"publication"},{"authors":["Thiago de Paula Oliveira"],"categories":null,"content":"","date":1632135600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632135600,"objectID":"0252c4a410d7deea4750e2580610a0f0","permalink":"https://prof-thiagooliveira.netlify.com/talk/visualization-and-data-structure/","publishdate":"2021-09-20T12:00:00Z","relpermalink":"/talk/visualization-and-data-structure/","section":"event","summary":"Discussion on the principles of grammar of graphics and tidy data with application using the tidyverse","tags":["ggplot2","Visualization","tidyverse","tidy data","Data structure","Grammar of graphics"],"title":"Visualization and Data Structure","type":"event"},{"authors":["Thiago de Paula Oliveira","Georgie Bruinvels","Charles Pedlar","Brian Moore","John Newell"],"categories":null,"content":"","date":1629417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629417600,"objectID":"019a1dece72ceaa27e7429675873733b","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-scireports/","publishdate":"2021-08-20T00:00:00Z","relpermalink":"/publication/2020-scireports/","section":"publication","summary":"Development of an appropriate parametric state-space formulation for the marginal distribution of standard menstrual cycles for female athletes","tags":["Menstrual cycle","Longitudinal data","State-Space Models","Athletes","Statistical Modelling","Hierarchical data"],"title":"Modelling menstrual cycle length in athletes using state-space models","type":"publication"},{"authors":["Gregor Gorjanc","Jana Obsteter","Thiago de Paula Oliveira"],"categories":null,"content":"","date":1618012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618012800,"objectID":"7c654db9d4dcf96b387e1265ffc11300","permalink":"https://prof-thiagooliveira.netlify.com/publication/2021-alphapart-package/","publishdate":"2021-04-07T00:00:00Z","relpermalink":"/publication/2021-alphapart-package/","section":"publication","summary":"Partitioning genetic trends to quantify the sources of genetic gain in breeding programmes","tags":["R Package","Genetic","Quantitative Genetic","C++"],"title":"Partition/Decomposition of Breeding Values by Paths of Information","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral"],"categories":null,"content":"Supplementary notes were added here:\n  Github\n  Dashboard App\n  ","date":1617667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617667200,"objectID":"1d5f658795ef3d36ee6ca5bee4011ccb","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-covid/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/publication/2020-covid/","section":"publication","summary":"We show our model generates high accurate forecasts up to seven days ahead for COVID-19 reported cases.","tags":["COVID-19","SARS-Cov-2","Pandemic","Statistical Methodology","Statistical Modelling","Longitudinal data","State-Space Models","Hierarchical data"],"title":"Global Short-Term Forecasting of Covid-19 Cases","type":"publication"},{"authors":["Thiago de Paula Oliveira"],"categories":["C++","Computer Programs"],"content":"Introduction Expressions in C++ are fundamental constructs made up of operators, constants, and variables, following the language\u0026rsquo;s syntactical rules. Every expression is a segment of a code that returns a value. For instance:\nThis example demonstrates the creation of variables to store values: a box for $x$ and another for $y$, where $y$ equals the expression $x + 13$ (thus, $y = 23$). Now, let\u0026rsquo;s delve into a more complex example:\nThis statement encompasses three expressions:\n* The results of the expression $3 - x$ is stored in the variable $y$\r* The expression $y = 3 - x$ returns the value of $y$, and it is stored in the variable $v$\r* The results of the expression $y \\times \\left(\\frac{v}{5} + x\\right)$ is stored in the variable $z$\r\rIt\u0026rsquo;s essential to remember the precedence of operations: multiplication and division are executed before addition and subtraction. For example:\n1-3*4 = -11\r2/3-4*2/3 = -2\r2/3-4/4*2/3 = 0\r Operator precedence in C++ determines the sequence of operations in an expression. Operators have a specific order of execution relative to others. For instance, in the expression $\\frac{2}{4} - 3 + 4 \\times 6$, the subexpressions $\\frac{2}{4}$and$4 \\times 6$ are calculated first, followed by the addition and subtraction. When operators have the same precedence, their associativity dictates the order - either left-to-right or right-to-left.\nPrecedence order\n\rAssociativity specifies the order of operations for operators with the same precedence level. It can be left-to-right or right-to-left. Typically, addition, subtraction, multiplication, and division are left-associative, while assignment operators are right-associative. Some operators are non-associative, meaning their behaviour is undefined if used sequentially in an expression. Parentheses can alter the default associativity, enforcing a specific order.\nExample of left-associative, right-associative, and non-associative\n\rUsing Parentheses () The operator () has the highest precedente order (see Table 1), as consequence, we can use parentheses to change the sequence of operators. Consider the following example:\n5 + 6 * 7\r The * operator is evaluated firstly, followed by the + operator, so the result is $5+6\\times 7 = 47$. However, if we want to account for the addiction first and then the multiplication, we can rewrite the code as:\n(5 + 6) * 7\r Then, the program will compute $\\left(5+6\\right)\\times 7=11\\times 7=77$. Sometimes, parentheses' inclusion should be important to make your code easier to understand, and therefore easier to maintain.\nModulus operator (%) The modulus operator evaluates the remainder when dividing the first operand by the second one. Ex.: a % b is the remainder when $a$ is dividedby $b$ ($a$ modulus $b$).by $b$ ($a$ modulus $b$).\nExample of modulus\n\r* Dividing an integer by another one gives an integer.\r\rExample: int x = 10;\rint y = 3;\rx/y = 10/3 = 3 (dividing two integers)\rx % y = 1 (modulus)\r Short hand or syntatic sugar Short hand expressions provide a straightforward way to write common patterns over the algorithm for initialized variables.\n   Short hand Meaning Prefix and Postfix     $x+=y$ $x=x+y$    $x-=y$ $x=x-y$    $x*=y$ $x= x \\times y$    $x/=y$ $x=x/y$    $x++$ $x=x+1$ Return the value of $x$ first then increment it   $++x$ $x=x+1$ Increment first then return the value of $x$   $x\u0026ndash;$ $x=x-1$ Return the value of $x$ first then increment it   $\u0026ndash;x$ $x=x-1$ Increment first then return the value of $x$    Example 1: Here you can see that y ++= x * z; is calculate as $y=y+x \\times z = 30 + 2 \\times 4 = 38$.\nExample 2: In this example you can see that we used the postfix x++ to first initialize $y$ ($y=8 \\times x = 8 \\times 7 = 56$) and then update $x$ to x=x+1=8. On the other hand, we used the prefix --y to first update the variable $y$ to y=y-1=55 and then calculate the variable z using the updated $y$ $\\left(z = y/5 = 55/5 = 11 \\right)$.\nNote that when we use x*= (y/z) % 2 the variable $x$ multiply the entire expression after = symbol. This expression is equivalent to x = x * ((y/z) % 2));.\nOperator precedence and associativity Table 1 shows a list of precedence (ordered) and associativity of C operators. This table was obtained from cppreference.com.\n\r\rTable 1: Precedence and associativity of C operators \rPrecedence\r\rOperator\r\rDescription\r\rAssociativity\r\r\r1\r\r++ \\-\\-\r\rSuffix/postfix increment and decrement\r\rLeft-to-right\r\r\r()\r\rFunction call\r\r\r[]\r\rArray subscripting\r\r\r.\r\rStructure and union member access\r\r\r-\u0026gt;\r\rStructure and union member access through pointer\r\r\r(type){list}\r\rCompound literal(C99)\r\r\r2\r\r++ \\-\\-\r\rPrefix increment and decrement[note 1]\r\rRight-to-left\r\r\r+ -\r\rUnary plus and minus\r\r\r! ~\r\rLogical NOT and bitwise NOT\r\r\r(type)\r\rCast\r\r\r*\r\rIndirection (dereference)\r\r\r\u0026amp;\r\rAddress-of\r\r\rsizeof\r\rSize-of[note 2]\r\r\r_Alignof\r\rAlignment requirement(C11)\r\r\r 3\r\r * / %\r\r Multiplication, division, and remainder\r\rLeft-to-right\r\r\r 4\r\r + -\r\r Addition and subtraction\r\r\r 5\r\r \u0026lt;\u0026lt; \u0026gt;\u0026gt;\r\r Bitwise left shift and right shift\r\r\r6\r\r\u0026lt; \u0026lt;=\r\rFor relational operators \u0026lt; and ‚â§ respectively\r\r\r\u0026gt; \u0026gt;=\r\rFor relational operators \u0026gt; and ‚â• respectively\r\r\r 7\r\r == !=\r\r For relational = and ‚â† respectively\r\r\r 8\r\r \u0026amp;\r\r Bitwise AND\r\r\r 9\r\r ^\r\r Bitwise XOR (exclusive or)\r\r\r 10\r\r |\r\r Bitwise OR (inclusive or)\r\r\r 11\r\r \u0026amp;\u0026amp;\r\r Logical AND\r\r\r 12\r\r ||\r\r Logical OR\r\r\r 13\r\r ?:\r\r Ternary conditional[note 3]\r\rRight-to-Left\r\r\r14[note 4]\r\r=\r\rSimple assignment\r\r\r+= -=\r\rAssignment by sum and difference\r\r\r*= /= %=\r\rAssignment by product, quotient, and remainder\r\r\r\u0026lt;\u0026lt;= \u0026gt;\u0026gt;=\r\rAssignment by bitwise left shift and right shift\r\r\r\u0026amp;= ^= |=\r\rAssignment by bitwise AND, XOR, and OR\r\r\r 15\r\r ,\r\r Comma\r\r Left-to-right\r\r‚Üë The operand of prefix ++ and \\-\\- can't be a type cast. This rule grammatically forbids some expressions that would be semantically invalid anyway. Some compilers ignore this rule and detect the invalidity semantically.\r\r‚Üë The operand of sizeof can't be a type cast: the expression sizeof (int) * p is unambiguously interpreted as (sizeof(int)) * p, but not sizeof((int)*p).\r\r‚Üë The expression in the middle of the conditional operator (between ? and :) is parsed as if parenthesized: its precedence relative to ?: is ignored.\r\r‚Üë Assignment operators' left operands must be unary (level-2 non-cast) expressions. This rule grammatically forbids some expressions that would be semantically invalid anyway. Many compilers ignore this rule and detect the invalidity semantically. For example, e = a \u0026lt; d ? a++ : a = d is an expression that cannot be parsed because of this rule. However, many compilers ignore this rule and parse it as e = ( ((a \u0026lt; d) ? (a++) : a) = d ), and then give an error because it is semantically invalid.\r\r\r\rImpact of Data Types on Expressions In C++, the data type of the variables involved in an expression significantly impacts the result. For instance, dividing two integers results in an integer, while using at least one floating-point number yields a floating-point result. Understanding how data types interact within expressions is crucial for accurate calculations and avoiding common pitfalls like integer truncation.\nHere are some key points about integer truncation and other common pitfalls in C++:\n  Integer Truncation: This occurs when the result of a division or other operation between integers is a floating-point number, but the data type is an integer. For example, int result = 5 / 2; will store 2 in result, not 2.5, as the fractional part is truncated.\n  Implicit Type Conversions: C++ automatically converts types in certain situations, which can lead to unexpected results. For instance, mixing signed and unsigned integers in expressions can cause unexpected behaviours due to implicit type conversions.\n  Overflow and Underflow: This happens when a variable is assigned a value outside its range. For example, storing a value larger than the maximum value that an int can hold will result in overflow, leading to unexpected values.\n  Precision Loss in Floating-Point Numbers: Floating-point variables can lose precision, especially when dealing with very large or very small numbers. This can result in rounding errors in calculations.\n  Division by Zero: This can occur if a program inadvertently divides a number by zero. It\u0026rsquo;s a critical error in C++ and can cause a program to crash or behave unpredictably.\n  Uninitialized Variables: Using variables before initializing them can lead to unpredictable results, as they may contain random data.\n  Pointer Errors: Common mistakes with pointers include dereferencing a null or uninitialized pointer, pointer arithmetic errors, and memory leaks.\n  Operator Precedence Mistakes: Misunderstanding the order in which operations are performed can lead to bugs. For example, assuming that a + b * c adds a and b before multiplying by c (it doesn\u0026rsquo;t; multiplication is done first).\n  Assuming Size of Data Types is Constant: The size of data types like int can vary depending on the system. Assuming a constant size can lead to errors, particularly when performing operations like bit manipulation or working with binary file formats.\n  Not Checking the Return Value of Functions: When functions return values to indicate success or failure, not checking these can lead to the program continuing as if nothing went wrong, even when errors have occurred.\n  Role of Type Casting in Expressions Type casting in expressions can be used to explicitly convert data from one type to another. This technique is particularly useful in situations where operations between different data types are necessary. For example, casting an integer to a float in a division operation to obtain a floating-point result. However, it\u0026rsquo;s important to use type casting judiciously to maintain the precision and integrity of data.\nThe Significance of Expression Evaluation Order While operator precedence and associativity rules dictate the order of operations in an expression, the sequence in which expressions are evaluated can also be influenced by function calls, side effects, and sequence points. Understanding how C++ evaluates expressions, especially in complex statements, is essential for debugging and writing predictable code.\nCompiler Optimizations and Expressions Modern C++ compilers often optimize expressions to enhance performance. These optimizations might include reordering operations (while respecting the language rules), eliminating redundant calculations, or simplifying expressions at compile time. Being aware of these potential optimizations can help in writing more efficient code and understanding any discrepancies between the written code and its execution behaviour.\nBest Practices for Writing Expressions To maintain readability and reduce errors in C++, it\u0026rsquo;s advisable to write clear and simple expressions. Avoid overly complex expressions, use parentheses to clarify order of operations, and follow coding standards and guidelines. Readable expressions are easier to debug, maintain, and understand, especially in collaborative environments.\nAdding these paragraphs can provide a more comprehensive and nuanced understanding of expressions in C++, catering to both beginners and experienced programmers.\nReferences  C Operator Precedence - https://en.cppreference.com/w/c/language/operator_precedence#cite_note-1  Citation  For attribution, please cite this work as:  Oliveira T.P. (2020, Dec. 16). Expressions in C++\r\rBibTeX citation  @misc{oliveira2020expression,\rauthor = {Oliveira, Thiago},\rtitle = {Expressions in C++},\rurl = {https://prof-thiagooliveira.netlify.app/post/expressions/},\ryear = {2020}\r}\r Did you find this page helpful? Consider sharing it üôå\n","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704543354,"objectID":"974dd795c8e7490101d09add351d92fd","permalink":"https://prof-thiagooliveira.netlify.com/post/expressions/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/expressions/","section":"post","summary":"Introduction Expressions in C++ are fundamental constructs made up of operators, constants, and variables, following the language\u0026rsquo;s syntactical rules. Every expression is a segment of a code that returns a value.","tags":["C++","Computer Science","Computer Programs"],"title":"Expressions in C++","type":"post"},{"authors":["Rafael Moral","Thiago de Paula Oliveira","Andrew Parnell"],"categories":["COVID-19","Statistical Models","Forecast Models"],"content":"\rtl;dr\r\rMany different variables affect how the pandemic progresses and it is extremely difficult to identify each one, and precisely measure them.\rThe data we have is surely innacurate, but could be a good proxy for understanding the behaviour of the coronavirus outbreak\rWe developed a statistical model to obtain short-term forecasts of the number of COVID-19 cases\rWe constantly update forecasts and make all results freely available to any country in the world through a web app\r\r\rHow many people will get infected tomorrow?\r‚ÄúHow many cases do you think we‚Äôre going to have today?‚Äù, my fianc√®e asked me just as I‚Äôm writing this post ‚Äì and quite frankly I‚Äôve asked that myself many times over the last several months. Wouldn‚Äôt it be great if we had a method to accurately predict the number of confirmed COVID-19 cases we‚Äôll have every single day for, say, the next month? If we could do that, we‚Äôd know whether our measures to contain the virus are working, whether we would be able to lift particular restrictions here and there, invest in intensive care units, or whether that wedding we had planned long ago would finally happen or have to be postponed‚Ä¶ again.\nIt is very hard, however, to pinpoint exactly every single factor that affects the number of reported COVID-19 cases, and most importantly, measure them all. Here we try and outline different techniques we could use to try and predict how the outbreak will behave in the future, and show a particular method we have developed to obtain short-term forecasts with a reasonable degree of accuracy. We have packaged the method into an app, which you can access here.\n\rWhat strategies can we use?\rThere are many different strategies and mathematical/statistical tools we can use to attempt to predict the future. These can include what we call mechanistic, or compartment models, for example. These make assumptions based on empirical evidence of the biological system being studied and translate them into mathematical equations based on the flow of individuals to/from specific compartments. For COVID-19 the SEIR-type model has been widely used by many research groups to describe the behaviour of the outbreak (see our blog post on the use of SEIR models to predict when the pandemic will end). They are realistic in the sense that they reflect the epidemiological behaviour of the outbreak.\nThere are other alternatives that do not take into account the true biological nature of the phenomenon per se, but may use it as input in a different way. Many machine learning techniques could sometimes be seen as black-box methods, that would e.g.¬†take the reported number of past COVID-19 cases and other variables that we would believe could influence this number and spit out a prediction for tomorrow, or next week, next month, etc. There are cases where these methods are even more accurate than mechanistic models, however there is a trade-off to consider here in terms of prediction accuracy vs.¬†explainability, as discussed here. If a new event or variable comes into play, which could empirically be very important to dictate the future behaviour of the pandemic, it is very difficult to gauge its effects using a black-box method.\nWe could also simply assume that the number of reported COVID-19 cases today is purely a reflection of the reported number of cases yesterday, and the day before, and so on. So we pretty much assume all variables that influence this process can be summarised purely by the outcomes we have observed in the past, and this can in turn be used to forecast what future numbers will be. Of course, there are plenty of different ways to include other variables in these types of models, but the important thing is to notice that we place a very heavy assumption on an underlying process that is able to explain its own behaviour. We usually refer to these models as ‚Äútime series‚Äù or ‚Äústate-space‚Äù models.\n\rWhy is it so difficult?\rThere are many factors that influence our ability to predict the number of future COVID-19 cases. Imagine we have, for example, a fantastic SEIR-type model that can reproduce the dynamics of the disease almost perfectly up to today. To be able to predict with great accuracy what will happen tomorrow (or even further down the line), we must assume, among other things, that the assumptions that hold today will still hold tomorrow and so on. If any new variable comes into play, or if the variables that are involved change over time, our predictions can be completely off.\nThis is not the worst problem, however. There are in fact many variables that we‚Äôre simply not able to measure with good precision. This includes knowing, for example, where everybody in the country is at all times, who they talk to, for how long, where they will be, etc. This is why it is important to do contact tracing, although this matters mostly in a retrospective way, not necessarily to predict what will happen in the future.\nBut wait a minute now, we don‚Äôt even know whether the data we can actually measure is in fact accurate! Or to be more specific, we do know that our data is definitely not 100% accurate. Cases reported today could reflect infections that happened between a few days ago to several weeks. Tests are not 100% accurate either, so there is a pool of false positives in there, as well as false negatives not being included in the whole sum. Simply put, the data we have is pretty much a proxy of the real thing. Hence why it is so important to understand what these numbers could actually mean, and not imbue them with improper meaning.\n\rWhat about short-term forecasting?\rSo long-term forecasting is very prone to built-up variation and error, as we all know. It‚Äôs just like predicting what time you‚Äôll wake up on your birthday 10 years from now. But there must be something we could do in the short-term, right? Well, it depends on how ‚Äúlong‚Äù this short-term is. And it also depends on how we want to use this information.\nWe developed a modelling framework in an attempt to predict the number of reported COVID-19 cases for up to 7 days in the future. We fitted our models to the data collected by the ECDC to generate the forecasts. See below for a validation study we carried out back in May/2020.\nThe panels are in the logarithmic scale, but in essence, the closer the points are to the identity line (dashed line), the closer our model was in predicting the number of COVID-19 cases up to 7 days ahead (panels in part A). In part B we see that the accuracy of the method is high for all 7 days ahead, but we begin to lose in terms of precision from day four onwards. (\\(r\\) represents Pearson‚Äôs linear correlation coefficient, the closest it is to 1 the better the method is; the same applies to the CCC - the concordance correlation coefficient.)\nThe idea behind this is not to be able to inform governments the exact numbers we‚Äôd expect tomorrow, but to give more perspective in terms of the types of trends we expect in the near future. This is useful to inform decision making related to the healthcare services. For instance, if a particular country‚Äôs healthcare system is currently at capacity, and we are predicting an upward trend in the number of infections, then this could guide policy in terms of resource allocation to accommodate the extra patients that are likely to seek health professionals in the upcoming weeks. This is why it is so important to look at overall trends (for example, the number of cases per 100,000 people over the last 14 days).\nOur model creates predictions based on two components. The first, called the autoregressive component, uses information on the past number of cases to predict future ones. The second is included to account for extra variability that could occur for a variety of different reasons. The autoregressive component is directly linked to the behaviour of the outbreak, so it is useful to detect waves of the pandemic. See, for example, our latest estimates for Ireland:\nWe can clearly see that towards the end of July this second wave was already starting to take shape, and now we are aiming at a new peak of cases.\n\rGrouping countries together\rNow that we have profiles for each country on how the pandemic is behaving in terms of number of cases, perhaps it would be a good idea to look at which countries present a similar behaviour over the last, say, 60 days. We created a dendrogram based on a cluster analysis performed using the values of the autoregressive parameter and produced the figure below ‚Äì\nHere we see that over looking at the past two months, the country that has presented the most similar behaviour to Ireland was Croatia. In our app you can play with different ways of presenting the dendrogram, as well as print names of different countries in bold to aid in finding them easily when looking at the picture. You can also change the number of clusters.\nPerhaps these comparisons would be useful in terms of comparing government policies on how to deal with the COVID-19 outbreak, and learn lessons from successful policies vs unsuccessful ones. Also, this type of modelling can help to detect a further wave of the outbreak sooner rather than when we are already in the middle of it!\n\rAll models are wrong‚Ä¶\rIn the end of the day, there is no true, correct model we can apply. After all, it is impossible to know exactly what the data generating mechanism is. We can only attempt to understand it and reproduce its behaviour using mathematical/statistical tools. We hope, however, that our modelling approach can be useful. We could point a whole list of problems with it here, such as completely ignoring biological mechanisms and using just past behaviour to explain future behaviour without any additional context. But we believe it represents a reasonable attempt at forecasting the number of COVID-19 cases in the short-term.\nDid you find this page helpful? Consider sharing it üôå\n\rCitation\rFor attribution, please cite this work as:\r\rMoral, et al.¬†(2020, Sept.¬†29). Ireland‚Äôs COVID-19 Data Dive: How hard is it to predict COVID-19 cases?. Retrieved from https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/\n\rBibTeX citation\r\r@misc{moral2020how,\rauthor = {Moral, Rafael and Oliveira, Thiago and Parnell, Andrew},\rtitle = {Ireland\u0026#39;s COVID-19 Data Dive: How hard is it to predict COVID-19 cases?},\rurl = {https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/},\ryear = {2020}\r}\r\r","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608121186,"objectID":"8a17a1381fc8f69cddde821d4df2bfcb","permalink":"https://prof-thiagooliveira.netlify.com/post/how-hard-is-it-to-predict-covid-19-cases/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/how-hard-is-it-to-predict-covid-19-cases/","section":"post","summary":"tl;dr\r\rMany different variables affect how the pandemic progresses and it is extremely difficult to identify each one, and precisely measure them.\rThe data we have is surely innacurate, but could be a good proxy for understanding the behaviour of the coronavirus outbreak\rWe developed a statistical model to obtain short-term forecasts of the number of COVID-19 cases\rWe constantly update forecasts and make all results freely available to any country in the world through a web app\r\r\rHow many people will get infected tomorrow?","tags":["Covid-19","Statistics","State-Space Model","Statistical Models","Statistical Methodology"],"title":"How hard is it to predict COVID-19 cases?","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["C++","Computer Programs"],"content":"Introduction When we think about writing a program in C, the first step is understand how variables should be assigned. There are several variable\u0026rsquo;s type in C, and here we are introducing the type int, which is used for integer data types. Basically, we can define a variable as an integer in two ways:\n* Uninitialized variable: defined as `int x;`, where no value is assign to the variable $x$ (Figure 1), which generally is not a good idea as it could lead to a bug in the algorithm if no value is assign over the code. * Initialized variable: there are two ways to assign a value to a variable $x$ (Figure 1):\r* in a single declaration - `int x = 3;`\r* in a double step declaration - `int x;` and `x = 3;`\r\r\rDeclaring variables in C\n\rAdditionally, there are a large set of storage size-specific declarations for a integer, and here we will explain just an initial idea about it. Figure 2 showns the Integer representation of whole numbers or fixed-point numbers (fixed number of digits). Generally, computers use a fixed number of bits to represent them, where commonly used bit-lengths for integers are 8-bit, 16-bit (short), 32-bit (long) or 64-bit (long long). There are two representation schemes for integers called signed integer type (signed int) capable of containing the range of values from -32,767 to 32,767, and unsigned integer type (unsigned int) containing the range of values from 0 to 65,535 ($32767 \\times 2+1$). Therefore, unsigned qualifier should be used when we are working with only positive values.\nInteger Representation\n\rFurthermore, there are three representation schemes for signed integers called Sign-Magnitude representation, 1\u0026rsquo;s Complement representation, and 2\u0026rsquo;s Complement representation. The 1‚Äôs and the 2‚Äôs complements of a binary number are important because they permit different representation for negative numbers. In all of these schemes, positive signed binary numbers starts with value 0 while negative ones starts with value 1 (Figure 3).\nSigned binary numbers\n\rConsequently, the disadvantage of signed binary numbers is that there is 1 bit used to store the sign positive or negative while the remaning $n-1$ bits are assign to the range of digits from $-2^{n-1}$ to $2^{n-1}$. If we have 8 bits to represent a signed binary number, we have to use 1 bit for the **sign bit** and 7 bits for the **magnitude bits**:\r* Using Sign-Magnitude Representation:\r$$-|2^{\\left(8-1\\right)}-1| \\mbox{ to } 2^{\\left(8-1\\right)}-1 = -127 \\mbox{ to } 127$$\r* Using 2's Complement Representation:\r$$-2^{\\left(8-1\\right)} \\mbox{ to } 2^{\\left(8-1\\right)}-1 = -128 \\mbox{ to } 127$$\r\rThus, we can representing the numbers ranging from -128 to 127 using 2's Complement Representation. Probably now you are asking why there is one extra number being accounted when using 2's Complement Representation. The answer can be found in the Figure 4.\rRepresentation schemes of Sign-Magnitude Representation and 2's Complement Representation\n\rExamples Unsigned int Supose we are interested in representing a sequence of number $x$ where $x \\in \\lbrace 0, 1, \\ldots, 15\\rbrace$. We can assign these numbers as unsigned numbers of 4 bits. Consequently, we have 4 zero bits associated to describe this numbers because our variable belongs to the interval $[0, 2^{4}‚àí1] \\in \\mathcal{N}_{0}$.\nRepresentation of numbers from 0 to 15 in 4 bits\r\r\rbits \r0000 \r0001 \r0010 \r0011 \r0100 \r0101 \r0110 \r0111 \r1000 \r1001 \r1010 \r1011 \r1100 \r1101 \r1110 \r1111 \r\r\rx \r0 \r1 \r2 \r3 \r4 \r5 \r6 \r7 \r8 \r9 \r10 \r11 \r12 \r13 \r14 \r15 \r\r\r\rSigned int Supose now we are interested in representing a sequence of number $y$ where $y \\in \\lbrace -7, -6, \\ldots,6, 7\\rbrace$. We have to assign them as signed numbers using 4 bits because 1 bit will be used for sign bit and 3 bits for the magnitude bits to describe $y \\in \\left[-|2^3-1|,2^3-1\\right] \\in \\mathcal{Z}$.\nSign-Magnitude Representation of numbers from -7 to 7 using 4 bits\r\r\rbits \r0111 \r0110 \r0101 \r0100 \r0011 \r0010 \r0001 \r0000 \r1000 \r1001 \r1010 \r1011 \r1100 \r1101 \r1110 \r1111 \r\r\ry \r7 \r6 \r5 \r4 \r3 \r2 \r1 \r0 \r-0 \r-1 \r-2 \r-3 \r-4 \r-5 \r-6 \r-7 \r\r\r\r2's Complement Representation of numbers from -8 to 7 using 4 bits\r\r\rbits \r1000 \r1001 \r1010 \r1011 \r1100 \r1101 \r1110 \r1111 \r0000 \r0001 \r0010 \r0011 \r0100 \r0101 \r0110 \r0111 \r\r\ry \r-8 \r-7 \r-6 \r-5 \r-4 \r-3 \r-2 \r-1 \r0 \r1 \r2 \r3 \r4 \r5 \r6 \r7 \r\r\r\rReferences Barnett R.; O\u0026rsquo;Cull L.; Cox, S. Embedded C Programming and the Microship PIC. Delmar Learning, ed. 1, 2004.\nCadenhead, R.; Liberty, J. Sams Teach Yoirself C++. Pearson Education, ed. 6, 2017.\nC Data Types - https://en.wikipedia.org/wiki/C_data_types\nDid you find this page helpful? Consider sharing it üôå\n","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608120651,"objectID":"ecf8761302e0db12a772dcb1425899c6","permalink":"https://prof-thiagooliveira.netlify.com/post/signed-and-unsigned-binary-numbers/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/signed-and-unsigned-binary-numbers/","section":"post","summary":"Introduction When we think about writing a program in C, the first step is understand how variables should be assigned. There are several variable\u0026rsquo;s type in C, and here we are introducing the type int, which is used for integer data types.","tags":["C++","Computer Science","Computer Programs"],"title":"Signed and Unsigned Binary Numbers","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["C++","Computer Programs"],"content":"\rOverview of the Seven Steps\rThe seven steps proposed by Hilton et al.¬†(2019) present an intriguing strategy for initiating a new project involving programming. This approach is concisely summarized in Figure 1. In this discussion, we will elaborate on these steps, drawing upon the work of Hilton et al.¬†(2019).\n\rFigure 1: The seven steps (modified from Hilton et al.¬†(2019))\r\rAll steps are then described in the sections below.\n\rStep 1 - Project definition using simple examples\rThis step involves dedicating time to conceptualize the project and breaking it down into manageable tasks. Begin by manually sketching a diagram of the project, highlighting key topics, strategies for addressing challenges, and estimating the number of primary algorithms required for completion. This should also encompass the subdivision of the project into smaller tasks, their interconnections, and any sequential order for their execution, as depicted in Figure 2. An effective approach in this stage is crucial as it simplifies the subsequent steps.\n\rFigure 2: Example of how divide the main project into small tasks\r\rExample 1\rImagine we need to develop a C++ algorithm to calculate the total fat content (\\(y\\)) of a portion of ice cream. Assume this response variable is determined by the amounts of butyric fat (\\(x_1\\)) and vegetable fat (\\(x_2\\)). Let \\(E[y]\\) represent the expected value of \\(y\\), defined as:\n\\[E[y]=10-0.5x_1+0.6x_1^2-0.6x_2+0.2x_2^2+0.1x_1x_2\\]\rWe can manually compute the total fat \\(y\\) for specific values of \\(x_1\\) and \\(x_2\\). For instance, if \\(x_1=2\\) and \\(x_2=1\\), then:\n\\[y=10-0.5\\times2+0.6\\times 2^2-0.6\\times 1+0.2 \\times 1^2+0.1\\times 2 \\times 1 = 11.2.\\]\nNow, let‚Äôs assume the secondary goal is to optimize the fat content in the ice cream formulation based on this model. This involves searching for the global minimum on the response surface. Consequently, we can break down our project into two tasks:\nGeneralize the function for any \\(x_1\\) and \\(x_2\\);\rCalculate the global (or absolute) minimum point;\r\r\rIf you encounter difficulties in these tasks, it‚Äôs often due to a gap in specific domain knowledge, such as a lack of expertise in mathematics:\n\rHow could I calculate the global minimum?\rHow can I use partial derivatives?\r\r\rTherefore, during this step, it is essential to identify all the necessary domain knowledge and address these gaps before proceeding to the next stage. Sometimes, this knowledge may stem from specialized areas such as computer science, sports, agriculture, statistics, or engineering.\n\r\rStep 2 - Write everything you did\rIn this phase, it is crucial to meticulously record every action undertaken to resolve the project‚Äôs challenges or tasks. Ensure that your notes are clear and detailed enough for others to replicate your solutions effortlessly. Be cautious not to overlook steps that might seem obvious, such as basic operations like multiplying \\(x\\) by \\(y\\), or the sequence in which tasks are to be executed.\nExample 2\rConsider the task of calculating \\(f(x, y) = x^y + 3x\\) for \\(x=2\\) and \\(y=4\\). Here‚Äôs how you could document the process in a detailed and comprehensible manner:\n\rMultiply 2 by 2 \\(\\rightarrow\\) you get 4\rMultiply 4 by 2 \\(\\rightarrow\\) you get 8\rMultiply 8 by 2 \\(\\rightarrow\\) you get 16\rSum 16 plus 3 multiplied by 2 \\(\\rightarrow\\) you get 22\r22 is the answer.\r\r\rBy following these explicit steps, anyone with basic mathematical skills should be able to reach the same result.\n\r\rStep 3 - Generalize\rThe goal now is to transform the specific steps from earlier into a universal algorithm that applies to a broader range of cases, not just specific parameter values. Here are two common methods to achieve this generalization:\n\rRe-examine the details from Step 2, as the key to generalization often lies within its description.\rIdentify repetitive patterns, particularly where the same step is executed multiple times.\r\r\rFor instance, let‚Äôs generalize our Example 2 by adapting the steps from Step 2, replacing the specific occurrences of 2 with a variable \\(x\\):\n\rMultiply 2 by \\(x\\) \\(\\rightarrow\\) you get 4\rMultiply 4 by \\(x\\) \\(\\rightarrow\\) you get 8\rMultiply 8 by \\(x\\) \\(\\rightarrow\\) you get 16\rSum 16 plus 3 multiplied by \\(x\\) \\(\\rightarrow\\) you get 22\r22 is the answer.\r\r\rIt is important to note that the initial multiplication should start with \\(x \\times x = x^2\\). Therefore, we multiply \\(x\\) by itself \\(y-1\\) times to obtain \\(x^y\\). This leads us to the following generalized steps for any values of \\(x\\) and \\(y\\):\nAlgorithm sketch 1\rstart with x = 2 and y = 4\rn[1] = x\rCount up from i in 1 to y-1 n[i+1] = n[i] * x z = n[y] + 3 * x\rz is the answer\rThis process is referred to as writing ‚Äòpseudo-code‚Äô as an algorithm design with no particular target language.\n\r\rStep 4 - Test Your Algorithm\rTesting your algorithm is a crucial step to ensure the correctness of steps 1-3 before advancing to step 5. Here are some key actions and considerations during this stage:\n\rTest your algorithm with varying parameter values.\rAssess the algorithm‚Äôs behavior for positive, negative, or zero values.\rDetermine if you have confined the parameter space, e.g., \\(y \\geq 0\\).\rEmploy mathematical proofs to validate your approach.\rRecognize that there may be more than one correct solution to a programming problem.\r\r\rRemember, the parameter space refers to the range of possible parameter values that define a specific mathematical or statistical model, typically within a subset of the finite-dimensional Euclidean space.\n\rAt times, the generalization in step 3 might be incomplete, leading to a revisit of steps 1-2. This oversight often occurs when not all potential cases are considered or when mathematical proofs are lacking.\nA notable example of an algorithmic error is seen in Example 2. What if \\(y=0\\) or \\(y\u0026lt;0\\)? Our algorithm incorrectly addresses these cases. For example, with \\(x=2\\) and \\(y=0\\), the algorithm erroneously calculates \\(2^0=2\\) instead of the correct \\(2^0=1\\). Also, for any \\(y \\leq 0\\), the algorithm erroneously tries to count from \\(1\\) to \\(y-1 \u0026lt; 0\\), which is not applicable for natural numbers, leading to an error. Therefore, we must ensure \\(|y| \\in \\mathcal{N}_{0}\\), where \\(\\mathcal{N}_{0}\\) represents the set of natural numbers including zero. Consequently, we should aim to generalize our algorithm to accommodate a broader range of cases:\nAlgorithm Sketch 2 y must be an integer number.\rStart with x = 2 and y = 4.\rIf y=0 {\rn[1] = 1;\ri=0;\r} else {\rCount from i = 1 to |y|-1;\rIf y \u0026lt; 0 {\rn[1] = 1/x;\rn[i+1] = n[i] * (1/x);\r} else {\rn[1] = x;\rn[i+1] = n[i] * x;\r}\r}\rz = n[i+1] + 3 * x;\rz is the answer.\rQuestion: How can we improve this algorithm? Consider the case where \\(x=y=0\\).\n\rWhen encountering problems with our algorithm at this stage, we have two options:\nReturn to steps 1-3 to gather more information for broadening the algorithm‚Äôs applicability.\rDirectly fix the algorithm in step 4 when the solution is known.\r\r\rExample 3\rThe data in Figure 4 originates from an algorithm that accepts a single parameter, \\(N\\), belonging to the set \\(\\mathcal{N}_{0}\\), where \\(\\mathcal{N}_{0} = \\mathcal{N} \\cup \\{ 0 \\}\\) denotes the set of natural numbers including zero. This algorithm generates a sequence of output values corresponding to each specified value of \\(N\\).\n\rFigure 3: Output of sequences of integers based on values of \\(N\\) from 0 to 4\r\rQuestion: Can you deduce the algorithm that produced the numbers in this figure? Additionally, what would be the result for \\(N=5\\)?\n\r\r\rReferences\r[1] Hilton, AD; Lipp, GM; Rodger, SH, Translation from Problem to Code in Seven Steps, Comped 2019 Proceedings of the Acm Conference on Global Computing Education (2019), pp.¬†78-84.\n\rAnswers\rExample 3\rAlgorithm Sketch 3\rSet N as a non-negative integer (Natural number with zero).\rInitialize N with a specific value n.\rDefine the sequence parameters:\rMinimum Value = 4 * N\rMaximum Value = 9 * N + 6\rSequence Increment = 3\rSet x[1] to the Minimum Value.\rIteratively calculate the sequence:\rFor each iteration i,\rif x[i-1] \u0026lt; Maximum Value,\rthen x[i] = x[i-1] + Sequence Increment.\relse,\rbreak the loop.\rThe sequence x represents the final answer.\r# N = 6\rN=5\rseq \u0026lt;- seq(4*N, 9*N+6, 3)\rcat(\u0026quot;The answer is\u0026quot;, seq)\r## The answer is 20 23 26 29 32 35 38 41 44 47 50\r\r\rCitation\rFor attribution, please cite this work as:\r\rOliveira T.P. (2020, Dec.¬†16). The seven steps of a programer\n\rBibTeX citation\r\r@misc{oliveira2020seven,\rauthor = {Oliveira, Thiago},\rtitle = {The seven steps of a programer},\rurl = {https://prof-thiagooliveira.netlify.app/post/the-seven-steps-of-a-programer/},\ryear = {2020}\r}\rDid you find this page helpful? Consider sharing it üôå\n\r","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608119583,"objectID":"2a434fb16cdef7db0fdaca23027b096c","permalink":"https://prof-thiagooliveira.netlify.com/post/the-seven-steps-of-a-programer/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/the-seven-steps-of-a-programer/","section":"post","summary":"Overview of the Seven Steps\rThe seven steps proposed by Hilton et al.¬†(2019) present an intriguing strategy for initiating a new project involving programming. This approach is concisely summarized in Figure 1.","tags":["Algorithm","C++","Computer Science"],"title":"The seven steps of a programer","type":"post"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral"],"categories":null,"content":"Abstract:\nThe continuously growing number of COVID-19 cases pressures healthcare services worldwide. Accurate short-term forecasting is thus vital to support country-level policy making. The strategies adopted by countries to combat the pandemic vary, generating different uncertainty levels about the actual number of cases. Accounting for the hierarchical structure of the data and accommodating extra-variability is therefore fundamental. We introduce a new modelling framework to describe the course of the pandemic with great accuracy, and provide short-term daily forecasts for every country in the world. We show that our model generates highly accurate forecasts up to six days ahead, and use estimated model components to cluster countries based on recent events. We introduce statistical novelty in terms of modelling the autoregressive parameter as a function of time, increasing predictive power and flexibility to adapt to each country. Our model can also be used to forecast the number of deaths, study the effects of covariates (such as lockdown policies), and generate forecasts for smaller regions within countries. Consequently, it has strong implications for global planning and decision making. We constantly update forecasts and make all results freely available to any country in the world through an online Shiny dashboard.\n","date":1605178800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605178800,"objectID":"e2d66752b29503eaf43d2b19263b5f0e","permalink":"https://prof-thiagooliveira.netlify.com/talk/global-short-term-forecasting-of-covid-19-cases/","publishdate":"2020-11-10T12:00:00Z","relpermalink":"/talk/global-short-term-forecasting-of-covid-19-cases/","section":"event","summary":"Accurate short-term forecasting is thus vital to support country-level policy making during COVID-19 outbreak","tags":["COVID-19","Outbreak","State-Space Model","Longitudinal Data"],"title":"Global Short-Term Forecasting of Covid-19 Cases","type":"event"},{"authors":["Heloisa Thomazi Kleina1","Karla Kudlawiec","Mariana B. Esteves,","Marco A. Dalb√≥","Thiago de Paula Oliveira","Nathalie Maluta","Jo√£o R. S. Lopes","Louise L. May-De-Mio1"],"categories":null,"content":"","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597622400,"objectID":"4d8941c87310fec1359b93fd5113237a","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-plant_pathology/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/publication/2020-plant_pathology/","section":"publication","summary":"We focused to investigate the host non preference and suitability for xylem-sap feeding among plum genotypes that differ in bacterial leaf scald intensity in the field.","tags":["Phytopathology","Statistical Modelling","Experimental design","Agriculture","Mixed-Effects Models","Generalized linear models","Hierarchical data"],"title":"Settling and feeding behavior of sharpshooter vectors on plum genotypes with different susceptibility levels to leaf scald disease (Xylella fastidiosa)","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral","Silvio Sandoval Zocchi","Clarice G. B. Demetrio","John Hinde"],"categories":null,"content":"Supplementary notes were added here:\n lcc package  Github: https://github.com/Prof-ThiagoOliveira/lcc CRAN: https://CRAN.R-project.org/package=lcc    ","date":1597276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597276800,"objectID":"e82d87d3e10df07ca946d00c59174724","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-lccpeerj/","publishdate":"2020-08-13T00:00:00Z","relpermalink":"/publication/2020-lccpeerj/","section":"publication","summary":"Describes the statistical package lcc using three real examples.","tags":["R Package","Bootstrap Confidence Intervals","Statistical Methods","Longitudinal data","Concordance Correlation Coefficient","Accuracy","Precision","Mixed-Effects Model"],"title":"lcc: an R package to estimate the concordance correlation, Pearson correlation, and accuracy over time","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral"],"categories":null,"content":"Abstract:\nThe continuously growing number of COVID-19 cases pressures healthcare services worldwide. Accurate short-term forecasting is thus vital to support country-level policy making. The strategies adopted by countries to combat the pandemic vary, generating different uncertainty levels about the actual number of cases. Accounting for the hierarchical structure of the data and accommodating extra-variability is therefore fundamental. We introduce a new modelling framework to describe the course of the pandemic with great accuracy, and provide short-term daily forecasts for every country in the world. We show that our model generates highly accurate forecasts up to six days ahead, and use estimated model components to cluster countries based on recent events. We introduce statistical novelty in terms of modelling the autoregressive parameter as a function of time, increasing predictive power and flexibility to adapt to each country. Our model can also be used to forecast the number of deaths, study the effects of covariates (such as lockdown policies), and generate forecasts for smaller regions within countries. Consequently, it has strong implications for global planning and decision making. We constantly update forecasts and make all results freely available to any country in the world through an online Shiny dashboard.\n","date":1591016400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591016400,"objectID":"7924c6f4fa53e4a406ec75819fe98ecf","permalink":"https://prof-thiagooliveira.netlify.com/talk/global-short-term-forecasting-of-covid-19-cases/","publishdate":"2020-05-30T00:00:00Z","relpermalink":"/talk/global-short-term-forecasting-of-covid-19-cases/","section":"event","summary":"Accurate short-term forecasting is thus vital to support country-level policy making during COVID-19 outbreak","tags":["COVID-19","Outbreak","State-Space Model","Longitudinal Data"],"title":"Global Short-Term Forecasting of Covid-19 Cases","type":"event"},{"authors":["Thiago de Paula Oliveira","John Newell"],"categories":null,"content":"Abstract:\nIn basketball, the athlete performance evaluation are generally based on variants of plus-minus and PER statistics Optimizing Athlete Performance calculated through multiple regression, ridge, or lasso models using likelihood-based or Bayesian approach. We developed a novel methodology based on principal components analysis and multilevel model to create new indexes such as Oliveira-Newell Score that can be used to evaluate player performance during a match, relevance score used to rank players in a season, and the consistence score used to evaluate the player contribution for their team based on random effects.\n","date":1587463200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587463200,"objectID":"78bc82b7e9a4f5e1a4fef630537a702d","permalink":"https://prof-thiagooliveira.netlify.com/talk/estimating-nba-athlete-performance-using-multilevel-models/","publishdate":"2020-04-21T10:00:00Z","relpermalink":"/talk/estimating-nba-athlete-performance-using-multilevel-models/","section":"event","summary":"Athlete performance evaluation based on novel methodology using principal components and multilevel models","tags":["Athlete Performance","Agreement Measures","Multilevel Model","Longitudinal Data"],"title":"Estimating NBA athlete performance using multilevel models","type":"event"},{"authors":null,"categories":null,"content":"Plant breeding programmes are a complex network of a multitude of operations and decisions. Quantifying the drives of genetic change in such programmes is challenging. Traditionally we measure the genetic change with a phenotypic or genetic trend, but these measures only change the overall genetic mean. To understand the genetic change more comprehensively, we also need to measure the change in genetic variance and drives of mean and variance changes. To quantify the drives of genetic change in mean we can i) partition breeding values into parent average and Mendelian sampling terms, ii) allocate the terms to analyst-defined \"paths\" (specific individuals or groups of individuals), and iii) summarise the path specific terms to quantify path contributions to the overall genetic trend in mean. We have used the partitioning method in several cases with profound results:\n  To estimate the contribution of different cattle breeding programmes globally and in particular countries;  To estimate the contribution of national selection and import in cattle; and To evaluate national selection and import in pig breeds.  This project aims to apply the partitioning method to plant breeding programmes and to expand its versatility. Specifically, we are aiming to:\r  Utilise genomic information to identify which genome regions drive genetic change and which sources contribute to favourable alleles in these genome regions  Analyse changes in genetic variance in addition to the genetic mean Account for uncertainty in genetic trends and their partitions, and Develop an R package  ","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"a079055becb5e8745abc07e89c75c78b","permalink":"https://prof-thiagooliveira.netlify.com/project/genetic-project/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/project/genetic-project/","section":"project","summary":"Quantitative genetics is a field of biology studying the effect of genetic and environmental factors on quantitative traits.","tags":["Statistics","Statistical modelling","Genetic","Genomics","Plant breeding"],"title":"Statistical models applied to quantitative genetics and genomics of plant breeding","type":"project"},{"authors":["Thiago de Paula Oliveira","John Newell"],"categories":null,"content":"Abstract:\nThe ability to predict menstrual cycle length to a high degree of precision enables female athletes to track their period and tailortheir training and nutrition correspondingly knowing when to push harder when to prioritise recovery and how to minimise theimpact of menstrual symptoms on performance. Such individualisation is possible if cycle length can be predicted to a highdegree of accuracy. To achieve this, a hybrid predictive model was built using data on 16,990 cycles collected from a sampleof 2,178 women (mean age 33.89 years, range 14.98 - 47.10, number of menstrual cycles ranging from 4 - 53). To capture thewithin-subject temporal correlation, a mixed-effect state-space model was fitted incorporating a Bayesian approach for processforecasting to predict the duration (in days) of the next menstrual cycle. The modelling procedure was split into three steps(i)a time trend component using a random walk with an overdispersion parameter, (ii) an autocorrelation component using anautoregressive moving-average (ARMA) model, and (iii) a linear predictor to account for covariates (e.g. injury, stomach cramps,training intensity). The inclusion of an overdispersion parameter suggested that26.81% [24.14%,29.58%]of cycles in the samplewere overdispersed where the random walk standard deviation under a non-overdispersed cycle is1.0530 [1.0060,1.0526]days whileunder an overdispersed cycle it increased to4.7386 [4.5379,4.9492]days. To assess the performance and prediction accuracy ofthe model, each woman‚Äôs last observation was used as test data. The Root Mean Square Error (RMSE), Concordance CorrelationCoefficient (CCC) and Pearson correlation coefficient (r) between the observed and predicted values were calculated. The modelhad an RMSE of 1.6126 days, a precision of 0.7501 and overall accuracy of 0.9855. In the absence of hormonal measurements,knowing how aspects of physiology and psychology are changing across the menstrual cycle has the potential to help femaleathletes personalise their training, nutrition and recovery tailored to their cycle to sustain peak performance at the highest leveland gain a competitive edge. In conclusion, the hybrid model presented here is a useful approach for predicting menstrual cyclelength which in turn can be used to support female athlete wellness to optimise performance\r","date":1570179600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570179600,"objectID":"27444c4282bba3c0d95ddb2c8d585400","permalink":"https://prof-thiagooliveira.netlify.com/talk/modelling-menstrual-cycle-length-using-state-space-models/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/modelling-menstrual-cycle-length-using-state-space-models/","section":"event","summary":"Times are changing. At an elite level, female athletes and coaches across the globe are now starting to work with the menstrual cycle to gain a performance edge. By tracking the menstrual cycle, and knowing how, why and when hormone fluctuations affect female physiology, an athlete's training, nutrition and recovery can be tailored to their cycle to sustain peak performance\n","tags":["Bayesian Approach","State Space Models","Cycle Length","Performance","Autoregressive Models"],"title":"Modelling menstrual cycle length using state space models","type":"event"},{"authors":["Gustavo V. Popin","Arthur K. B. Santos","Thiago de Paula Oliveira","Pl√≠nio B. de Camargo","Carlos E. P. Cerri","Marcos Siqueira-Neto"],"categories":null,"content":"Supplementary notes were added here, including figures.\n","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563235200,"objectID":"5e8883e7dec03615d2d23522b2cb0e5a","permalink":"https://prof-thiagooliveira.netlify.com/publication/2019-soil/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-soil/","section":"publication","summary":"Evaluate the effects of global warming and straw removal on gas emissions","tags":["Global warming","Statistical Modelling","Gas emissions"],"title":"Sugarcane straw management for bioenergy: effects of global warming on greenhouse gas emissions and soil carbon storage","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral","John Hinde","Silvio Sandoval Zocchi","Clarice Garcia Borges Demetrio"],"categories":null,"content":"Abstract:\nWe present the lcc package, available from the Comprehensive R Archive Network (CRAN). The package implements estimation procedures for the longitudinal concordance correlation (LCC), using fixed effects and variance components estimates from linear mixed models. The LCC is a quantity that measures the extent of agreement between two (or more) methods used to evaluate a response variable of interest and is frequently applied in medicine, pharmacology, and agronomy. The main features of the package are the estimation and inference of the extent of agreement using numerical and graphical summaries. Moreover, our approach presents flexibility in the sense that it accommodates both balanced and unbalanced experimental designs, allows for different within-group error structures, while also allowing for the inclusion of covariates in the linear predictor to control systematic variations in the response. We illustrate our methodology by comparing different methods used to measure the peel colour of fruit as an assessment of ripeness.\n","date":1559898000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559898000,"objectID":"502f85002fd9bb5031c4a64b2cc56113","permalink":"https://prof-thiagooliveira.netlify.com/talk/the-longitudinal-concordance-correlation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/the-longitudinal-concordance-correlation/","section":"event","summary":"Abstract:\nWe present the lcc package, available from the Comprehensive R Archive Network (CRAN). The package implements estimation procedures for the longitudinal concordance correlation (LCC), using fixed effects and variance components estimates from linear mixed models.","tags":["R Package","Agreement Measures","Mixed-Effects Model","Longitudinal Data"],"title":"The longitudinal concordance correlation","type":"event"},{"authors":null,"categories":null,"content":"Utilizing statistical frameworks in sports analytics enables more precise evaluations of athlete performance and team dynamics. This advancement provides coaches and stakeholders with deeper insights, fostering informed decisions that can significantly elevate a team's competitive edge.\r","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"3603a336fa84cc99d0a9988da59d5e03","permalink":"https://prof-thiagooliveira.netlify.com/project/sports-project/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/project/sports-project/","section":"project","summary":"Developed statistical frameworks for sports analytics","tags":["Statistics","Statistical modelling","Sports"],"title":"Development and Application of Statistical Frameworks in Sports Analytics","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne \r**Two** \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://prof-thiagooliveira.netlify.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral","Silvio S. Zocchi","Clarice G. B. Demetrio","John Hinde"],"categories":null,"content":"","date":1542326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542326400,"objectID":"45351a12a748374de8bdd61a5c271dc4","permalink":"https://prof-thiagooliveira.netlify.com/publication/2019-lcc-package/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-lcc-package/","section":"publication","summary":"We proposed an R package to estimate the longitudinal concordance correlation (LCC)","tags":["R Package","Bootstrap Confidence Intervals","Statistical Methods","Longitudinal data","Concordance Correlation Coefficient","Accuracy","Precision","Mixed-Effects Model"],"title":"lcc: Longitudinal Concordance Correlation","type":"publication"},{"authors":["Mariana B. Esteves","Heloisa T. Kleina","Tiago de Melo Sales","Thiago de Paula Oliveira","Idemauro A.R. Lara","Rodrigo P.P. Almeida","Helvecio D. Coletta-Filho","Jo√£o R.S. Lopes"],"categories":null,"content":"","date":1541808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541808000,"objectID":"6af98efad9f8f156e5616058ca2fbfe4","permalink":"https://prof-thiagooliveira.netlify.com/publication/2019-phitopatology/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-phitopatology/","section":"publication","summary":"Adapt and validate an in vitro acquisition system for X. fastidiosa by sharpshooters.","tags":["Phytopathology","Statistical Modelling","Experimental design","Agriculture","Mixed-Effects Models","Generalized linear models"],"title":"Transmission efficiency of xylella fastidiosa subsp. Pauca sequence types by sharpshooter vectors after in vitro acquisition","type":"publication"},{"authors":["Thiago de Paula Oliveira","Silvio S. Zocchi","John Hinde"],"categories":null,"content":"Supplementary notes were added here, including code and data.\n","date":1521676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521676800,"objectID":"f1a5a0e3d674f9b95525b671e1f8a5a5","permalink":"https://prof-thiagooliveira.netlify.com/publication/2018-lcc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2018-lcc/","section":"publication","summary":"We proposed a longitudinal concordance correlation (LCC) to estimate agreement over time among methods","tags":["Longitudinal data","Concordance Correlation Coefficient","Accuracy","Precision","Statistical Modelling","Statistical Methods","R Package","Bootstrap Confidence Intervals","Mixed-Effects Model"],"title":"Longitudinal concordance correlation function based on variance components: an application in fruit color analysis","type":"publication"},{"authors":["Thiago de Paula Oliveira","Silvio S. Zocchi","Angelo P. M. Jacomino"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0352785b21d368da5202302fd9ba162c","permalink":"https://prof-thiagooliveira.netlify.com/publication/2017-papaya/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017-papaya/","section":"publication","summary":"It is recommended the usage of digital image analysis to access the fruit peel color when it has non-uniform coloration.","tags":["Fruit","Postharvest","Colour","Statistics"],"title":"Measuring color hue in 'Sunrise Solo' papaya using a flatbed scanner","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://prof-thiagooliveira.netlify.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]