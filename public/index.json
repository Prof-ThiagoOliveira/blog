[{"authors":null,"categories":null,"content":"I hold a PhD in Statistics from the the University of S√£o Paulo, Brazil, and have at least eight years of experience in experimental statistics and statistical modelling. As a PhD student, in 2016, I was a visiting Scholar at the National University of Ireland Galway, Ireland, where I worked with statistical modelling in agricultural data. Moreover, I worked as a lecturer in the Department of Exact Sciences at ESALQ/University of S√£o Paulo, Piracicaba, S√£o Paulo, Brazil, from 2017 to 2019. I also worked as a Researcher Biostatistician from 2019-2020 at Insight Centre for Data Analytics in partnership with Orreco, School of Mathematics, Statistics, and Applied Mathematics, and NUI Galway in the development of statistical methods applied to athlete performance, and predictive models for COVID-19. I developed statistical methods in longitudinal concordance correlation, multilevel model (hierarchical model), generalized linear mixed-effects model, state-space models, experimental design, longitudinal data. As an enthusiast of using dashboard apps to create an interactive data visualization, I believe apps are easier to make visual representations of large scale data sets. It also allows the user to explore the complex reality of the database or even handle multiple data locations in a single visualization. I was recently awarded a Marie Sk≈Çodowska-Curie Actions (MSCA) COFOUND Fellowship (Train@Ed) to work at The Roslin Institute , University of Edinburgh, where I currently work on the development of statistical models applied to quantitative genetics genomics of plant breeding in partnership with Limagrain. ","date":1635724800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1635724800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://prof-thiagooliveira.netlify.com/author/thiago-de-paula-oliveira/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/thiago-de-paula-oliveira/","section":"authors","summary":"I hold a PhD in Statistics from the the University of S√£o Paulo, Brazil, and have at least eight years of experience in experimental statistics and statistical modelling. As a PhD student, in 2016, I was a visiting Scholar at the National University of Ireland Galway, Ireland, where I worked with statistical modelling in agricultural data.","tags":null,"title":"Thiago de Paula Oliveira","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"https://prof-thiagooliveira.netlify.com/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"Table of Contents  Summary    Summary I contributed with two talks related to the tidyverse world. In the first talk, I covered the grammar of graphics using ggplot2 and, in the second one, I showed how to use tidyverse functions to prepare tidy data.\n","date":1631955600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1631955600,"objectID":"56ca024d2ed51f518606266ad8e5658a","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/","publishdate":"2021-09-18T09:00:00Z","relpermalink":"/courses/2021_ddbg/","section":"courses","summary":"Overview of the basics experimental designs","tags":null,"title":"Breeding Programme Modelling with AlphaSimR","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Summary    Summary Planning observational studies, and experiment are one of the essential steps in scientific methodology. Unfortunately, in some cases, the statistical planning is neglected, leading to an analysis, sometimes, that does not answer properly the questions of researcher interest. When we are planning an experiment, we should account for non-biased samples, at same time that we consider the availability of experimental material. Moreover, we should to establish the experimental design with or without randomization restrictions.\n","date":1535360400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1535475600,"objectID":"06e45f2789bdef92c34686e706f56260","permalink":"https://prof-thiagooliveira.netlify.com/courses/2019_workshop/","publishdate":"2018-08-27T09:00:00Z","relpermalink":"/courses/2019_workshop/","section":"courses","summary":"Overview of the basics experimental designs","tags":null,"title":"I Workshop in introduction of experimental design","type":"docs"},{"authors":null,"categories":null,"content":"          ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"b9d4fcbb902407e45834ac9cc593f7d4","permalink":"https://prof-thiagooliveira.netlify.com/gallery/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/gallery/","section":"gallery","summary":"          ","tags":null,"title":"Gallery","type":"docs"},{"authors":null,"categories":null,"content":"Did you find this page helpful? Consider sharing it üôå ","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"246300c7e70a1142a658f22bdbb04f77","permalink":"https://prof-thiagooliveira.netlify.com/post/experimental_design/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/post/experimental_design/","section":"post","summary":"Did you find this page helpful? Consider sharing it üôå ","tags":null,"title":"Main elements and questions of a good experimental plan.","type":"post"},{"authors":null,"categories":null,"content":"R code containing the basics on how to randomize an experiment.\nTip 1 Example of a completely randomized design.\n############################################################################ #################### Completely Randomized Design ########################## ############################################################################ # Type of Rootstock Trat \u0026lt;- gl(5,1,labels = c(\u0026quot;P1\u0026quot;, \u0026quot;P2\u0026quot;, \u0026quot;P3\u0026quot;, \u0026quot;P4\u0026quot;, \u0026quot;Controle\u0026quot;)) # Repetitions Rep \u0026lt;- 5 # Draw of treatments to plots DIC \u0026lt;- function(Trat,Rep){ Trat \u0026lt;- rep(Trat,Rep) N \u0026lt;- length(levels(Trat))*Rep # N√∫mero total de parcelas Plan\u0026lt;-as.data.frame( matrix( sample(Trat,N), ncol = length(levels(Trat)), nrow=Rep ) ) colnames(Plan)\u0026lt;-paste(\u0026quot;Coluna\u0026quot;, c(seq(1:length(levels(Trat))))) rownames(Plan)\u0026lt;-paste(\u0026quot;Linha\u0026quot;, c(seq(1:Rep))) return(Plan) } # Experiment Sketch DIC(Trat,Rep)  Tip 2 Example of a Randomized Block Design.\n############################################################################ ################### Randomized Block Design ################################ ############################################################################ # A researcher wishes to evaluate the color, odor and consistency of ruminal # juice samples from cattle of same breed, who are treated with 3 types of # feeds. As a restriction ofexperiment implementation, we can confine up to # 4 cattle per sector and the maximum number of sectors available is 5. # In addition, those animals were classified into three groups of carcasses: # i) light (226-228 kg), ii) medium (241-243 kg) and iii) higher (259-261 Kg). # Feed Trat \u0026lt;- gl(3,1,labels = c(\u0026quot;Ra√ß√£o 1\u0026quot;, \u0026quot;Ra√ß√£o 2\u0026quot;, \u0026quot;Ra√ß√£o 3\u0026quot;)) # Number of Blocks - carcass groups Bloco \u0026lt;- gl(5,1,labels = c(\u0026quot;Bloco I\u0026quot;, \u0026quot;Bloco II\u0026quot;, \u0026quot;Bloco III\u0026quot;, \u0026quot;Bloco IV\u0026quot;, \u0026quot;Bloco V\u0026quot;)) # Total number of plots N \u0026lt;- length(levels(Trat))*length(levels(Bloco)) # Draw of treatments to plots within blocks DCB \u0026lt;- function(Trat,Bloco){ Trat_Bloco \u0026lt;- list(NA) for(i in 1:length(levels(Bloco))){ Trat_Bloco[[i]]\u0026lt;-matrix( sample(Trat,length(levels(Trat)))) } Plan \u0026lt;- do.call(cbind.data.frame, Trat_Bloco) colnames(Plan) \u0026lt;- c(levels(Bloco)) rownames(Plan) \u0026lt;- paste(\u0026quot;Linha\u0026quot;, c(1:length(levels(Trat)))) return(Plan) } # Experiment Sketch DCB(Trat,Bloco)  Tip 3 Example of a completely randomized designs for factorial structure\n############################################################################ ################## Completely randomized designs ########################### ####################### Factorial structure ################################ ############################################################################ # Five soy cultivar Cultivar \u0026lt;- gl(5,1,labels=c(\u0026quot;BRS 1003IPro\u0026quot;, \u0026quot;BRS 1007IPro\u0026quot;, \u0026quot;BRS 1010IPro\u0026quot;, \u0026quot;BRS 1074IPro\u0026quot;, \u0026quot;BRS 706IPro\u0026quot;)) # Four nutrient solution with levels of 0.1.2, and 4 mg / liter of manganese. Solucao \u0026lt;- gl(4,1,labels = c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;4\u0026quot;)) # Treatments Trat \u0026lt;- expand.grid(Cultivar = Cultivar, Solu√ß√£o = Solucao) Trat$Tratamento \u0026lt;- as.factor(paste(Trat$Cultivar,Trat$Solu√ß√£o)) # Repetition Rep \u0026lt;- 10 # Draw of treatments to plots DIC \u0026lt;- function(Trat,Rep){ N \u0026lt;- length(levels(Trat))*Rep # Total number of plots Trat \u0026lt;- rep(Trat,Rep) Plan \u0026lt;- as.data.frame( matrix( sample(Trat,N), ncol = Rep, nrow=length(levels(Trat)) ) ) colnames(Plan) \u0026lt;- paste(\u0026quot;Coluna\u0026quot;, c(seq(1:Rep))) rownames(Plan) \u0026lt;- paste(\u0026quot;Linha\u0026quot;, c(seq(1:length(levels(Trat))))) return(Plan) } # Experiment Sketch DIC(Trat$Tratamento,Rep)  Tip 4 Example of a Latin Square Design\n############################################################################ ######################## Latin Square Design ############################### ############################################################################ L1 \u0026lt;- gl(4,1,labels = c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;)) L2 \u0026lt;- gl(4,1,labels = c(\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;A\u0026quot;)) L3 \u0026lt;- gl(4,1,labels = c(\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;)) L4 \u0026lt;- gl(4,1,labels = c(\u0026quot;D\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;C\u0026quot;)) Trat \u0026lt;- rbind(paste0(L1),paste0(L2),paste0(L3),paste0(L4)) QL \u0026lt;- function(Trat){ #Sorteando as linhas Linha \u0026lt;- Trat[sample(nrow(Trat),size=ncol(Trat)),] Coluna \u0026lt;- Linha[, sample(nrow(Linha),size=ncol(Linha))] return(Coluna) } QL(Trat)  ","date":1535360400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535360400,"objectID":"8b34388f306b41795f09acd044fa1ee8","permalink":"https://prof-thiagooliveira.netlify.com/courses/2019_workshop/visualization/","publishdate":"2018-08-27T09:00:00Z","relpermalink":"/courses/2019_workshop/visualization/","section":"courses","summary":"R code containing the basics on how to randomize an experiment.","tags":null,"title":"R Code","type":"book"},{"authors":null,"categories":null,"content":"   Summary 2021   Summary  2021  Workflows with Nextflow, University of Edinburgh Introduction to Bash Shell Scripting, Coursera Project Network   ","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"d54c26bac14317e687b42b74dba30877","permalink":"https://prof-thiagooliveira.netlify.com/accomplishments/online_courses/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/accomplishments/online_courses/","section":"accomplishments","summary":" ","tags":null,"title":"Online Courses","type":"book"},{"authors":null,"categories":null,"content":"Visualization and Data Structure What is the grammar of graphics? It is a tool that enables us to concisely describe the components of a graphic and provides a strong foundation for understanding a diverse range of graphics.\nPart I:  Part II:  Slides  R Code Exercises #======================================================================= # Packages #======================================================================= if (!require(\u0026quot;pacman\u0026quot;)) { install.packages(\u0026quot;pacman\u0026quot;) } # Include all packages here pacman::p_load( knitr, tidyverse, kableExtra, prettycode, formattable, DT, AlphaSimR, patchwork, # ggplot design tufte, # quotes ggridges, pathwork, ggpmisc, egg, datarium, tools ) prettycode::prettycode()  If you\u0026rsquo;re at all familiar with ggplot, you may know the basic structure of a call to the ggplot() function. For an introduction to the ggplot package, you can check out the visualization and data structure lecture. When you call ggplot, you should provide a data source, usually a data frame or tibble. Afterward, you can build a ggplot by mapping different variables in your data source to different aesthetics. For example, there are colour, shape, size of points, the position of the x or y-axis, ans so on. To demonstrate this and even more procedures using ggplot, we will work with different data sets such as:\n Growth of Orange Trees: the Orange data frame has 35 rows and 3 columns of records of the growth of orange trees.  glimpse(Orange)   Storm tracks data: This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.  glimpse(storms)   Carbon Dioxide Uptake in Grass Plants: The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.  glimpse(CO2)   Simulated cattle breeding program (cbp): the cbp data frame has 10,000 rows and 7 columns of data from a simulation using the AlphaSimR package. The data is comprised of founders and phenotyped individuals (on the Milk Yield), where for each one, we have information of parents, sex, and herd.  cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;) glimpse(cbp)  Mapping p \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex)) p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE))  Practice Use the data CO2 (Biochemical Oxygen Demand) to build a layer with geom = \u0026quot;point\u0026quot; using conc as the response variable, uptake as an explanatory variable, and colour the points by Treatment.\ndata(CO2) pEx1 \u0026lt;- ggplot(data = , aes(x = , y = , color = )) pEx1 + layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE))  Facets # facet_wrap p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + facet_wrap(facet = \u0026quot;sex\u0026quot;) # facet_grid p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + facet_grid(rows = vars(sex), cols = vars(herd))  Practice  Using the previous exercise as a starting point, add a facet_wrap() using Plant as a subgroup.  pEx1 + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + facet_wrap()  Make a plot using the storms database considering ts_diameter as response, hu_diameter as explanatory variable, and add a facet_grid() using rows as status and columns as category.  ggplot(data = , aes(y= , x=)) + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + facet_grid(rows = , cols = )  Scale p2 \u0026lt;- p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + facet_grid(rows = vars(sex), cols = vars(herd)) + scale_colour_manual(values = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#FC4E07\u0026quot;)) p2 p2 + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;)  Practice Make a plot using the storms database using ts_diameter as a response, pressure as an explanatory variable, colored by  status. Also add a facet_wrap() by category and do a scale transformation on the x and y axis using a log10() function.\nggplot(data = storms, aes(y= , x= )) + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + facet_wrap(...) + scale_y_*() + scale_x_*() + ylab(expression(log[10](\u0026quot;ts_diameter\u0026quot;))) + xlab(expression(log[10](\u0026quot;pressure\u0026quot;)))  Statistics # facet_wrap data(Orange) ggplot(data = Orange, aes(x = age, y = circumference)) + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + layer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(method = \u0026quot;lm\u0026quot;, se = FALSE)) ggplot(data = cbp, aes(x = year, y = phenotype, group = year)) + layer(geom = \u0026quot;boxplot\u0026quot;, stat = \u0026quot;boxplot\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(outlier.colour=\u0026quot;red\u0026quot;, outlier.shape = 1, na.rm = FALSE)) + facet_grid(rows = vars(sex), cols = vars(herd))  Practice The main difference between the regression line and LOESS is that while the regression line is a straight line representing the relationship between the x and y, a LOESS line is mainly used to identify trends in the data. Faceting allows you to split the data into subgroups, build a plot adding points, a regression line, and a LOESS curve for each Tree level in the Orange data set. For this, you can use the formula circumference ~ age.\nggplot(data = Orange, aes(x = , y = )) + layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE)) + layer(geom = , stat = , position = \u0026quot;identity\u0026quot;, params = list(method = , se = FALSE)) + layer(geom = , stat = , position = \u0026quot;identity\u0026quot;, params = list(method = , se = FALSE, colour = \u0026quot;red\u0026quot;)) + facet_wrap(...)  Geometries ggplot(data = Orange, aes(x = age, y = circumference)) + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(shape=1, na.rm = FALSE, width = 0.1, height = 0.1)) + layer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;, params = list(method = \u0026quot;lm\u0026quot;, se = FALSE)) + facet_wrap(facet = \u0026quot;Tree\u0026quot;)  Examples # Density plot p \u0026lt;- ggplot(data = cbp) p + geom_density(aes(x=phenotype)) # Density plot per year p + geom_density_ridges(aes(x = phenotype, y = year, fill = year)) # Histogram - absolute frequency of storms per month and year ggplot(data = storms, aes(x = year)) + facet_wrap(~month)+ geom_histogram(fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+ ylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Histogram - relative frequency of storms per month ggplot(data = storms, aes(x = year)) + geom_histogram(aes(y=stat(count)/sum(..count..)), fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+ scale_y_continuous(labels=scales::percent) + ylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Histogram - absolute frequency of storms per year (stack by month) ggplot(data = storms, aes(x = year, fill = factor(month))) + geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;stack\u0026quot;)+ ylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + labs(fill=\u0026quot;Month\u0026quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Histogram - proportion of storms per month by year ggplot(data = storms, aes(x = year, fill = factor(month))) + geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+ ylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + labs(fill=\u0026quot;Month\u0026quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Exercise: build a histogram with relative frequency per month within year # Ribbon and stat_summary - Plot of a time-series ggplot(data = Orange, aes(x = age, y = circumference)) + stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;, alpha = 0.3, fun.max = function(x) mean(x) + 2*sd(x), fun.min = function(x) mean(x) - 2*sd(x), fill = \u0026quot;#00AFBB\u0026quot;) + stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) + stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;, size = 2) + geom_point(shape=1)  Practice A violin plot is a method of plotting numeric data, and it is similar to a box plot but with a rotated kernel density plot on each side. Violin plots can show the probability density of the data at different values, usually smoothed by a kernel density estimator. We can also combine both stories into only one. In this exercise, your task is to build a violin plot adding a box plot internally using the cbp data. Also try to modify the arguments (like alpha, coef etc) to see how they change the final plot.\nggplot(data = cbp, aes(x = year, y = phenotype, group = year)) + geom_violin(aes(fill = ), size = 1, alpha = 0.5) + geom_boxplot(outlier.alpha = 0, coef =0, colour = \u0026quot;black\u0026quot;, width = 0.2)  Coordinates # Cartesian coordinate p \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(category))) + geom_bar(aes(y = stat(count)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + labs(title = \u0026quot;Cartesian Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;) p # Polar coordinate p + coord_polar(theta=\u0026quot;y\u0026quot;) + labs(title = \u0026quot;Polar Coordinate\u0026quot;)  Themes p \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) + stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;, alpha = 0.3, fun.max = max, fun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) + stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) + stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;, size = 2) + geom_point(shape=1) p + theme_bw() p + theme_classic() p + theme_light() p + theme_grey() p + theme_minimal() p + theme_void()  p + labs(title = \u0026quot;Modifing themes\u0026quot;) + theme(axis.title = element_text(size = 13, colour = \u0026quot;blue\u0026quot;, family=\u0026quot;serif\u0026quot;, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;)) p + labs(title = \u0026quot;Modifing themes\u0026quot;) + theme(text = element_text(size = 13, colour = \u0026quot;blue\u0026quot;, family=\u0026quot;serif\u0026quot;, face = \u0026quot;bold\u0026quot;)) p + theme(panel.grid = element_blank(), panel.grid.minor = element_blank(), panel.border = element_rect(fill=NA, color=\u0026quot;black\u0026quot;, size=0.5, linetype=\u0026quot;dashed\u0026quot;), axis.line = element_line(colour = \u0026quot;darkblue\u0026quot;, size = 1, linetype = \u0026quot;solid\u0026quot;), axis.ticks = element_line(color=\u0026quot;black\u0026quot;, size=1.2), panel.background = element_rect(fill = \u0026quot;white\u0026quot;), axis.title = element_text(size = 13, colour = \u0026quot;darkblue\u0026quot;, family=\u0026quot;serif\u0026quot;, face = \u0026quot;bold\u0026quot;), axis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;)) p \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex)) + geom_point() p + theme_classic() + theme(legend.position=\u0026quot;top\u0026quot;, legend.background = element_rect(fill=NULL, size=0.5, linetype=\u0026quot;solid\u0026quot;, colour =\u0026quot;darkblue\u0026quot;))  Practice Let we start with this initial ggplot object:\ncbp$year2 \u0026lt;- as.numeric(cbp$year) p \u0026lt;- ggplot(data = cbp, aes(x = year2, y = phenotype)) + geom_smooth(aes(linetype = herd, color = herd), method = \u0026quot;loess\u0026quot;, se = FALSE) + stat_summary(aes(group = herd,colour = herd),fun = mean, geom =\u0026quot;point\u0026quot;, size = 1.3, shape = 1) + facet_wrap(~sex) p  Now we have some tasks to make this plot looks more professional.\n Add a new standard theme:  p2 \u0026lt;- p + theme_*() # suggestion to classic  Change the legend from right to top, increase text (title and axis) size to 13, increase the axis ticks to 1.3, increase legend key width to 1.3 cm, and include a legend key with fill = \u0026quot;white\u0026quot; and colour = \u0026quot;gray40\u0026quot;:  p3 \u0026lt;- p2 + theme(legend.position = \u0026quot;top\u0026quot;, axis.* = element_*(size = ), axis.* = element_*(size= ), axis.* = element_*(size = ), legend.* = element_*(fill = \u0026quot;white\u0026quot;, colour = \u0026quot;gray40\u0026quot;), legend.key.* = unit( , \u0026quot;cm\u0026quot;))  Change the y label to Phenotype, x label to Year, and legend title from herd to Herd.  p3 + labs(...)  Quick plot # Using qplot qplot( year, phenotype, data = cbp, facets = sex ~ herd) # using ggplot ggplot(data = cbp, aes(x = year, y = phenotype)) + geom_point() + facet_grid(rows = vars(sex), cols = vars(herd))  Annotate # Example 1 ggplot(storms, aes(x = wind, y = pressure, colour = status)) + geom_point(shape=1) + annotate(\u0026quot;rect\u0026quot;, xmin = 63, xmax = 161, ymin = 880, ymax = 1010, alpha = .2) + annotate(\u0026quot;text\u0026quot;, x = 110, y = 888, label = \u0026quot;Gilbert (1988)\u0026quot;) + annotate(\u0026quot;text\u0026quot;, x = 110, y = 882, label = \u0026quot;Wilma (2005)\u0026quot;) + geom_segment(aes(x = 135, y = 888, xend = 159, yend = 888), arrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)), show.legend = FALSE)+ geom_segment(aes(x = 135, y = 882, xend = 159, yend = 882), arrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)), show.legend = FALSE) ### Example 2 colnames(Orange)[2:3] \u0026lt;- c(\u0026quot;x\u0026quot;,\u0026quot;y\u0026quot;) p1 \u0026lt;- ggplot(data = Orange, aes(x = x, y = y)) + geom_point(shape=1) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) + ggpmisc::stat_poly_eq( aes(label = paste(after_stat(eq.label), after_stat(adj.rr.label), sep = \u0026quot;*\\\u0026quot;, \\\u0026quot;*\u0026quot;)), formula = y ~ poly(x, 1, raw = TRUE)) p1 # Using the package ggpmisc with facets p2 \u0026lt;- p1 + facet_wrap(~Tree) tag_facet(p2, x = 1300, y = -Inf, vjust = -1, open = \u0026quot;\u0026quot;, close = \u0026quot;)\u0026quot;, tag_pool = LETTERS) # Example 3 ex1 \u0026lt;- data.frame(y = 1:3, family = c(\u0026quot;sans\u0026quot;, \u0026quot;serif\u0026quot;, \u0026quot;Roboto\u0026quot;)) ggplot(data=ex1) + geom_text(aes(x=1, y=y, label = family, colour = family, family = family), show.legend = FALSE, size = 6)+ geom_label(aes(x=0, y=y, label = family, family = family, fontface = c(1:3)))+ xlim(c(-0.5,1.5)) + theme_classic(base_size = 12)  Practice Use the function lmNote to add the regression equation and $R^2$ to the plot using a label geometry.\n# Function to extract coefficients lm(y~x) lmNote \u0026lt;- function(y, x, digits=2) { p \u0026lt;- lm(y~x) z \u0026lt;- list(beta0 = format(as.numeric(coef(p)[1]), digits = digits), beta1 = format(abs(as.numeric(coef(p)[2])), digits = digits), r2 = format(summary(p)$r.squared, digits = digits)); if (coef(p)[2] \u0026gt;= 0) { tmp \u0026lt;- substitute(hat(y) == beta0 + beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z) } else { tmp \u0026lt;- substitute(hat(y) == beta0 - beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z) } as.character(as.expression(tmp)) } data(\u0026quot;Orange\u0026quot;) ggplot(data = Orange, aes(x = age, y = circumference)) + geom_point(shape=1) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) + geom_*(x=400, y=200, label = lmNote(y = Orange$, x = Orange$), parse = TRUE)  Plot Composition p1 \u0026lt;- ggplot(data = storms, aes(x = year, fill = factor(month))) + geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+ ylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + labs(fill=\u0026quot;Month\u0026quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) p2 \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(status):factor(category))) + geom_bar(aes(y = stat(count)/sum(..count..))) + scale_y_continuous(labels=scales::percent) + labs(title = \u0026quot;Polar Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;) + coord_polar(theta=\u0026quot;y\u0026quot;) p3 \u0026lt;- ggplot(data = storms, aes(x = year)) + geom_histogram(aes(y=stat(count)/sum(..count..)), fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+ scale_y_continuous(labels=scales::percent) + ylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) + xlab(\u0026quot;Year\u0026quot;) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) (p3 | p2) + plot_annotation(tag_levels = \u0026quot;A\u0026quot;) p1|(p3/p2)  data(\u0026quot;Orange\u0026quot;) p1 \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) + stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;, alpha = 0.3, fun.max = max, fun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) + stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) + stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;, size = 2) + geom_point(shape=1) OrangeSummary \u0026lt;- Orange %\u0026gt;% group_by(age) %\u0026gt;% summarise( Age = unique(age), Mean = mean(circumference), Median = median(circumference), Sd = sd(circumference) ) %\u0026gt;% round(1) p1 | gridExtra::tableGrob(OrangeSummary) text \u0026lt;- paste(\u0026quot;The Orange data frame has 35 rows and 3 columns\u0026quot;, \u0026quot;of records of the growth of orange trees.\u0026quot;, sep = \u0026quot;\\n\u0026quot;) p4 \u0026lt;- wrap_elements(ggpubr::text_grob(text, face=\u0026quot;bold\u0026quot;,color = \u0026quot;blue\u0026quot;)) p4/( p1 | gridExtra::tableGrob(OrangeSummary))  ","date":1631923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631923200,"objectID":"b2ced46b92d7cbbcd1979336a0d519d9","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/ggplot2/","publishdate":"2021-09-18T00:00:00Z","relpermalink":"/courses/2021_ddbg/ggplot2/","section":"courses","summary":"Visualization and Data Structure What is the grammar of graphics? It is a tool that enables us to concisely describe the components of a graphic and provides a strong foundation for understanding a diverse range of graphics.","tags":null,"title":"The grammar of graphics","type":"book"},{"authors":null,"categories":null,"content":"  pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .sourceCode { overflow: visible; } code.sourceCode  span { color: inherit; text-decoration: inherit; } div.sourceCode { margin: 1em 0; } pre.sourceCode { margin: 0; } @media screen { div.sourceCode { overflow: auto; } } @media print { pre  code.sourceCode { white-space: pre-wrap; } pre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; } } pre.numberSource code { counter-reset: source-line 0; } pre.numberSource code  span { position: relative; left: -4em; counter-increment: source-line; } pre.numberSource code  span  a:first-child::before { content: counter(source-line); position: relative; left: -1em; text-align: right; vertical-align: baseline; border: none; display: inline-block; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; padding: 0 4px; width: 4em; background-color: #ffffff; color: #a0a0a0; } pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0; padding-left: 4px; } div.sourceCode { color: #1f1c1b; background-color: #ffffff; } @media screen { pre  code.sourceCode  span  a:first-child::before { text-decoration: underline; } } code span { color: #1f1c1b; } /* Normal */ code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */ code span.an { color: #ca60ca; } /* Annotation */ code span.at { color: #0057ae; } /* Attribute */ code span.bn { color: #b08000; } /* BaseN */ code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */ code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */ code span.ch { color: #924c9d; } /* Char */ code span.cn { color: #aa5500; } /* Constant */ code span.co { color: #898887; } /* Comment */ code span.cv { color: #0095ff; } /* CommentVar */ code span.do { color: #607880; } /* Documentation */ code span.dt { color: #0057ae; } /* DataType */ code span.dv { color: #b08000; } /* DecVal */ code span.er { color: #bf0303; text-decoration: underline; } /* Error */ code span.ex { color: #0095ff; font-weight: bold; } /* Extension */ code span.fl { color: #b08000; } /* Float */ code span.fu { color: #644a9b; } /* Function */ code span.im { color: #ff5500; } /* Import */ code span.in { color: #b08000; } /* Information */ code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */ code span.op { color: #1f1c1b; } /* Operator */ code span.ot { color: #006e28; } /* Other */ code span.pp { color: #006e28; } /* Preprocessor */ code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */ code span.sc { color: #3daee9; } /* SpecialChar */ code span.ss { color: #ff5500; } /* SpecialString */ code span.st { color: #bf0303; } /* String */ code span.va { color: #0057ae; } /* Variable */ code span.vs { color: #bf0303; } /* VerbatimString */ code span.wa { color: #bf0303; } /* Warning */   Mapping  Practice  Facets  Practice  Scale  Practice  Statistics  Practice  Geometries  Examples Practice  Coordinates Themes  Practice  Quick plot Annotate  Practice  Plot Composition   If you‚Äôre at all familiar with ggplot, you may know the basic structure of a call to the ggplot() function. For an introduction to the ggplot package, you can check out the visualization and data structure lecture. When you call ggplot, you should provide a data source, usually a data frame or tibble. Afterward, you can build a ggplot by mapping different variables in your data source to different aesthetics. For example, there are colour, shape, size of points, the position of the x or y-axis, ans so on. To demonstrate this and even more procedures using ggplot, we will work with different data sets such as:\n Growth of Orange Trees: the Orange data frame has 35 rows and 3 columns of records of the growth of orange trees.  glimpse(Orange) ## Rows: 35 ## Columns: 3 ## $ Tree \u0026lt;ord\u0026gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3,‚Ä¶ ## $ age \u0026lt;dbl\u0026gt; 118, 484, 664, 1004, 1231, 1372, 1582, 118, 484, 664, 10‚Ä¶ ## $ circumference \u0026lt;dbl\u0026gt; 30, 58, 87, 115, 120, 142, 145, 33, 69, 111, 156, 172, 2‚Ä¶  Storm tracks data: This data is a subset of the NOAA Atlantic hurricane database best track data, https://www.nhc.noaa.gov/data/#hurdat. The data includes the positions and attributes of 198 tropical storms, measured every six hours during the lifetime of a storm.  glimpse(storms) ## Rows: 10,010 ## Columns: 13 ## $ name \u0026lt;chr\u0026gt; \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;Amy\u0026quot;, \u0026quot;A‚Ä¶ ## $ year \u0026lt;dbl\u0026gt; 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975, 1975‚Ä¶ ## $ month \u0026lt;dbl\u0026gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7‚Ä¶ ## $ day \u0026lt;int\u0026gt; 27, 27, 27, 27, 28, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30‚Ä¶ ## $ hour \u0026lt;dbl\u0026gt; 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18, 0, 6, 12, 18, 0,‚Ä¶ ## $ lat \u0026lt;dbl\u0026gt; 27.5, 28.5, 29.5, 30.5, 31.5, 32.4, 33.3, 34.0, 34.4, 34.0‚Ä¶ ## $ long \u0026lt;dbl\u0026gt; -79.0, -79.0, -79.0, -79.0, -78.8, -78.7, -78.0, -77.0, -7‚Ä¶ ## $ status \u0026lt;chr\u0026gt; \u0026quot;tropical depression\u0026quot;, \u0026quot;tropical depression\u0026quot;, \u0026quot;tropical de‚Ä¶ ## $ category \u0026lt;ord\u0026gt; -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶ ## $ wind \u0026lt;int\u0026gt; 25, 25, 25, 25, 25, 25, 25, 30, 35, 40, 45, 50, 50, 55, 60‚Ä¶ ## $ pressure \u0026lt;int\u0026gt; 1013, 1013, 1013, 1013, 1012, 1012, 1011, 1006, 1004, 1002‚Ä¶ ## $ ts_diameter \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶ ## $ hu_diameter \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶  Carbon Dioxide Uptake in Grass Plants: The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.  glimpse(CO2) ## Rows: 84 ## Columns: 5 ## $ Plant \u0026lt;ord\u0026gt; Qn1, Qn1, Qn1, Qn1, Qn1, Qn1, Qn1, Qn2, Qn2, Qn2, Qn2, Qn2, ‚Ä¶ ## $ Type \u0026lt;fct\u0026gt; Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Quebec, Queb‚Ä¶ ## $ Treatment \u0026lt;fct\u0026gt; nonchilled, nonchilled, nonchilled, nonchilled, nonchilled, ‚Ä¶ ## $ conc \u0026lt;dbl\u0026gt; 95, 175, 250, 350, 500, 675, 1000, 95, 175, 250, 350, 500, 6‚Ä¶ ## $ uptake \u0026lt;dbl\u0026gt; 16.0, 30.4, 34.8, 37.2, 35.3, 39.2, 39.7, 13.6, 27.3, 37.1, ‚Ä¶  Simulated cattle breeding program (cbp): the cbp data frame has 10,000 rows and 7 columns of data from a simulation using the AlphaSimR package. The data is comprised of founders and phenotyped individuals (on the Milk Yield), where for each one, we have information of parents, sex, and herd.  cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;) glimpse(cbp) ## Rows: 10,000 ## Columns: 7 ## $ ind \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1‚Ä¶ ## $ father \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶ ## $ mother \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ‚Ä¶ ## $ year \u0026lt;fct\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶ ## $ sex \u0026lt;fct\u0026gt; M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, ‚Ä¶ ## $ phenotype \u0026lt;dbl\u0026gt; 37.66915, 35.79590, 28.42813, 33.62726, 32.86620, 31.63590, ‚Ä¶ ## $ herd \u0026lt;fct\u0026gt; E, B, A, D, A, A, A, E, E, C, A, B, B, A, C, C, E, C, A, A, ‚Ä¶ Mapping p \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex)) p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) Practice Use the data CO2 (Biochemical Oxygen Demand) to build a layer with geom = \"point\" using conc as the response variable, uptake as an explanatory variable, and colour the points by Treatment.\ndata(CO2) pEx1 \u0026lt;- ggplot(data = , aes(x = , y = , color = )) pEx1 + layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE))   Facets # facet_wrap p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  facet_wrap(facet = \u0026quot;sex\u0026quot;) # facet_grid p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  facet_grid(rows = vars(sex), cols = vars(herd)) Practice Using the previous exercise as a starting point, add a facet_wrap() using Plant as a subgroup.  pEx1 + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  facet_wrap()  Make a plot using the storms database considering ts_diameter as response, hu_diameter as explanatory variable, and add a facet_grid() using rows as status and columns as category.  ggplot(data = , aes(y= , x=)) +  layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  facet_grid(rows = , cols = )   Scale p2 \u0026lt;- p + layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  facet_grid(rows = vars(sex), cols = vars(herd)) +  scale_colour_manual(values = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#FC4E07\u0026quot;)) p2 p2 + scale_color_brewer(palette = \u0026quot;Dark2\u0026quot;) Practice Make a plot using the storms database using ts_diameter as a response, pressure as an explanatory variable, colored by status. Also add a facet_wrap() by category and do a scale transformation on the x and y axis using a log10() function.\nggplot(data = storms, aes(y= , x= )) +  layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  facet_wrap(...) +  scale_y_*() +  scale_x_*() +  ylab(expression(log[10](\u0026quot;ts_diameter\u0026quot;))) +  xlab(expression(log[10](\u0026quot;pressure\u0026quot;)))   Statistics # facet_wrap data(Orange) ggplot(data = Orange, aes(x = age, y = circumference)) +  layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  layer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;,  params = list(method = \u0026quot;lm\u0026quot;, se = FALSE)) ggplot(data = cbp, aes(x = year, y = phenotype, group = year)) +   layer(geom = \u0026quot;boxplot\u0026quot;, stat = \u0026quot;boxplot\u0026quot;, position = \u0026quot;identity\u0026quot;,  params = list(outlier.colour=\u0026quot;red\u0026quot;, outlier.shape = 1,  na.rm = FALSE)) +  facet_grid(rows = vars(sex), cols = vars(herd)) Practice The main difference between the regression line and LOESS is that while the regression line is a straight line representing the relationship between the x and y, a LOESS line is mainly used to identify trends in the data. Faceting allows you to split the data into subgroups, build a plot adding points, a regression line, and a LOESS curve for each Tree level in the Orange data set. For this, you can use the formula circumference ~ age.\nggplot(data = Orange, aes(x = , y = )) +  layer(geom = , stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE)) +  layer(geom = , stat = , position = \u0026quot;identity\u0026quot;,  params = list(method = , se = FALSE)) +  layer(geom = , stat = , position = \u0026quot;identity\u0026quot;,  params = list(method = , se = FALSE, colour = \u0026quot;red\u0026quot;)) +  facet_wrap(...)   Geometries ggplot(data = Orange, aes(x = age, y = circumference)) +  layer(geom = \u0026quot;point\u0026quot;, stat = \u0026quot;identity\u0026quot;, position = \u0026quot;identity\u0026quot;,   params = list(shape=1, na.rm = FALSE, width = 0.1, height = 0.1)) +  layer(geom = \u0026quot;line\u0026quot;, stat = \u0026quot;smooth\u0026quot;, position = \u0026quot;identity\u0026quot;,  params = list(method = \u0026quot;lm\u0026quot;, se = FALSE)) +  facet_wrap(facet = \u0026quot;Tree\u0026quot;) Examples # Density plot p \u0026lt;- ggplot(data = cbp) p + geom_density(aes(x=phenotype)) # Density plot per year p + geom_density_ridges(aes(x = phenotype, y = year, fill = year)) # Histogram - absolute frequency of storms per month and year ggplot(data = storms, aes(x = year)) +  facet_wrap(~month)+  geom_histogram(fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+  ylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) +  xlab(\u0026quot;Year\u0026quot;) +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Histogram - relative frequency of storms per month ggplot(data = storms, aes(x = year)) +  geom_histogram(aes(y=stat(count)/sum(..count..)),  fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+  scale_y_continuous(labels=scales::percent) +  ylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) +  xlab(\u0026quot;Year\u0026quot;) +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Histogram - absolute frequency of storms per year (stack by month) ggplot(data = storms, aes(x = year, fill = factor(month))) +  geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;stack\u0026quot;)+  ylab(\u0026quot;Absolute Frequency of Tropical Storms\u0026quot;) +  xlab(\u0026quot;Year\u0026quot;) +  labs(fill=\u0026quot;Month\u0026quot;) +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Histogram - proportion of storms per month by year ggplot(data = storms, aes(x = year, fill = factor(month))) +  geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+  ylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) +  xlab(\u0026quot;Year\u0026quot;) +  labs(fill=\u0026quot;Month\u0026quot;) +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Exercise: build a histogram with relative frequency per month within year  # Ribbon and stat_summary - Plot of a time-series ggplot(data = Orange, aes(x = age, y = circumference)) +  stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,  alpha = 0.3, fun.max = function(x) mean(x) + 2*sd(x),  fun.min = function(x) mean(x) - 2*sd(x), fill = \u0026quot;#00AFBB\u0026quot;) +  stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +  stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,  size = 2) +  geom_point(shape=1)  Practice A violin plot is a method of plotting numeric data, and it is similar to a box plot but with a rotated kernel density plot on each side. Violin plots can show the probability density of the data at different values, usually smoothed by a kernel density estimator. We can also combine both stories into only one. In this exercise, your task is to build a violin plot adding a box plot internally using the cbp data. Also try to modify the arguments (like alpha, coef etc) to see how they change the final plot.\nggplot(data = cbp, aes(x = year, y = phenotype, group = year)) +  geom_violin(aes(fill = ), size = 1, alpha = 0.5) +  geom_boxplot(outlier.alpha = 0, coef =0,   colour = \u0026quot;black\u0026quot;, width = 0.2)   Coordinates # Cartesian coordinate p \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(category))) +  geom_bar(aes(y = stat(count)/sum(..count..))) +  scale_y_continuous(labels=scales::percent) +  labs(title = \u0026quot;Cartesian Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;) p # Polar coordinate p + coord_polar(theta=\u0026quot;y\u0026quot;) +  labs(title = \u0026quot;Polar Coordinate\u0026quot;)  Themes p \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) +  stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,  alpha = 0.3, fun.max = max,  fun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) +  stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +  stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,  size = 2) +  geom_point(shape=1)  p + theme_bw() p + theme_classic() p + theme_light() p + theme_grey() p + theme_minimal() p + theme_void() p + labs(title = \u0026quot;Modifing themes\u0026quot;) +  theme(axis.title = element_text(size = 13, colour = \u0026quot;blue\u0026quot;,   family=\u0026quot;serif\u0026quot;,  face = \u0026quot;bold\u0026quot;),  axis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;)) p + labs(title = \u0026quot;Modifing themes\u0026quot;) +  theme(text = element_text(size = 13, colour = \u0026quot;blue\u0026quot;,   family=\u0026quot;serif\u0026quot;,  face = \u0026quot;bold\u0026quot;)) p +  theme(panel.grid = element_blank(),  panel.grid.minor = element_blank(),  panel.border = element_rect(fill=NA, color=\u0026quot;black\u0026quot;,   size=0.5, linetype=\u0026quot;dashed\u0026quot;),  axis.line = element_line(colour = \u0026quot;darkblue\u0026quot;,   size = 1, linetype = \u0026quot;solid\u0026quot;),  axis.ticks = element_line(color=\u0026quot;black\u0026quot;, size=1.2),  panel.background = element_rect(fill = \u0026quot;white\u0026quot;),  axis.title = element_text(size = 13, colour = \u0026quot;darkblue\u0026quot;,   family=\u0026quot;serif\u0026quot;,  face = \u0026quot;bold\u0026quot;),  axis.text = element_text(size=12, colour = \u0026quot;red\u0026quot;)) p \u0026lt;- ggplot(data = cbp, aes(x = year, y = phenotype, color = sex)) +  geom_point()  p + theme_classic() +  theme(legend.position=\u0026quot;top\u0026quot;,  legend.background = element_rect(fill=NULL,  size=0.5, linetype=\u0026quot;solid\u0026quot;,   colour =\u0026quot;darkblue\u0026quot;)) Practice Let we start with this initial ggplot object:\ncbp$year2 \u0026lt;- as.numeric(cbp$year) p \u0026lt;- ggplot(data = cbp, aes(x = year2, y = phenotype)) +  geom_smooth(aes(linetype = herd, color = herd),   method = \u0026quot;loess\u0026quot;, se = FALSE) +  stat_summary(aes(group = herd,colour = herd),fun = mean, geom =\u0026quot;point\u0026quot;,  size = 1.3, shape = 1) +  facet_wrap(~sex) p Now we have some tasks to make this plot looks more professional.\nAdd a new standard theme:  p2 \u0026lt;- p + theme_*() # suggestion to classic Change the legend from right to top, increase text (title and axis) size to 13, increase the axis ticks to 1.3, increase legend key width to 1.3 cm, and include a legend key with fill = \"white\" and colour = \"gray40\":  p3 \u0026lt;- p2 +  theme(legend.position = \u0026quot;top\u0026quot;,  axis.* = element_*(size = ),  axis.* = element_*(size= ),  axis.* = element_*(size = ),  legend.* = element_*(fill = \u0026quot;white\u0026quot;, colour = \u0026quot;gray40\u0026quot;),  legend.key.* = unit( , \u0026quot;cm\u0026quot;)) Change the y label to Phenotype, x label to Year, and legend title from herd to Herd.  p3 +   labs(...)   Quick plot # Using qplot qplot( year, phenotype, data = cbp, facets = sex ~ herd) # using ggplot ggplot(data = cbp, aes(x = year, y = phenotype)) +  geom_point() +  facet_grid(rows = vars(sex), cols = vars(herd))  Annotate # Example 1 ggplot(storms, aes(x = wind, y = pressure, colour = status)) +  geom_point(shape=1) +   annotate(\u0026quot;rect\u0026quot;, xmin = 63, xmax = 161, ymin = 880, ymax = 1010,  alpha = .2) +  annotate(\u0026quot;text\u0026quot;, x = 110, y = 888, label = \u0026quot;Gilbert (1988)\u0026quot;) +  annotate(\u0026quot;text\u0026quot;, x = 110, y = 882, label = \u0026quot;Wilma (2005)\u0026quot;) +  geom_segment(aes(x = 135, y = 888, xend = 159, yend = 888),  arrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)),  show.legend = FALSE)+  geom_segment(aes(x = 135, y = 882, xend = 159, yend = 882),  arrow = arrow(length = unit(0.2, \u0026quot;cm\u0026quot;)),  show.legend = FALSE) # Example 2 colnames(Orange)[2:3] \u0026lt;- c(\u0026quot;x\u0026quot;,\u0026quot;y\u0026quot;) p1 \u0026lt;- ggplot(data = Orange, aes(x = x, y = y)) +  geom_point(shape=1) +  geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) +  ggpmisc::stat_poly_eq(  aes(label = paste(after_stat(eq.label),  after_stat(adj.rr.label),  sep = \u0026quot;*\\\u0026quot;, \\\u0026quot;*\u0026quot;)),  formula = y ~ poly(x, 1, raw = TRUE)) p1 # Using the package ggpmisc with facets p2 \u0026lt;- p1 + facet_wrap(~Tree)  tag_facet(p2,   x = 1300, y = -Inf,   vjust = -1,  open = \u0026quot;\u0026quot;, close = \u0026quot;)\u0026quot;,  tag_pool = LETTERS) # Example 3 ex1 \u0026lt;- data.frame(y = 1:3, family = c(\u0026quot;sans\u0026quot;, \u0026quot;serif\u0026quot;, \u0026quot;Roboto\u0026quot;)) ggplot(data=ex1) +   geom_text(aes(x=1, y=y, label = family,   colour = family, family = family),  show.legend = FALSE, size = 6)+   geom_label(aes(x=0, y=y, label = family, family = family,  fontface = c(1:3)))+  xlim(c(-0.5,1.5)) +  theme_classic(base_size = 12) Practice Use the function lmNote to add the regression equation and \\(R^2\\) to the plot using a label geometry.\n# Function to extract coefficients lm(y~x) lmNote \u0026lt;- function(y, x, digits=2) {  p \u0026lt;- lm(y~x)  z \u0026lt;- list(beta0 = format(as.numeric(coef(p)[1]), digits = digits),  beta1 = format(abs(as.numeric(coef(p)[2])), digits = digits),  r2 = format(summary(p)$r.squared, digits = digits));  if (coef(p)[2] \u0026gt;= 0) {  tmp \u0026lt;- substitute(hat(y) == beta0 + beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z)  } else {  tmp \u0026lt;- substitute(hat(y) == beta0 - beta1 %.% x*\u0026quot;,\u0026quot;~~R^2~\u0026quot;=\u0026quot;~r2,z)   }  as.character(as.expression(tmp)) }  data(\u0026quot;Orange\u0026quot;) ggplot(data = Orange, aes(x = age, y = circumference)) +  geom_point(shape=1) +  geom_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) +  geom_*(x=400, y=200, label = lmNote(y = Orange$, x = Orange$),  parse = TRUE)   Plot Composition p1 \u0026lt;- ggplot(data = storms, aes(x = year, fill = factor(month))) +  geom_histogram(color = \u0026quot;black\u0026quot;, position = \u0026quot;fill\u0026quot;)+  ylab(\u0026quot;Proportion of Tropical Storms per Month by Year\u0026quot;) +  xlab(\u0026quot;Year\u0026quot;) +  labs(fill=\u0026quot;Month\u0026quot;) +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  p2 \u0026lt;- ggplot(data = storms, aes(x = \u0026quot;\u0026quot;, fill = factor(status):factor(category))) +  geom_bar(aes(y = stat(count)/sum(..count..))) +  scale_y_continuous(labels=scales::percent) +  labs(title = \u0026quot;Polar Coordinate\u0026quot;, fill = \u0026quot;Category\u0026quot;, y = \u0026quot;Proportion\u0026quot;) +   coord_polar(theta=\u0026quot;y\u0026quot;)  p3 \u0026lt;- ggplot(data = storms, aes(x = year)) +  geom_histogram(aes(y=stat(count)/sum(..count..)),  fill = \u0026quot;lightblue\u0026quot;, color = \u0026quot;black\u0026quot;)+  scale_y_continuous(labels=scales::percent) +  ylab(\u0026quot;Relative Frequency of Tropical Storms\u0026quot;) +  xlab(\u0026quot;Year\u0026quot;) +  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  (p3 | p2) +  plot_annotation(tag_levels = \u0026quot;A\u0026quot;) p1|(p3/p2) data(\u0026quot;Orange\u0026quot;) p1 \u0026lt;- ggplot(data = Orange, aes(x = age, y = circumference)) +  stat_summary(fun = mean, geom = \u0026quot;ribbon\u0026quot;,  alpha = 0.3, fun.max = max,  fun.min = min, fill = \u0026quot;#00AFBB\u0026quot;) +  stat_summary(fun = mean, geom =\u0026quot;line\u0026quot;, colour = \u0026quot;black\u0026quot;) +  stat_summary(fun = mean, geom =\u0026quot;point\u0026quot;, colour = \u0026quot;blue\u0026quot;,  size = 2) +  geom_point(shape=1)  OrangeSummary \u0026lt;- Orange %\u0026gt;%  group_by(age) %\u0026gt;%  summarise(  Age = unique(age),  Mean = mean(circumference),  Median = median(circumference),  Sd = sd(circumference)  ) %\u0026gt;%  round(1)  p1 | gridExtra::tableGrob(OrangeSummary) text \u0026lt;- paste(\u0026quot;The Orange data frame has 35 rows and 3 columns\u0026quot;,   \u0026quot;of records of the growth of orange trees.\u0026quot;, sep = \u0026quot;\\n\u0026quot;) p4 \u0026lt;- wrap_elements(ggpubr::text_grob(text, face=\u0026quot;bold\u0026quot;,color = \u0026quot;blue\u0026quot;))  p4/( p1 | gridExtra::tableGrob(OrangeSummary))  ","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"9e7de2e4c6300da0d6ca3fc574148f53","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/oliveiratp_ggplot2/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/courses/2021_ddbg/oliveiratp_ggplot2/","section":"courses","summary":"pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .","tags":null,"title":"The grammar of graphics: R code","type":"courses"},{"authors":null,"categories":null,"content":"Visualization and Data Structure Tidy data refers to data arranged to make data processing, analysis, and visualization simpler. Remember that in a tidy data set we should consider:\n Each variable must have its column. Each observation must have its row. Each value must have its cell.  Video  Slides  Exercises Exercise 1 Let‚Äôs say we want to organize the data anscombe. Below I show how this data looks like:\nanscombe   Organize this data set to obtain tidy data. Remember here we have two response variables been measured four times.  ex1 \u0026lt;- anscombe %\u0026gt;%  Filter the data set to get replications 2 and 4, and summarise it to get the maximum, minimum, and mean values.  ex2 %\u0026gt;% filter() %\u0026gt;% summarise( )  Exercise 2 Often you do not need the entire data set, but just part of it.\n Here, you should make the data mtcars tidy before making any selection.  (dataEx3 \u0026lt;- readRDS(\u0026quot;./data/dataEx3.rds\u0026quot;))  As you can see, some columns are not variable names but values. Create two new variables calling mpg (for observations) and gear (with column values).\ndataEx3 \u0026lt;- dataEx3 %\u0026gt;% pivot_longer( )  Select the columns mpg, hp, gear, and carb, and then make a plot using ggplot2 where  mpg is the response variable, and hp is the co-variate in the x-axis. Also include different shapes and colours for gear, and facets for carb.  dataEx3 %\u0026gt;% select() %\u0026gt;% ggplot() %\u0026gt;% geom_point() %\u0026gt;% facet_wrap() %\u0026gt;% theme_bw()  Exercise 3 The following data represents song rankings for Billboard top 100 in the year 2000. The rank of the song is displayed in each week after it entered.\nbillboard  A slightly more complex case where columns have a common prefix and missing missings are structural, so should be dropped. So, make this data tidy.\nbillboard %\u0026gt;%  Data Structure Exercise 1  Make this data tidy by including tmin and tmax as variable. Remember that here type is carrying to variables names rather than factors.  (dataEx2 \u0026lt;- as.tibble(readRDS(\u0026quot;./data/dataEx2.RDS\u0026quot;)))  dataEx2 \u0026lt;- dataEx2 %\u0026gt;% pivot_wider()  Now, build a new variable called tdiff, which is the difference between tmax and tmin. Moreover, display a ggplot2 graph that shows tdiff over time.\ndataEx2 %\u0026gt;%  Exercise 2 Our cattle data data is already in a tidy format.\n(cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;))  For this exercise, complete the following tasks with that data set:\n Calculate the average phenotype per year by sex and herd using the summarise() function in the dplyr package. Add two columns to cattle data using the mutate() function:  Column 1: Phenotype should be rescaled to have a mean of zero and a standard deviation of one. You can call this new variable as PhenoStd. Column 2: Rank the PhenoStd using the function min_rank(). The output data frame should have only PhenoStd \u0026gt; 0.    cbp %\u0026gt;% summarise( ) %\u0026gt;%  Exercise 3 Excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country. This data has 142 countries observed from the year 1952 to 2007 in increments of 5 years. The response variable observed was the life expectancy at birth (in years), population size, and Per capita gross domestic product (GDP).\nPer capita gross domestic product (GDP) measures a country\u0026rsquo;s economic response per person and is calculated by dividing its GDP by its population. It is a global measure for gauging the prosperity of nations as we can analyze the worth of a country based on its economic growth. Thus, countries that have the highest per capita GDP tend to be more developed.\ngapminder  Questions:\n What are the ten highest gpdPercap values?  gapminder %\u0026gt;%  Find both the median life expectancy (lifeExp) and the median and maximum GDP per capita (gdpPercap) in 1957, 1982, and 2007, by country and continent. Call them medianLifeExp, medianGdpPercap, and maxGdpPercap, respectively.  dat \u0026lt;- gapminder %\u0026gt;%  Use a scatter plot to compare the median GDP and median life expectancy. Use the variables continent and year to produce this plot.  dat %\u0026gt;%  ","date":1631923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631923200,"objectID":"d98bf4fb1b6752d8c7b4e1d23bcf457a","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/tidyverse/","publishdate":"2021-09-18T00:00:00Z","relpermalink":"/courses/2021_ddbg/tidyverse/","section":"courses","summary":"Visualization and Data Structure Tidy data refers to data arranged to make data processing, analysis, and visualization simpler. Remember that in a tidy data set we should consider:\n Each variable must have its column.","tags":null,"title":"Tidyverse","type":"book"},{"authors":null,"categories":null,"content":"  pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .sourceCode { overflow: visible; } code.sourceCode  span { color: inherit; text-decoration: inherit; } div.sourceCode { margin: 1em 0; } pre.sourceCode { margin: 0; } @media screen { div.sourceCode { overflow: auto; } } @media print { pre  code.sourceCode { white-space: pre-wrap; } pre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; } } pre.numberSource code { counter-reset: source-line 0; } pre.numberSource code  span { position: relative; left: -4em; counter-increment: source-line; } pre.numberSource code  span  a:first-child::before { content: counter(source-line); position: relative; left: -1em; text-align: right; vertical-align: baseline; border: none; display: inline-block; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; padding: 0 4px; width: 4em; background-color: #ffffff; color: #a0a0a0; } pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0; padding-left: 4px; } div.sourceCode { color: #1f1c1b; background-color: #ffffff; } @media screen { pre  code.sourceCode  span  a:first-child::before { text-decoration: underline; } } code span { color: #1f1c1b; } /* Normal */ code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */ code span.an { color: #ca60ca; } /* Annotation */ code span.at { color: #0057ae; } /* Attribute */ code span.bn { color: #b08000; } /* BaseN */ code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */ code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */ code span.ch { color: #924c9d; } /* Char */ code span.cn { color: #aa5500; } /* Constant */ code span.co { color: #898887; } /* Comment */ code span.cv { color: #0095ff; } /* CommentVar */ code span.do { color: #607880; } /* Documentation */ code span.dt { color: #0057ae; } /* DataType */ code span.dv { color: #b08000; } /* DecVal */ code span.er { color: #bf0303; text-decoration: underline; } /* Error */ code span.ex { color: #0095ff; font-weight: bold; } /* Extension */ code span.fl { color: #b08000; } /* Float */ code span.fu { color: #644a9b; } /* Function */ code span.im { color: #ff5500; } /* Import */ code span.in { color: #b08000; } /* Information */ code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */ code span.op { color: #1f1c1b; } /* Operator */ code span.ot { color: #006e28; } /* Other */ code span.pp { color: #006e28; } /* Preprocessor */ code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */ code span.sc { color: #3daee9; } /* SpecialChar */ code span.ss { color: #ff5500; } /* SpecialString */ code span.st { color: #bf0303; } /* String */ code span.va { color: #0057ae; } /* Variable */ code span.vs { color: #bf0303; } /* VerbatimString */ code span.wa { color: #bf0303; } /* Warning */   Data Structure  Pivoting: ‚Äòlong form‚Äô Pivoting: ‚Äòwide form‚Äô Filter Distinct Slice Arrange Add row Pull Select Relocate Mutate Transmute Across Rename Group by  Statistics Summary  Summarise    All code I used to build the slides can be found here. Making data and code available as supplementary material promotes transparency and reproducibility, enabling anyone to reproduce the methodology discussed during the lecture.\nData Structure Pivoting: ‚Äòlong form‚Äô dataEx1 \u0026lt;- readRDS(\u0026quot;./data/dataEx1.RDS\u0026quot;)   dataEx1.1 \u0026lt;- dataEx1 %\u0026gt;%  rownames_to_column(var = \u0026quot;Farm\u0026quot;) %\u0026gt;%  pivot_longer(cols = 2:4, names_to = \u0026quot;Income\u0026quot;, values_to = \u0026quot;Freq\u0026quot;)  Pivoting: ‚Äòwide form‚Äô dataEx2 \u0026lt;- as_tibble(readRDS(\u0026quot;./data/dataEx2.RDS\u0026quot;))  dataEx2.2 \u0026lt;- dataEx2 %\u0026gt;%  pivot_wider(values_from = value, names_from = type)  I \u0026lt;- dataEx2 %\u0026gt;%  pivot_wider(values_from = value, names_from = type) %\u0026gt;%  transform(id = str_replace(id,\u0026quot;Ind\u0026quot;,\u0026quot;\u0026quot;))  Filter cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;)  threshold \u0026lt;- with(cbp, mean(phenotype) + 2*sd(phenotype)) cbp %\u0026gt;%  filter(herd %in% c(\u0026quot;A\u0026quot;, \u0026quot;E\u0026quot;)) %\u0026gt;%  filter(year == \u0026quot;9\u0026quot; \u0026amp; phenotype \u0026gt; threshold) %\u0026gt;%  ggplot(aes(y = phenotype, x = sex)) +  geom_violin() +  geom_jitter(shape = 1, width = 0.15) cbp %\u0026gt;%  filter(herd %in% c(\u0026quot;A\u0026quot;, \u0026quot;E\u0026quot;)) %\u0026gt;%  filter(year == \u0026quot;9\u0026quot; \u0026amp; (phenotype \u0026gt; threshold | sex == \u0026quot;F\u0026quot;)) %\u0026gt;%  ggplot(aes(y = phenotype, x = sex)) +  geom_violin() +  geom_jitter(shape = 1, width = 0.15) ggsave(\u0026quot;filterData2.pdf\u0026quot;, width = 4, height = 4)  Distinct cbp %\u0026gt;%  distinct(dplyr::across(contains(\u0026quot;r\u0026quot;))) ## # A tibble: 8,186 √ó 4 ## father mother year herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 NA NA 0 E ## 2 NA NA 0 B ## 3 NA NA 0 A ## 4 NA NA 0 D ## 5 NA NA 0 C ## 6 418 692 1 E ## 7 461 614 1 B ## 8 195 524 1 A ## 9 198 768 1 D ## 10 122 537 1 A ## # ‚Ä¶ with 8,176 more rows cbp %\u0026gt;%  distinct(sex) ## # A tibble: 2 √ó 1 ## sex ## \u0026lt;fct\u0026gt; ## 1 M ## 2 F  Slice cbp %\u0026gt;% slice(1:3) ## # A tibble: 3 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A cbp %\u0026gt;% slice((n()-5L):n()) ## # A tibble: 6 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 9995 8463 8849 9 F 71.3 E ## 2 9996 8013 8628 9 F 61.9 E ## 3 9997 8342 8677 9 F 58.4 A ## 4 9998 8088 8510 9 F 58.2 E ## 5 9999 8449 8685 9 F 56.7 D ## 6 10000 8296 8854 9 F 67.1 B cbp %\u0026gt;% slice_sample(n = 5) ## # A tibble: 5 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 9516 8410 8794 9 F 58.3 C ## 2 6592 5280 5501 6 F 47.2 A ## 3 3973 2372 2794 3 F 35.6 E ## 4 5629 4404 4834 5 F 45.7 B ## 5 6441 5318 5840 6 M 56.7 B cbp %\u0026gt;% slice_min(phenotype, n = 3) ## # A tibble: 3 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1331 148 840 1 M 15.3 B ## 2 1316 62 551 1 M 20.2 D ## 3 1992 354 523 1 F 20.3 C  Arrange cbp %\u0026gt;% arrange(desc(phenotype)) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 9069 8366 8725 9 M 82.1 C ## 2 9979 8305 8994 9 F 80.0 B ## 3 9492 8346 8985 9 M 79.1 B ## 4 9934 8470 8776 9 F 78.9 A ## 5 8650 7117 7726 8 F 78.6 B cbp %\u0026gt;% arrange(desc(herd)) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 0 M 37.7 E ## 2 8 NA NA 0 M 33.3 E ## 3 9 NA NA 0 M 39.0 E ## 4 17 NA NA 0 M 42.5 E ## 5 23 NA NA 0 M 51.9 E cbp %\u0026gt;% arrange(sex, phenotype, herd) %\u0026gt;% slice_head(n=7) ## # A tibble: 7 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1992 354 523 1 F 20.3 C ## 2 618 NA NA 0 F 20.8 C ## 3 1830 378 900 1 F 20.8 A ## 4 557 NA NA 0 F 21.5 A ## 5 1663 102 983 1 F 21.9 B ## 6 800 NA NA 0 F 22.2 A ## 7 1645 47 541 1 F 22.9 D cbp %\u0026gt;% arrange(herd, phenotype, sex) %\u0026gt;% slice_head(n=7) ## # A tibble: 7 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1830 378 900 1 F 20.8 A ## 2 557 NA NA 0 F 21.5 A ## 3 95 NA NA 0 M 22.2 A ## 4 800 NA NA 0 F 22.2 A ## 5 1592 440 667 1 F 23.0 A ## 6 1192 355 642 1 M 24.7 A ## 7 1671 448 953 1 F 25.0 A  Add row df \u0026lt;- tibble(x = 1:4, y = 20:23) df %\u0026gt;% add_row(x = 3, y =21) ## # A tibble: 5 √ó 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 20 ## 2 2 21 ## 3 3 22 ## 4 4 23 ## 5 3 21 df %\u0026gt;% add_row(x = 3, y =21, .before = 4) ## # A tibble: 5 √ó 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 20 ## 2 2 21 ## 3 3 22 ## 4 3 21 ## 5 4 23 df %\u0026gt;% add_row(x = 3, .before = 4) ## # A tibble: 5 √ó 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 20 ## 2 2 21 ## 3 3 22 ## 4 3 NA ## 5 4 23  Pull df %\u0026gt;% pull(var = x) ## [1] 1 2 3 4 df %\u0026gt;% pull(var = x) %\u0026gt;% sum() ## [1] 10  Select cbp %\u0026gt;%  select(ind:mother) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 3 ## ind father mother ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 NA NA ## 2 2 NA NA ## 3 3 NA NA ## 4 4 NA NA ## 5 5 NA NA cbp %\u0026gt;%  select(starts_with(c(\u0026quot;p\u0026quot;,\u0026quot;m\u0026quot;))| ends_with(\u0026quot;r\u0026quot;)) %\u0026gt;%   slice_head(n=5) ## # A tibble: 5 √ó 4 ## phenotype mother father year ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 37.7 NA NA 0 ## 2 35.8 NA NA 0 ## 3 28.4 NA NA 0 ## 4 33.6 NA NA 0 ## 5 32.9 NA NA 0 cbp %\u0026gt;%  select(starts_with(c(\u0026quot;p\u0026quot;,\u0026quot;m\u0026quot;)) \u0026amp; !ends_with(\u0026quot;r\u0026quot;)) %\u0026gt;%   slice_head(n=5) ## # A tibble: 5 √ó 1 ## phenotype ## \u0026lt;dbl\u0026gt; ## 1 37.7 ## 2 35.8 ## 3 28.4 ## 4 33.6 ## 5 32.9 cbp %\u0026gt;% select(contains(\u0026quot;a\u0026quot;)) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 2 ## father year ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 NA 0 ## 2 NA 0 ## 3 NA 0 ## 4 NA 0 ## 5 NA 0  Relocate cbp %\u0026gt;%  relocate(mother, .before = father) %\u0026gt;%  relocate(year, herd, .after = sex) ## # A tibble: 10,000 √ó 7 ## ind mother father sex year herd phenotype ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 NA NA M 0 E 37.7 ## 2 2 NA NA M 0 B 35.8 ## 3 3 NA NA M 0 A 28.4 ## 4 4 NA NA M 0 D 33.6 ## 5 5 NA NA M 0 A 32.9 ## 6 6 NA NA M 0 A 31.6 ## 7 7 NA NA M 0 A 38.8 ## 8 8 NA NA M 0 E 33.3 ## 9 9 NA NA M 0 E 39.0 ## 10 10 NA NA M 0 C 46.0 ## # ‚Ä¶ with 9,990 more rows cbp %\u0026gt;%  relocate(where(is.factor),   .after = last_col()) ## # A tibble: 10,000 √ó 7 ## ind father mother phenotype year sex herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 37.7 0 M E ## 2 2 NA NA 35.8 0 M B ## 3 3 NA NA 28.4 0 M A ## 4 4 NA NA 33.6 0 M D ## 5 5 NA NA 32.9 0 M A ## 6 6 NA NA 31.6 0 M A ## 7 7 NA NA 38.8 0 M A ## 8 8 NA NA 33.3 0 M E ## 9 9 NA NA 39.0 0 M E ## 10 10 NA NA 46.0 0 M C ## # ‚Ä¶ with 9,990 more rows  Mutate cbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%  dplyr::mutate(logPheno = log10(phenotype)) %\u0026gt;%   slice_head(n=5) ## # A tibble: 5 √ó 3 ## sex phenotype logPheno ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 M 37.7 1.58 ## 2 M 35.8 1.55 ## 3 M 28.4 1.45 ## 4 M 33.6 1.53 ## 5 M 32.9 1.52 cbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%  dplyr::mutate(rankPheno = min_rank(desc(phenotype))) %\u0026gt;%  dplyr::arrange(rankPheno) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 3 ## sex phenotype rankPheno ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 M 82.1 1 ## 2 F 80.0 2 ## 3 M 79.1 3 ## 4 F 78.9 4 ## 5 F 78.6 5 cbp %\u0026gt;%  dplyr::mutate(dplyr::across(!phenotype, as.factor))%\u0026gt;%   dplyr::mutate(phenotype = NULL) %\u0026gt;%  slice_head(n=5) ## # A tibble: 5 √ó 6 ## ind father mother year sex herd ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M E ## 2 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M B ## 3 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A ## 4 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M D ## 5 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A cbp %\u0026gt;% select(sex, herd, phenotype) %\u0026gt;%  dplyr::mutate(herdSex = sex:herd) %\u0026gt;%  relocate(herdSex, .before = phenotype) %\u0026gt;%   slice_head(n=5) ## # A tibble: 5 √ó 4 ## sex herd herdSex phenotype ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 M E M:E 37.7 ## 2 M B M:B 35.8 ## 3 M A M:A 28.4 ## 4 M D M:D 33.6 ## 5 M A M:A 32.9  Transmute cbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%  dplyr::transmute(logPheno = log10(phenotype)) %\u0026gt;%   slice_head(n=5) ## # A tibble: 5 √ó 1 ## logPheno ## \u0026lt;dbl\u0026gt; ## 1 1.58 ## 2 1.55 ## 3 1.45 ## 4 1.53 ## 5 1.52 cbp %\u0026gt;% select(sex, phenotype) %\u0026gt;%  dplyr::transmute(rankPheno = min_rank(desc(phenotype))) %\u0026gt;%  dplyr::arrange(rankPheno) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 1 ## rankPheno ## \u0026lt;int\u0026gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 cbp %\u0026gt;%  dplyr::transmute(dplyr::across(!phenotype, as.factor))%\u0026gt;%   dplyr::mutate(phenotype = NULL) %\u0026gt;%  slice_head(n=5) ## # A tibble: 5 √ó 6 ## ind father mother year sex herd ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M E ## 2 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M B ## 3 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A ## 4 4 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M D ## 5 5 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 0 M A cbp %\u0026gt;% select(sex, herd, phenotype) %\u0026gt;%  dplyr::transmute(herdSex = sex:herd) %\u0026gt;%  slice_head(n=5) ## # A tibble: 5 √ó 1 ## herdSex ## \u0026lt;fct\u0026gt; ## 1 M:E ## 2 M:B ## 3 M:A ## 4 M:D ## 5 M:A  Across cbp %\u0026gt;%  dplyr::mutate(across(phenotype, round, 2)) ## # A tibble: 10,000 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A ## 6 6 NA NA 0 M 31.6 A ## 7 7 NA NA 0 M 38.8 A ## 8 8 NA NA 0 M 33.3 E ## 9 9 NA NA 0 M 39.0 E ## 10 10 NA NA 0 M 46.0 C ## # ‚Ä¶ with 9,990 more rows # Centering variable fun \u0026lt;- function(x, na.rm = TRUE){  x - mean(x, na.rm = na.rm) } cbp %\u0026gt;%  dplyr::mutate(year2 = as.numeric(year)-1) %\u0026gt;%  dplyr::mutate(across(c(year2, phenotype), fun)) ## # A tibble: 10,000 √ó 8 ## ind father mother year sex phenotype herd year2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 NA NA 0 M -12.3 E -4.5 ## 2 2 NA NA 0 M -14.2 B -4.5 ## 3 3 NA NA 0 M -21.6 A -4.5 ## 4 4 NA NA 0 M -16.4 D -4.5 ## 5 5 NA NA 0 M -17.1 A -4.5 ## 6 6 NA NA 0 M -18.4 A -4.5 ## 7 7 NA NA 0 M -11.2 A -4.5 ## 8 8 NA NA 0 M -16.7 E -4.5 ## 9 9 NA NA 0 M -11.0 E -4.5 ## 10 10 NA NA 0 M -4.02 C -4.5 ## # ‚Ä¶ with 9,990 more rows  Rename cbp %\u0026gt;%  rename(Pheno = phenotype) %\u0026gt;% slice_head(n=5) ## # A tibble: 5 √ó 7 ## ind father mother year sex Pheno herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A cbp %\u0026gt;%  rename_with(~(toupper(gsub(\u0026quot;r\u0026quot;, \u0026quot;r2\u0026quot;, .x, fixed = TRUE)))) %\u0026gt;%  slice_head(n=5) ## # A tibble: 5 √ó 7 ## IND FATHER2 MOTHER2 YEAR2 SEX PHENOTYPE HER2D ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A  Group by # Centering variable fun \u0026lt;- function(x, na.rm = TRUE){  x - mean(x, na.rm = na.rm) } cbp %\u0026gt;%  group_by(herd) %\u0026gt;%  dplyr::mutate(year2 = as.numeric(year)-1) %\u0026gt;%  dplyr::mutate(across(c(year2, phenotype), fun)) ## # A tibble: 10,000 √ó 8 ## # Groups: herd [5] ## ind father mother year sex phenotype herd year2 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 NA NA 0 M -12.4 E -4.5 ## 2 2 NA NA 0 M -14.2 B -4.5 ## 3 3 NA NA 0 M -21.6 A -4.5 ## 4 4 NA NA 0 M -16.2 D -4.5 ## 5 5 NA NA 0 M -17.2 A -4.5 ## 6 6 NA NA 0 M -18.4 A -4.5 ## 7 7 NA NA 0 M -11.2 A -4.5 ## 8 8 NA NA 0 M -16.8 E -4.5 ## 9 9 NA NA 0 M -11.1 E -4.5 ## 10 10 NA NA 0 M -4.11 C -4.5 ## # ‚Ä¶ with 9,990 more rows   Statistics Summary Summarise s1 \u0026lt;- cbp %\u0026gt;%  group_by(year, sex, herd) %\u0026gt;%  summarise(  mean = mean(phenotype),  median = median(phenotype),  min = min(phenotype),  max = max(phenotype),  IQR = IQR(phenotype),  sd = sd(phenotype),  var = sd^2,  n = n()  ) %\u0026gt;%  dplyr::mutate(across(where(is.numeric), round,1))  s1 %\u0026gt;%  ggplot(aes(y=mean, x =year)) +  geom_point(aes(shape =herd, colour = herd)) +  facet_wrap(~sex) +  labs(x = \u0026quot;Generation\u0026quot;, y = \u0026quot;Milk Yield (kg/day\u0026quot;,  colour = \u0026quot;Herd\u0026quot;, shape = \u0026quot;Herd\u0026quot;) +  theme_bw(base_size = 13) ggsave(\u0026quot;milk.pdf\u0026quot;, width = 5, height = 3) s1 %\u0026gt;%  filter(year %in% c(\u0026quot;8\u0026quot;,\u0026quot;9\u0026quot;) \u0026amp;   max \u0026gt; 76 \u0026amp; min \u0026gt;40) ## # A tibble: 7 √ó 11 ## # Groups: year, sex [4] ## year sex herd mean median min max IQR sd var n ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 8 F B 58.3 57.9 41.7 78.6 8.9 6.6 43.2 114 ## 2 8 M C 59.7 59.9 45.3 77.6 7.3 6 35.8 108 ## 3 8 M D 59.2 59.1 45.1 76.7 7.7 6.3 39.5 106 ## 4 9 F E 59.6 60.1 42.9 77.4 7.5 6.3 40.3 99 ## 5 9 M B 61.1 60.7 45 79.1 11.1 7.2 51.9 86 ## 6 9 M C 61.2 61 42.4 82.1 8.5 6.5 41.8 108 ## 7 9 M E 60.8 59.6 45.1 77.7 8.4 6.3 39.9 101 s1 %\u0026gt;%  filter(year %in% c(\u0026quot;8\u0026quot;,\u0026quot;9\u0026quot;) \u0026amp;   max \u0026gt; 76 \u0026amp; min \u0026gt;40) %\u0026gt;%  transmute(CV = sd / mean,   CV = scales::percent(CV)) %\u0026gt;%  dplyr::mutate(across(where(is.numeric), round,1)) ## # A tibble: 7 √ó 3 ## # Groups: year, sex [4] ## year sex CV ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 8 F 11% ## 2 8 M 10.05% ## 3 8 M 10.64% ## 4 9 F 11% ## 5 9 M 11.78% ## 6 9 M 10.62% ## 7 9 M 10.36%   ","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"b25b1e0ebe7029da45a741031abf4603","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/oliveiratp_tidyverse_slidescode/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/courses/2021_ddbg/oliveiratp_tidyverse_slidescode/","section":"courses","summary":"pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .","tags":null,"title":"Tidyverse: R code","type":"courses"},{"authors":null,"categories":null,"content":"  pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .sourceCode { overflow: visible; } code.sourceCode  span { color: inherit; text-decoration: inherit; } div.sourceCode { margin: 1em 0; } pre.sourceCode { margin: 0; } @media screen { div.sourceCode { overflow: auto; } } @media print { pre  code.sourceCode { white-space: pre-wrap; } pre  code.sourceCode  span { text-indent: -5em; padding-left: 5em; } } pre.numberSource code { counter-reset: source-line 0; } pre.numberSource code  span { position: relative; left: -4em; counter-increment: source-line; } pre.numberSource code  span  a:first-child::before { content: counter(source-line); position: relative; left: -1em; text-align: right; vertical-align: baseline; border: none; display: inline-block; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; padding: 0 4px; width: 4em; background-color: #ffffff; color: #a0a0a0; } pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0; padding-left: 4px; } div.sourceCode { color: #1f1c1b; background-color: #ffffff; } @media screen { pre  code.sourceCode  span  a:first-child::before { text-decoration: underline; } } code span { color: #1f1c1b; } /* Normal */ code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */ code span.an { color: #ca60ca; } /* Annotation */ code span.at { color: #0057ae; } /* Attribute */ code span.bn { color: #b08000; } /* BaseN */ code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */ code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */ code span.ch { color: #924c9d; } /* Char */ code span.cn { color: #aa5500; } /* Constant */ code span.co { color: #898887; } /* Comment */ code span.cv { color: #0095ff; } /* CommentVar */ code span.do { color: #607880; } /* Documentation */ code span.dt { color: #0057ae; } /* DataType */ code span.dv { color: #b08000; } /* DecVal */ code span.er { color: #bf0303; text-decoration: underline; } /* Error */ code span.ex { color: #0095ff; font-weight: bold; } /* Extension */ code span.fl { color: #b08000; } /* Float */ code span.fu { color: #644a9b; } /* Function */ code span.im { color: #ff5500; } /* Import */ code span.in { color: #b08000; } /* Information */ code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */ code span.op { color: #1f1c1b; } /* Operator */ code span.ot { color: #006e28; } /* Other */ code span.pp { color: #006e28; } /* Preprocessor */ code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */ code span.sc { color: #3daee9; } /* SpecialChar */ code span.ss { color: #ff5500; } /* SpecialString */ code span.st { color: #bf0303; } /* String */ code span.va { color: #0057ae; } /* Variable */ code span.vs { color: #bf0303; } /* VerbatimString */ code span.wa { color: #bf0303; } /* Warning */   Tidy data  Exercise 1 Exercise 2  Exercise 3 Data Structure  Exercise 1 Exercise 2 Exercise 3    Tidy data Tidy data refers to data arranged to make data processing, analysis, and visualization simpler. Remember that in a tidy data set we should consider:\n Each variable must have its column. Each observation must have its row. Each value must have its cell.  Exercise 1 Let‚Äôs say we want to organize the data anscombe. Below I shpw how this data looks like:\nanscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.10 5.39 12.50 ## 9 12 12 12 8 10.84 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 Organize this data set to obtain tidy data. Remember here we have two response variables been measured four times.  Most of the selecting, separating, mutating and renaming is taking place within the pivot function calls.\n(ex1 \u0026lt;- anscombe %\u0026gt;%  pivot_longer(everything(),  names_to = c(\u0026quot;.value\u0026quot;, \u0026quot;rep\u0026quot;),  names_pattern = \u0026quot;(.)([0-9])\u0026quot;  )) ## # A tibble: 44 √ó 3 ## rep x y ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 10 8.04 ## 2 2 10 9.14 ## 3 3 10 7.46 ## 4 4 8 6.58 ## 5 1 8 6.95 ## 6 2 8 8.14 ## 7 3 8 6.77 ## 8 4 8 5.76 ## 9 1 13 7.58 ## 10 2 13 8.74 ## # ‚Ä¶ with 34 more rows Filter the data set to get replications 2 and 4, and summarise it to get the maximum, minimum, and mean values.  ex1 %\u0026gt;%  filter(rep %in% c(2,4)) %\u0026gt;%  summarise(  across(c(x,y), list(mean = mean, min = min, max = max),  .names = \u0026quot;{.col}.{.fn}\u0026quot;  )) ## # A tibble: 1 √ó 6 ## x.mean x.min x.max y.mean y.min y.max ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 9 4 19 7.50 3.1 12.5  Exercise 2 Often you do not need the entire data set, but just part of it.\nHere, you should make the data mtcars tidy before making any selection.  (dataEx3 \u0026lt;- readRDS(\u0026quot;./data/dataEx3.rds\u0026quot;)) ## # A tibble: 32 √ó 12 ## cyl disp hp drat wt qsec vs am carb `4` `3` `5` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 6 160 110 3.9 2.62 16.5 0 1 4 21 NA NA ## 2 6 160 110 3.9 2.88 17.0 0 1 4 21 NA NA ## 3 4 108 93 3.85 2.32 18.6 1 1 1 22.8 NA NA ## 4 6 258 110 3.08 3.22 19.4 1 0 1 NA 21.4 NA ## 5 8 360 175 3.15 3.44 17.0 0 0 2 NA 18.7 NA ## 6 6 225 105 2.76 3.46 20.2 1 0 1 NA 18.1 NA ## 7 8 360 245 3.21 3.57 15.8 0 0 4 NA 14.3 NA ## 8 4 147. 62 3.69 3.19 20 1 0 2 24.4 NA NA ## 9 4 141. 95 3.92 3.15 22.9 1 0 2 22.8 NA NA ## 10 6 168. 123 3.92 3.44 18.3 1 0 4 19.2 NA NA ## # ‚Ä¶ with 22 more rows As you can see, some columns are not variable names but values. Create two new variables calling mpg (for observations) and gear (with column values).\ndataEx3 \u0026lt;- dataEx3 %\u0026gt;%  pivot_longer(  cols = matches(\u0026quot;([1-9])\u0026quot;),  names_to = \u0026quot;gear\u0026quot;,  values_to = \u0026quot;mpg\u0026quot;,  values_drop_na = TRUE  ) Select the columns mpg, hp, gear, and carb, and then make a plot using ggplot2 where mpg is the response variable, and hp is the co-variate in the x-axis. Also include different shapes and colours for gear, and facets for carb.  dataEx3 %\u0026gt;%  select(mpg, hp, gear, carb) %\u0026gt;%  ggplot(aes(y=mpg, x = hp, shape = gear,  colour = gear)) +  geom_point() +  facet_wrap(~carb) +  theme_bw()   Exercise 3 The following data represents song rankings for Billboard top 100 in the year 2000. The rank of the song is displayed in each week after it entered.\nbillboard ## # A tibble: 317 √ó 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2 Pac Baby D‚Ä¶ 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+her The Ha‚Ä¶ 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doors‚Ä¶ Krypto‚Ä¶ 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doors‚Ä¶ Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 Boyz Wobble‚Ä¶ 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give M‚Ä¶ 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Teens Dancin‚Ä¶ 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliyah I Don\u0026#39;‚Ä¶ 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliyah Try Ag‚Ä¶ 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams, ‚Ä¶ Open M‚Ä¶ 2000-08-26 76 76 74 69 68 67 61 58 ## # ‚Ä¶ with 307 more rows, and 68 more variables: wk9 \u0026lt;dbl\u0026gt;, wk10 \u0026lt;dbl\u0026gt;, ## # wk11 \u0026lt;dbl\u0026gt;, wk12 \u0026lt;dbl\u0026gt;, wk13 \u0026lt;dbl\u0026gt;, wk14 \u0026lt;dbl\u0026gt;, wk15 \u0026lt;dbl\u0026gt;, wk16 \u0026lt;dbl\u0026gt;, ## # wk17 \u0026lt;dbl\u0026gt;, wk18 \u0026lt;dbl\u0026gt;, wk19 \u0026lt;dbl\u0026gt;, wk20 \u0026lt;dbl\u0026gt;, wk21 \u0026lt;dbl\u0026gt;, wk22 \u0026lt;dbl\u0026gt;, ## # wk23 \u0026lt;dbl\u0026gt;, wk24 \u0026lt;dbl\u0026gt;, wk25 \u0026lt;dbl\u0026gt;, wk26 \u0026lt;dbl\u0026gt;, wk27 \u0026lt;dbl\u0026gt;, wk28 \u0026lt;dbl\u0026gt;, ## # wk29 \u0026lt;dbl\u0026gt;, wk30 \u0026lt;dbl\u0026gt;, wk31 \u0026lt;dbl\u0026gt;, wk32 \u0026lt;dbl\u0026gt;, wk33 \u0026lt;dbl\u0026gt;, wk34 \u0026lt;dbl\u0026gt;, ## # wk35 \u0026lt;dbl\u0026gt;, wk36 \u0026lt;dbl\u0026gt;, wk37 \u0026lt;dbl\u0026gt;, wk38 \u0026lt;dbl\u0026gt;, wk39 \u0026lt;dbl\u0026gt;, wk40 \u0026lt;dbl\u0026gt;, ## # wk41 \u0026lt;dbl\u0026gt;, wk42 \u0026lt;dbl\u0026gt;, wk43 \u0026lt;dbl\u0026gt;, wk44 \u0026lt;dbl\u0026gt;, wk45 \u0026lt;dbl\u0026gt;, wk46 \u0026lt;dbl\u0026gt;, ‚Ä¶ A slightly more complex case where columns have a common prefix and missing missings are structural, so should be dropped. So, make this data tidy.\nbillboard %\u0026gt;%  pivot_longer(  cols = starts_with(\u0026quot;wk\u0026quot;),  names_to = \u0026quot;week\u0026quot;,  names_prefix = \u0026quot;wk\u0026quot;,  values_to = \u0026quot;rank\u0026quot;,  values_drop_na = TRUE  )  Data Structure Exercise 1 Make this data tidy by including tmin and tmax as variable. Remember that here type is carrying to variables names rather than factors.  (dataEx2 \u0026lt;- as_tibble(readRDS(\u0026quot;./data/dataEx2.RDS\u0026quot;))) ## # A tibble: 20 √ó 4 ## id date type value ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Ind 1 2020-01-01 tmin 35.8 ## 2 Ind 1 2020-01-01 tmax 36.7 ## 3 Ind 1 2020-01-02 tmin 35.9 ## 4 Ind 1 2020-01-02 tmax 36.8 ## 5 Ind 1 2020-01-03 tmin 36.0 ## 6 Ind 1 2020-01-03 tmax 36.8 ## 7 Ind 1 2020-01-04 tmin 36.1 ## 8 Ind 1 2020-01-04 tmax 36.9 ## 9 Ind 1 2020-01-05 tmin 36.1 ## 10 Ind 1 2020-01-05 tmax 36.9 ## 11 Ind 1 2020-01-06 tmin 36.2 ## 12 Ind 1 2020-01-06 tmax 37.0 ## 13 Ind 1 2020-01-07 tmin 36.3 ## 14 Ind 1 2020-01-07 tmax 37.1 ## 15 Ind 1 2020-01-08 tmin 36.3 ## 16 Ind 1 2020-01-08 tmax 37.2 ## 17 Ind 1 2020-01-09 tmin 36.6 ## 18 Ind 1 2020-01-09 tmax 37.3 ## 19 Ind 1 2020-01-10 tmin 36.6 ## 20 Ind 1 2020-01-10 tmax 38.0 (dataEx2 \u0026lt;- dataEx2 %\u0026gt;%  pivot_wider(values_from = value, names_from = type)) ## # A tibble: 10 √ó 4 ## id date tmin tmax ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Ind 1 2020-01-01 35.8 36.7 ## 2 Ind 1 2020-01-02 35.9 36.8 ## 3 Ind 1 2020-01-03 36.0 36.8 ## 4 Ind 1 2020-01-04 36.1 36.9 ## 5 Ind 1 2020-01-05 36.1 36.9 ## 6 Ind 1 2020-01-06 36.2 37.0 ## 7 Ind 1 2020-01-07 36.3 37.1 ## 8 Ind 1 2020-01-08 36.3 37.2 ## 9 Ind 1 2020-01-09 36.6 37.3 ## 10 Ind 1 2020-01-10 36.6 38.0 Now, build a new variable called tdiff, which is the difference between tmax and tmin. Moreover, display a ggplot2 graph that shows tdiff over time.\ndataEx2 %\u0026gt;%  dplyr::mutate(tdiff = tmax - tmin) %\u0026gt;%  ggplot(aes(y = tdiff, x = date)) +  geom_point() +  geom_smooth(se = FALSE) +  ylab(\u0026quot;tmax - tmin\u0026quot;) + xlab (\u0026quot;Date\u0026quot;) +  theme_classic()  Exercise 2 Our cattle data data is already in a tidy format.\n(cbp \u0026lt;- readRDS(\u0026quot;./data/animal_sim.RDS\u0026quot;)) ## # A tibble: 10,000 √ó 7 ## ind father mother year sex phenotype herd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 NA NA 0 M 37.7 E ## 2 2 NA NA 0 M 35.8 B ## 3 3 NA NA 0 M 28.4 A ## 4 4 NA NA 0 M 33.6 D ## 5 5 NA NA 0 M 32.9 A ## 6 6 NA NA 0 M 31.6 A ## 7 7 NA NA 0 M 38.8 A ## 8 8 NA NA 0 M 33.3 E ## 9 9 NA NA 0 M 39.0 E ## 10 10 NA NA 0 M 46.0 C ## # ‚Ä¶ with 9,990 more rows For this exercise, complete the following tasks with that data set:\nCalculate the average phenotype per year by sex and herd using the summarise() function in the dplyr package. Add two columns to cattle data using the mutate() function: Column 1: Phenotype should be rescaled to have a mean of zero and a standard deviation of one. You can call this new variable as PhenoStd. Column 2: Rank the PhenoStd using the function min_rank(). The output data frame should have only PhenoStd \u0026gt; 0.   cbp %\u0026gt;%  group_by(year, sex, herd) %\u0026gt;%  summarise(  mean = mean(phenotype)  ) ## # A tibble: 100 √ó 4 ## # Groups: year, sex [20] ## year sex herd mean ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 F A 40.3 ## 2 0 F B 40.6 ## 3 0 F C 40.3 ## 4 0 F D 39.1 ## 5 0 F E 40.1 ## 6 0 M A 41.0 ## 7 0 M B 40.9 ## 8 0 M C 40.7 ## 9 0 M D 39.9 ## 10 0 M E 40.4 ## # ‚Ä¶ with 90 more rows .scale \u0026lt;- function(x){  (x - mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE) }  cbp %\u0026gt;%  dplyr::mutate(PhenoStd = .scale(phenotype)) %\u0026gt;%  dplyr::mutate(RankPhenoStd = min_rank(PhenoStd)) %\u0026gt;%  filter(PhenoStd \u0026gt; 0) ## # A tibble: 5,057 √ó 9 ## ind father mother year sex phenotype herd PhenoStd RankPhenoStd ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 11 NA NA 0 M 51.0 A 0.103 5338 ## 2 23 NA NA 0 M 51.9 E 0.191 5661 ## 3 29 NA NA 0 M 53.5 C 0.355 6258 ## 4 34 NA NA 0 M 50.1 D 0.00987 4981 ## 5 61 NA NA 0 M 53.6 C 0.366 6306 ## 6 65 NA NA 0 M 55.2 A 0.529 6883 ## 7 69 NA NA 0 M 51.3 C 0.135 5461 ## 8 84 NA NA 0 M 52.0 A 0.207 5726 ## 9 90 NA NA 0 M 50.5 B 0.0506 5144 ## 10 106 NA NA 0 M 52.1 C 0.209 5731 ## # ‚Ä¶ with 5,047 more rows  Exercise 3 Excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country. This data has 142 countries observed from the year 1952 to 2007 in increments of 5 years. The response variable observed was the life expectancy at birth (in years), population size, and Per capita gross domestic product (GDP).\nPer capita gross domestic product (GDP) measures a country‚Äôs economic response per person and is calculated by dividing its GDP by its population. It is a global measure for gauging the prosperity of nations as we can analyze the worth of a country based on its economic growth. Thus, countries that have the highest per capita GDP tend to be more developed.\ngapminder ## # A tibble: 1,704 √ó 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ‚Ä¶ with 1,694 more rows Questions:\nWhat are the ten highest gdpPercap values?  gapminder %\u0026gt;%  slice_max(gdpPercap, n = 10) ## # A tibble: 10 √ó 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Kuwait Asia 1957 58.0 212846 113523. ## 2 Kuwait Asia 1972 67.7 841934 109348. ## 3 Kuwait Asia 1952 55.6 160000 108382. ## 4 Kuwait Asia 1962 60.5 358266 95458. ## 5 Kuwait Asia 1967 64.6 575003 80895. ## 6 Kuwait Asia 1977 69.3 1140357 59265. ## 7 Norway Europe 2007 80.2 4627926 49357. ## 8 Kuwait Asia 2007 77.6 2505559 47307. ## 9 Singapore Asia 2007 80.0 4553009 47143. ## 10 Norway Europe 2002 79.0 4535591 44684. Find both the median life expectancy (lifeExp) and the median and maximum GDP per capita (gdpPercap) in 1957, 1982, and 2007, by country and continent. Call them medianLifeExp, medianGdpPercap, and maxGdpPercap, respectively.  (dat \u0026lt;- gapminder %\u0026gt;%  filter(year %in% c(\u0026quot;1957\u0026quot;,\u0026quot;1982\u0026quot;,\u0026quot;2007\u0026quot;)) %\u0026gt;%  group_by(year, country, continent) %\u0026gt;%  summarise(  medianlifeExp = median(lifeExp),  medianGdpPercap = median(gdpPercap),  maxGdpPercap = max(gdpPercap)  )) ## # A tibble: 426 √ó 6 ## # Groups: year, country [426] ## year country continent medianlifeExp medianGdpPercap maxGdpPercap ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1957 Afghanistan Asia 30.3 821. 821. ## 2 1957 Albania Europe 59.3 1942. 1942. ## 3 1957 Algeria Africa 45.7 3014. 3014. ## 4 1957 Angola Africa 32.0 3828. 3828. ## 5 1957 Argentina Americas 64.4 6857. 6857. ## 6 1957 Australia Oceania 70.3 10950. 10950. ## 7 1957 Austria Europe 67.5 8843. 8843. ## 8 1957 Bahrain Asia 53.8 11636. 11636. ## 9 1957 Bangladesh Asia 39.3 662. 662. ## 10 1957 Belgium Europe 69.2 9715. 9715. ## # ‚Ä¶ with 416 more rows Use a scatter plot to compare the median GDP and median life expectancy. Use the variables continent and year to produce this plot.  dat %\u0026gt;%  ggplot(aes(x = medianGdpPercap, y = medianlifeExp)) +  facet_wrap(~ continent) +  geom_point(shape = 1)   ","date":1640736000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640736000,"objectID":"4550a78f519351e225018b836c431d9c","permalink":"https://prof-thiagooliveira.netlify.com/courses/2021_ddbg/oliveiratp_tidyverse_answers/","publishdate":"2021-12-29T00:00:00Z","relpermalink":"/courses/2021_ddbg/oliveiratp_tidyverse_answers/","section":"courses","summary":"pre  code.sourceCode { white-space: pre; position: relative; } pre  code.sourceCode  span { display: inline-block; line-height: 1.25; } pre  code.sourceCode  span:empty { height: 1.2em; } .","tags":null,"title":"Tidyverse: answers","type":"courses"},{"authors":["Leticia A. P. Lara","Ivan Pocrnic","Thiago de Paula Oliveira","Chris Gaynor","Gregor Gorjanc"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"3e3c911af88919b6c71b633c4c220cbc","permalink":"https://prof-thiagooliveira.netlify.com/publication/2021-llara_gen_var_plants/","publishdate":"2021-12-15T00:00:00Z","relpermalink":"/publication/2021-llara_gen_var_plants/","section":"publication","summary":"Evaluate the temporal and genomic analysis of additive genetic variance in different stages of a breeding programme","tags":["Genomic analysis","Genetic variance","Breeding programme","Simulation","Variance components","Linkage-disequilibrium"],"title":"Temporal and genomic analysis of additive genetic variance in breeding programmes","type":"publication"},{"authors":["Thiago de Paula Oliveira"],"categories":null,"content":"","date":1632135600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632135600,"objectID":"0252c4a410d7deea4750e2580610a0f0","permalink":"https://prof-thiagooliveira.netlify.com/talk/visualization-and-data-structure/","publishdate":"2021-09-20T12:00:00Z","relpermalink":"/talk/visualization-and-data-structure/","section":"event","summary":"Discussion on the principles of grammar of graphics and application using the ggplot2 package","tags":["ggplot2","Visualization","Data structure","Grammar of graphics"],"title":"Visualization and Data Structure","type":"event"},{"authors":["Thiago de Paula Oliveira"],"categories":null,"content":"","date":1632135600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632135600,"objectID":"f9358607db712ff78a492a3ee9144f86","permalink":"https://prof-thiagooliveira.netlify.com/talk/visualization-and-data-structure/","publishdate":"2021-09-20T12:00:00Z","relpermalink":"/talk/visualization-and-data-structure/","section":"event","summary":"Discussion on the principles of data manipulation using dplyr, tidyr, and readr","tags":["ggplot2","Visualization","Data structure","Grammar of graphics"],"title":"Visualization and Data Structure","type":"event"},{"authors":["Gregor Gorjanc","Jana Obsteter","Thiago de Paula Oliveira"],"categories":null,"content":"","date":1618012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618012800,"objectID":"7c654db9d4dcf96b387e1265ffc11300","permalink":"https://prof-thiagooliveira.netlify.com/publication/2021-alphapart-package/","publishdate":"2021-04-07T00:00:00Z","relpermalink":"/publication/2021-alphapart-package/","section":"publication","summary":"Partitioning genetic trends to quantify the sources of genetic gain in breeding programmes","tags":["R Package","Genetic","Quantitative Genetic","C++"],"title":"Partition/Decomposition of Breeding Values by Paths of Information","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral"],"categories":null,"content":"Supplementary notes were added here:\n  Github\n  Dashboard App\n  ","date":1617667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617667200,"objectID":"1d5f658795ef3d36ee6ca5bee4011ccb","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-covid/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/publication/2020-covid/","section":"publication","summary":"We show our model generates high accurate forecasts up to seven days ahead for COVID-19 reported cases.","tags":["COVID-19","SARS-Cov-2","Pandemic","Statistical Methodology","Statistical Modelling","Longitudinal data","State-Space Models","Hierarchical data"],"title":"Global Short-Term Forecasting of Covid-19 Cases","type":"publication"},{"authors":["Thiago de Paula Oliveira"],"categories":["C++","Computer Programs"],"content":"  Introduction Expressions are combinations of operators, values (constants), and variables arranged according to the rules established throughout the code. Thus, every expression is any part of a statement that returns a value, as in the following example:\nThis statement creates a box to store the value of \\(x\\) and another to keep the value of \\(y\\) equal to the expression \\(x\\) plus 13 (\\(y=23\\)). Now consider a more complex statement:\nThis statment consists of three expressions:  The results of the expression \\(3 - x\\) is stored in the variable \\(y\\) The expression \\(y = 3 - x\\) returns the value of \\(y\\), and it is stored in the variable \\(v\\) The results of the expression \\(y \\times \\left(\\frac{v}{5} + x\\right)\\) is stored in the variable \\(z\\)   Remember that multiplication and division occur before addition and subtraction. Ex.:\n1-3*4 = -11 2/3-4*2/3 = -2 2/3-4/4*2/3 = 0 The operator precedence dictates the order of evaluation of operators in an expression. In C, each operator has a fixed priority order to be executed or precedence concerning other operators. As multiplication or division has higher precedence than addition and subtraction, in the expression \\(\\frac{2}{4}-3+ 4 \\times 6\\), firstly, the subexpressions \\(\\frac{2}{4}\\) and \\(4 \\times 6\\) will be evaluated (Step 1 in Figure 1), and then addition and subtraction (Step 2 in Figure 1). Note that multiplication and division, or addition and subtraction have the same precedence, then they are evaluated from left to right due to their associativity.\n Figure 1: Precedence order  Associativity defines the order in which operators of the same precedence are evaluated in an expression, and it can be either from left to right or right to left (Figure 2). Generally, addition, subtraction, multiplication, and division operators are usually left-associative, while assignment operators are typically right-associative. Besides, some operators have no defined behaviour when used in sequence over an expression, and they are called non-associative (Figure 2). When we include parentheses, we can force an expression to be right-associative rather than left-associative as usual.\n Figure 2: Example of left-associative, right-associative, and non-associative   Using Parentheses () The operator () has the highest precedente order (see Table 1), as consequence, we can use parentheses to change the sequence of operators. Consider the following example:\n5 + 6 * 7 The * operator is evaluated firstly, followed by the + operator, so the result is \\(5+6\\times 7 = 47\\). However, if we want to account for the addiction first and then the multiplication, we can rewrite the code as:\n(5 + 6) * 7 Then, the program will compute \\(\\left(5+6\\right)\\times 7=11\\times 7=77\\). Sometimes, parentheses‚Äô inclusion should be important to make your code easier to understand, and therefore easier to maintain.\n Modulus operator (%) The modulus operator evaluates the remainder when dividing the first operand by the second one. Ex.: a % b is the remainder when \\(a\\) is divided by \\(b\\) (\\(a\\) modulus \\(b\\)). by \\(b\\) (\\(a\\) modulus \\(b\\)).\n Figure 3: Example of modulus   Dividing an integer by another one gives an integer.   Example: int x = 10; int y = 3; x/y = 10/3 = 3 (dividing two integers) x % y = 1 (modulus)   Short hand or syntatic sugar Short hand expressions provide a straightforward way to write common patterns over the algorithm for initialized variables.\n  Short hand Meaning Prefix and Postfix    \\(x+=y\\) \\(x=x+y\\)   \\(x-=y\\) \\(x=x-y\\)   \\(x*=y\\) \\(x= x \\times y\\)   \\(x/=y\\) \\(x=x/y\\)   \\(x++\\) \\(x=x+1\\) Return the value of \\(x\\) first then increment it  \\(++x\\) \\(x=x+1\\) Increment first then return the value of \\(x\\)  \\(x--\\) \\(x=x-1\\) Return the value of \\(x\\) first then increment it  \\(--x\\) \\(x=x-1\\) Increment first then return the value of \\(x\\)    Example 1: Here you can see that y ++= x * z; is calculate as \\(y=y+x \\times z = 30 + 2 \\times 4 = 34\\).\n Example 2: In this example you can see that we used the postfix x++ to first initialize \\(y\\) (\\(y=8 \\times x = 8 \\times 7 = 56\\)) and then update \\(x\\) to x=x+1=8. On the other hand, we used the prefix --y to first update the variable \\(y\\) to y=y-1=55 and then calculate the variable z using the updated \\(y\\) \\(\\left(z = y/5 = 55/5 = 11 \\right)\\).\nNote that when we use x*= (y/z) % 2 the variable \\(x\\) multiply the entire expression after = symbol. This expression is equivalent to x = x * ((y/z) % 2));.\n  Operator precedence and associativity Table 1 shows a list of precedence (ordered) and associativity of C operators. This table was obtained from cppreference.com.\n   Table 1: Precedence and associativity of C operators  Precedence  Operator  Description  Associativity    1  ++ --  Suffix/postfix increment and decrement  Left-to-right    ()  Function call    []  Array subscripting    .  Structure and union member access    -\u0026gt;  Structure and union member access through pointer    (type){list}  Compound literal(C99)    2  ++ --  Prefix increment and decrement[note 1]  Right-to-left    + -  Unary plus and minus    ! ~  Logical NOT and bitwise NOT    (type)  Cast    *  Indirection (dereference)    \u0026amp;  Address-of    sizeof  Size-of[note 2]    _Alignof  Alignment requirement(C11)     3   * / %   Multiplication, division, and remainder  Left-to-right     4   + -   Addition and subtraction     5   \u0026lt;\u0026lt; \u0026gt;\u0026gt;   Bitwise left shift and right shift    6  \u0026lt; \u0026lt;=  For relational operators \u0026lt; and ‚â§ respectively    \u0026gt; \u0026gt;=  For relational operators \u0026gt; and ‚â• respectively     7   == !=   For relational = and ‚â† respectively     8   \u0026amp;   Bitwise AND     9   ^   Bitwise XOR (exclusive or)     10   |   Bitwise OR (inclusive or)     11   \u0026amp;\u0026amp;   Logical AND     12   ||   Logical OR     13   ?:   Ternary conditional[note 3]  Right-to-Left    14[note 4]  =  Simple assignment    += -=  Assignment by sum and difference    *= /= %=  Assignment by product, quotient, and remainder    \u0026lt;\u0026lt;= \u0026gt;\u0026gt;=  Assignment by bitwise left shift and right shift    \u0026amp;= ^= |=  Assignment by bitwise AND, XOR, and OR     15   ,   Comma   Left-to-right     ‚Üë The operand of prefix ++ and -- can‚Äôt be a type cast. This rule grammatically forbids some expressions that would be semantically invalid anyway. Some compilers ignore this rule and detect the invalidity semantically.  ‚Üë The operand of sizeof can‚Äôt be a type cast: the expression sizeof (int) * p is unambiguously interpreted as (sizeof(int)) * p, but not sizeof((int)*p).  ‚Üë The expression in the middle of the conditional operator (between ? and :) is parsed as if parenthesized: its precedence relative to ?: is ignored.  ‚Üë Assignment operators‚Äô left operands must be unary (level-2 non-cast) expressions. This rule grammatically forbids some expressions that would be semantically invalid anyway. Many compilers ignore this rule and detect the invalidity semantically. For example, e = a \u0026lt; d ? a++ : a = d is an expression that cannot be parsed because of this rule. However, many compilers ignore this rule and parse it as e = ( ((a \u0026lt; d) ? (a++) : a) = d ), and then give an error because it is semantically invalid.     References  C Operator Precedence - https://en.cppreference.com/w/c/language/operator_precedence#cite_note-1   Citation For attribution, please cite this work as:  Oliveira T.P. (2020, Dec.¬†16). Expressions in C++\n BibTeX citation  @misc{oliveira2020expression, author = {Oliveira, Thiago}, title = {Expressions in C++}, url = {https://prof-thiagooliveira.netlify.app/post/expressions/}, year = {2020} } Did you find this page helpful? Consider sharing it üôå\n ","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608120954,"objectID":"505ea7ea3b3953f93ed1b36b33bb0f23","permalink":"https://prof-thiagooliveira.netlify.com/post/expressions/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/expressions/","section":"post","summary":"Introduction Expressions are combinations of operators, values (constants), and variables arranged according to the rules established throughout the code. Thus, every expression is any part of a statement that returns a value, as in the following example:","tags":["C++","Computer Science","Computer Programs"],"title":"Expressions in C++","type":"post"},{"authors":["Rafael Moral","Thiago de Paula Oliveira","Andrew Parnell"],"categories":["COVID-19","Statistical Models","Forecast Models"],"content":" tl;dr  Many different variables affect how the pandemic progresses and it is extremely difficult to identify each one, and precisely measure them. The data we have is surely innacurate, but could be a good proxy for understanding the behaviour of the coronavirus outbreak We developed a statistical model to obtain short-term forecasts of the number of COVID-19 cases We constantly update forecasts and make all results freely available to any country in the world through a web app   How many people will get infected tomorrow? ‚ÄúHow many cases do you think we‚Äôre going to have today?‚Äù, my fianc√®e asked me just as I‚Äôm writing this post ‚Äì and quite frankly I‚Äôve asked that myself many times over the last several months. Wouldn‚Äôt it be great if we had a method to accurately predict the number of confirmed COVID-19 cases we‚Äôll have every single day for, say, the next month? If we could do that, we‚Äôd know whether our measures to contain the virus are working, whether we would be able to lift particular restrictions here and there, invest in intensive care units, or whether that wedding we had planned long ago would finally happen or have to be postponed‚Ä¶ again.\nIt is very hard, however, to pinpoint exactly every single factor that affects the number of reported COVID-19 cases, and most importantly, measure them all. Here we try and outline different techniques we could use to try and predict how the outbreak will behave in the future, and show a particular method we have developed to obtain short-term forecasts with a reasonable degree of accuracy. We have packaged the method into an app, which you can access here.\n What strategies can we use? There are many different strategies and mathematical/statistical tools we can use to attempt to predict the future. These can include what we call mechanistic, or compartment models, for example. These make assumptions based on empirical evidence of the biological system being studied and translate them into mathematical equations based on the flow of individuals to/from specific compartments. For COVID-19 the SEIR-type model has been widely used by many research groups to describe the behaviour of the outbreak (see our blog post on the use of SEIR models to predict when the pandemic will end). They are realistic in the sense that they reflect the epidemiological behaviour of the outbreak.\nThere are other alternatives that do not take into account the true biological nature of the phenomenon per se, but may use it as input in a different way. Many machine learning techniques could sometimes be seen as black-box methods, that would e.g.¬†take the reported number of past COVID-19 cases and other variables that we would believe could influence this number and spit out a prediction for tomorrow, or next week, next month, etc. There are cases where these methods are even more accurate than mechanistic models, however there is a trade-off to consider here in terms of prediction accuracy vs.¬†explainability, as discussed here. If a new event or variable comes into play, which could empirically be very important to dictate the future behaviour of the pandemic, it is very difficult to gauge its effects using a black-box method.\nWe could also simply assume that the number of reported COVID-19 cases today is purely a reflection of the reported number of cases yesterday, and the day before, and so on. So we pretty much assume all variables that influence this process can be summarised purely by the outcomes we have observed in the past, and this can in turn be used to forecast what future numbers will be. Of course, there are plenty of different ways to include other variables in these types of models, but the important thing is to notice that we place a very heavy assumption on an underlying process that is able to explain its own behaviour. We usually refer to these models as ‚Äútime series‚Äù or ‚Äústate-space‚Äù models.\n Why is it so difficult? There are many factors that influence our ability to predict the number of future COVID-19 cases. Imagine we have, for example, a fantastic SEIR-type model that can reproduce the dynamics of the disease almost perfectly up to today. To be able to predict with great accuracy what will happen tomorrow (or even further down the line), we must assume, among other things, that the assumptions that hold today will still hold tomorrow and so on. If any new variable comes into play, or if the variables that are involved change over time, our predictions can be completely off.\nThis is not the worst problem, however. There are in fact many variables that we‚Äôre simply not able to measure with good precision. This includes knowing, for example, where everybody in the country is at all times, who they talk to, for how long, where they will be, etc. This is why it is important to do contact tracing, although this matters mostly in a retrospective way, not necessarily to predict what will happen in the future.\nBut wait a minute now, we don‚Äôt even know whether the data we can actually measure is in fact accurate! Or to be more specific, we do know that our data is definitely not 100% accurate. Cases reported today could reflect infections that happened between a few days ago to several weeks. Tests are not 100% accurate either, so there is a pool of false positives in there, as well as false negatives not being included in the whole sum. Simply put, the data we have is pretty much a proxy of the real thing. Hence why it is so important to understand what these numbers could actually mean, and not imbue them with improper meaning.\n What about short-term forecasting? So long-term forecasting is very prone to built-up variation and error, as we all know. It‚Äôs just like predicting what time you‚Äôll wake up on your birthday 10 years from now. But there must be something we could do in the short-term, right? Well, it depends on how ‚Äúlong‚Äù this short-term is. And it also depends on how we want to use this information.\nWe developed a modelling framework in an attempt to predict the number of reported COVID-19 cases for up to 7 days in the future. We fitted our models to the data collected by the ECDC to generate the forecasts. See below for a validation study we carried out back in May/2020.\nThe panels are in the logarithmic scale, but in essence, the closer the points are to the identity line (dashed line), the closer our model was in predicting the number of COVID-19 cases up to 7 days ahead (panels in part A). In part B we see that the accuracy of the method is high for all 7 days ahead, but we begin to lose in terms of precision from day four onwards. (\\(r\\) represents Pearson‚Äôs linear correlation coefficient, the closest it is to 1 the better the method is; the same applies to the CCC - the concordance correlation coefficient.)\nThe idea behind this is not to be able to inform governments the exact numbers we‚Äôd expect tomorrow, but to give more perspective in terms of the types of trends we expect in the near future. This is useful to inform decision making related to the healthcare services. For instance, if a particular country‚Äôs healthcare system is currently at capacity, and we are predicting an upward trend in the number of infections, then this could guide policy in terms of resource allocation to accommodate the extra patients that are likely to seek health professionals in the upcoming weeks. This is why it is so important to look at overall trends (for example, the number of cases per 100,000 people over the last 14 days).\nOur model creates predictions based on two components. The first, called the autoregressive component, uses information on the past number of cases to predict future ones. The second is included to account for extra variability that could occur for a variety of different reasons. The autoregressive component is directly linked to the behaviour of the outbreak, so it is useful to detect waves of the pandemic. See, for example, our latest estimates for Ireland:\nWe can clearly see that towards the end of July this second wave was already starting to take shape, and now we are aiming at a new peak of cases.\n Grouping countries together Now that we have profiles for each country on how the pandemic is behaving in terms of number of cases, perhaps it would be a good idea to look at which countries present a similar behaviour over the last, say, 60 days. We created a dendrogram based on a cluster analysis performed using the values of the autoregressive parameter and produced the figure below ‚Äì\nHere we see that over looking at the past two months, the country that has presented the most similar behaviour to Ireland was Croatia. In our app you can play with different ways of presenting the dendrogram, as well as print names of different countries in bold to aid in finding them easily when looking at the picture. You can also change the number of clusters.\nPerhaps these comparisons would be useful in terms of comparing government policies on how to deal with the COVID-19 outbreak, and learn lessons from successful policies vs unsuccessful ones. Also, this type of modelling can help to detect a further wave of the outbreak sooner rather than when we are already in the middle of it!\n All models are wrong‚Ä¶ In the end of the day, there is no true, correct model we can apply. After all, it is impossible to know exactly what the data generating mechanism is. We can only attempt to understand it and reproduce its behaviour using mathematical/statistical tools. We hope, however, that our modelling approach can be useful. We could point a whole list of problems with it here, such as completely ignoring biological mechanisms and using just past behaviour to explain future behaviour without any additional context. But we believe it represents a reasonable attempt at forecasting the number of COVID-19 cases in the short-term.\nDid you find this page helpful? Consider sharing it üôå\n Citation For attribution, please cite this work as:  Moral, et al.¬†(2020, Sept.¬†29). Ireland‚Äôs COVID-19 Data Dive: How hard is it to predict COVID-19 cases?. Retrieved from https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/\n BibTeX citation  @misc{moral2020how, author = {Moral, Rafael and Oliveira, Thiago and Parnell, Andrew}, title = {Ireland\u0026#39;s COVID-19 Data Dive: How hard is it to predict COVID-19 cases?}, url = {https://www.hamilton.ie/covid19/posts/2020-10-01-how-hard-to-predict-cases/}, year = {2020} }  ","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608121186,"objectID":"8a17a1381fc8f69cddde821d4df2bfcb","permalink":"https://prof-thiagooliveira.netlify.com/post/how-hard-is-it-to-predict-covid-19-cases/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/how-hard-is-it-to-predict-covid-19-cases/","section":"post","summary":"tl;dr  Many different variables affect how the pandemic progresses and it is extremely difficult to identify each one, and precisely measure them. The data we have is surely innacurate, but could be a good proxy for understanding the behaviour of the coronavirus outbreak We developed a statistical model to obtain short-term forecasts of the number of COVID-19 cases We constantly update forecasts and make all results freely available to any country in the world through a web app   How many people will get infected tomorrow?","tags":["Covid-19","Statistics","State-Space Model","Statistical Models","Statistical Methodology"],"title":"How hard is it to predict COVID-19 cases?","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["C++","Computer Programs"],"content":"   Introduction When we think about writing a C program, the first step is to understand how variables should be assigned. There are several variable‚Äôs types in C, and here we are introducing the type int, which is used for integer data types. We can define a variable as an integer in two ways:  Uninitialized variable: defined as int x;, where no value is assigned to the variable \\(x\\) (Figure 1), which generally is not a good idea. It could lead to a bug in the algorithm if no value is assigned over the code. Initialized variable: there are two ways to assign a value to a variable \\(x\\) (Figure 1):  in a single declaration - int x = 3; in a double step declaration - int x; and x = 3;      Figure 1: Declaring variables in C  Additionally, there is a large set of storage size-specific declarations for an integer, and here we will explain just an initial idea about it. Figure 2 shows the Integer representation of whole numbers or fixed-point numbers (fixed number of digits). Generally, computers use a set number of bits to represent them, where commonly used bit-lengths for integers are 8-bit, 16-bit (short), 32-bit (long) or 64-bit (long long). There are two representation schemes for integers called signed integer type (signed int), capable of containing the range of values from -32,767 to 32,767, and unsigned integer type (unsigned int) containing the range of deals from 0 to 65,535 (\\(32767 \\times 2+1\\)). Therefore, an unsigned qualifier should be used when we are working with only positive values.\n Figure 2: Integer Representation  Furthermore, there are three representation schemes for signed integers called Sign-Magnitude representation, * 1‚Äôs Complement representation, and  2‚Äôs Complement representation*. The 1‚Äôs and the 2‚Äôs complements of a binary number are essential because they permit different negative numbers representation. In all of these schemes, positive signed binary numbers start with value 0 while negative ones start with value 1 (Figure 3).\n Figure 3: Signed binary numbers  Consequently, the disadvantage of signed binary numbers is that there is 1 bit used to store the sign positive or negative while the remaining \\(n-1\\) bits are assigned to the range of digits from \\(-2^{n-1}\\) to \\(2^{n-1}\\). If we have 8 bits to represent a signed binary number, we have to use 1 bit for the sign bit and 7 bits for the magnitude bits:\n Using Sign-Magnitude Representation: \\[-|2^{\\left(8-1\\right)}-1| \\mbox{ to } 2^{\\left(8-1\\right)}-1 = -127 \\mbox{ to } 127\\] Using 2‚Äôs Complement Representation: \\[-2^{\\left(8-1\\right)} \\mbox{ to } 2^{\\left(8-1\\right)}-1 = -128 \\mbox{ to } 127\\]   Thus, we can represent the numbers ranging from -128 to 127 using 2‚Äôs Complement Representation. You probably ask why there is one extra number being accounted for when using 2‚Äôs Complement Representation. The answer can be found in Figure 4.\n Figure 4: Representation schemes of Sign-Magnitude Representation and 2‚Äôs Complement Representation   Examples Unsigned int Suppose we are interested in representing a sequence of number \\(x\\) where \\(x \\in \\lbrace 0, 1, \\ldots, 15\\rbrace\\). We can assign these numbers as unsigned numbers of 4 bits. Consequently, we have 4 zero bits associated with describing these numbers because our variable belongs to the interval \\([0, 2^{4}‚àí1] \\in \\mathcal{N}_{0}\\).\n Table 1: Representation of numbers from 0 to 15 in 4 bits    bits  0000  0001  0010  0011  0100  0101  0110  0111  1000  1001  1010  1011  1100  1101  1110  1111    x  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15      Signed int Suppose now we are interested in representing a sequence of number \\(y\\) where \\(y \\in \\lbrace -7, -6, \\ldots,6, 7\\rbrace\\). We have to assign them as signed numbers using 4 bits because 1 bit will be used for sign bit and 3 bits for the magnitude bits to describe \\(y \\in \\left[-|2^3-1|,2^3-1\\right] \\in \\mathcal{Z}\\).\n Table 2: Sign-Magnitude Representation of numbers from -7 to 7 using 4 bits    bits  0111  0110  0101  0100  0011  0010  0001  0000  1000  1001  1010  1011  1100  1101  1110  1111    y  7  6  5  4  3  2  1  0  -0  -1  -2  -3  -4  -5  -6  -7      Table 3: 2‚Äôs Complement Representation of numbers from -8 to 7 using 4 bits    bits  1000  1001  1010  1011  1100  1101  1110  1111  0000  0001  0010  0011  0100  0101  0110  0111    y  -8  -7  -6  -5  -4  -3  -2  -1  0  1  2  3  4  5  6  7       References Barnett R.; O‚ÄôCull L.; Cox, S. Embedded C Programming and the Microship PIC. Delmar Learning, ed.¬†1, 2004.\nCadenhead, R.; Liberty, J. Sams Teach Yoirself C++. Pearson Education, ed.¬†6, 2017.\nC Data Types - https://en.wikipedia.org/wiki/C_data_types\n Citation For attribution, please cite this work as:  Oliveira T.P. (2020, Dec.¬†16). Signed and Unsigned Binary Numbers\n BibTeX citation  @misc{oliveira2020signed, author = {Oliveira, Thiago}, title = {Signed and Unsigned Binary Numbers}, url = {https://prof-thiagooliveira.netlify.app/post/signed-and-unsigned-binary-numbers/}, year = {2020} } Did you find this page helpful? Consider sharing it üôå\n ","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608120651,"objectID":"f9d843a0159600db57c80e48bbc780f6","permalink":"https://prof-thiagooliveira.netlify.com/post/signed-and-unsigned-binary-numbers/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/signed-and-unsigned-binary-numbers/","section":"post","summary":"Introduction When we think about writing a C program, the first step is to understand how variables should be assigned. There are several variable‚Äôs types in C, and here we are introducing the type int, which is used for integer data types.","tags":["C++","Computer Science","Computer Programs"],"title":"Signed and Unsigned Binary Numbers","type":"post"},{"authors":["Thiago de Paula Oliveira"],"categories":["C++","Computer Programs"],"content":"  Overview of the Seven Steps The seven steps proposed by Hilton et al.¬†(2019) is very interesting strategy to start a new project that involves programming process, where a summary of the entire process is shown in the Figure 1. Here we will describe these steps based on thw work of Hilton et al.¬†(2019).\n Figure 1: The seven steps (modified from Hilton et al.¬†(2019))  All steps are then described in the sections below.\n Step 1 - Project definition using simple examples This is when you spend time thinking about the project and how could you divide them into small tasks. Start your project by drawing a diagram of it by hand, including the main subjects, how you could sort out the problems, and how many main algorithms should you create to have your project done. Here you may include how the project could be sub-divided into smaller tasks, how these tasks are connected, and if there is an order to execute them (Figure 2). Consequently, this should reflect in the number of main algorithms to be built. Doing an excellent job during this stage will facilitate the remainder steps.\n Figure 2: Example of how divide the main project into small tasks  Example 1 Suppose we would want to write an algorithm in C++ to compute the total fat of an ice cream portion basis (\\(y\\)). Suppose also this response variable is a function of butyric fat \\(x_1\\) and vegetable fat \\(x_2\\). Let \\(E[y]\\) be the expected value of \\(y\\) defined as\n\\[E[y]=10-0.5x_1+0.6x_1^2-0.6x_2+0.2x_2^2+0.1x_1x_2\\] Thus, we can pick particular values for \\(x_1\\), and \\(x_2\\) to calculate the total fat \\(y\\) by hand. If \\(x_{1}=2\\), \\(x_{2}=1\\), then \\[y=10-0.5\\times2+0.6\\times 2^2-0.6\\times 1+0.2 \\times 1^2+0.1\\times 2 \\times 1 = 11.2.\\]\nNow, suppose the second aim is to optimise fat ice cream formulation from this fitted model. In this sense, we are looking for the global minimum through the response surface. Consequently, we can divide our project into, e. g., two tasks:\nGeneralize the function for any \\(x_1\\) and \\(x_2\\); Calculate the global (or absolute) minimum point;   If you get stuck in one of these steps, probably this difficult is comming from a lack of domain knowledge of a particular field, e. g., a lack of domain in mathematics:\n How could I calculate the global minimum? How can I use partial derivatives?   Thus, during this step, you have to identify all domain knowledge and then overcome them before going to the next step. Sometimes, domain knowledge may come from particular fields of computer science, sports, agriculture, statistics, or engineering.\n  Step 2 - Write everything you did In this step, you should take notes about what you did to solve the project‚Äôs project or tasks. Your notes describing all solutions by reading should be precise enough to anyone else reproduce them quickly. Sometimes we may omit common-sense steps such as multiply \\(x\\) by \\(y\\) or the order in which each task should be executed.\nExample 2 Suppose we are interested in computing \\(f(x,y)=x^y+3x\\) when \\(x=2\\) and \\(y=4\\), then we might write down a deatiled descriptions of all steps to compute \\(f(x,y)\\):\n Multiply 2 by 2 \\(\\rightarrow\\) you get 4 Multiply 4 by 2 \\(\\rightarrow\\) you get 8 Multiply 8 by 2 \\(\\rightarrow\\) you get 16 Sum 16 plus 3 multiplied by 2 \\(\\rightarrow\\) you get 22 22 is the answer.   The steps are precise as anyone who can perform basic math can follow these steps to get the same answer.\n  Step 3 - Generalize Our task is to generalize the last steps into an algorithm, finding patterns that allow us to solve the whole class rather than for particular parameter values. Here is two common way to generalize those steps into the algorithm:\n Look all details of your step 2 because, sometimes, you can find the generalization into it description. Look for repetition patterns - when the same step repeats several times   We can, e. g., generalize our Example 2 just looking the description into step 2, where we can replacing the occurrence of 2 by \\(x\\):  Multiply 2 by \\(x\\) \\(\\rightarrow\\) you get 4 Multiply 4 by \\(x\\) \\(\\rightarrow\\) you get 8 Multiply 8 by \\(x\\) \\(\\rightarrow\\) you get 16 Sum 16 plus 3 multiplied by \\(x\\) \\(\\rightarrow\\) you get 22 22 is the answer.   Note that, in the first multiplication, we have to start with \\(x \\times x=2 \\times x = 4\\), thus the number of times that we should multiply 2 by \\(x\\) are \\(y-1\\). Thus, we would lead to the following generalized steps:\nAlgorithm sketch 1 start with x = 2 and y = 4 n[1] = x Count up from i in 1 to y-1 n[i+1] = n[i] * x z = n[y] + 3 * x z is the answer This process is referred to as writing ‚Äòpseudo-code‚Äô as an algorithm design with no particular target language.\n  Step 4 - Test Your Algorithm Testing your algorithm is a practical step to ensure steps 1-3 are right before proceeding to step 5. Some examples of what you should do/think during this stage are described below:\n Test your algorithm choosing different values for parameters What happens if the value is positive, negative or equal to zero? Have you restricted parameter space? Ex.: \\(y\\geq 0\\). Use mathematical proofs There is always more than one right answer to a programming problem   Remember that parameter space is the space of possible parameter values that define a particular mathematical/statistical model, and they are generally a subset of finite-dimensional Euclidean space.\n Sometimes, we miss-generalizing our algorithm at step 3, and this mistake leads us again to steps 1-2. Generally, miss-generalization happens when we did not consider all possible cases during step 3 or did not have mathematical proofs about what we are doing.\nA good example of an algorithm mistake could be seen in Example 2. What happens if \\(y=0\\), or \\(y\u0026lt;0\\)? We can see that our algorithm mishandles these cases. If you calculate the algorithm steps by hand with \\(x=2\\) and \\(y=0\\), you will get \\(2^0=2\\) rather than \\(2^0=1\\) (correct answer). Additionally, for any value \\(y\\leq 0\\) the algorithm try to count from \\(1\\) to \\(y-1\u0026lt;0\\), of which are no Natural number, leading an error in the process. Thus, we can conclude that \\(|y| \\in \\mathcal{N}_{0}\\), where \\(\\mathcal{N}_{0}=\\mathcal{N} \\cup \\lbrace 0 \\rbrace\\) represents Natural numbers with zero. In this sense, we might attempt to generalize our algorithm to a higher number of cases:\nAlgorithm sketch 2 y must be a integer number start with x = 2 and y = 4 if y=0 { n[1] = 1 i=0 } else{ Count up from i in 1 to |y|-1 if y\u0026lt;0 { n[1] = 1/x n[i+1] = n[i] * 1/x } else{ n[1] = x n[i+1] = n[i] * x } } z = n[i+1] + 3 * x z is the answer  Figure 3: Example of output using the algorithm 2  Question: How can we improve this algorithm? Think about the case where \\(x=y=0\\).\n For some problems, particular cases require our attention. Every time we detect a problem with our algorithm in this step, we have to choose one of this option: Return to steps 1-3 to get more information to generalize the algorithm to a higher number of cases. Skip the last steps and fix the algorithm directly in step 4 (when we know how to fix the problem).   Example 3 The numbers in Figure 4 were obtained from an algorithm that has one parameter \\(N \\in \\mathcal{N}_{0}\\) to be specified, where \\(\\mathcal{N}_{0}=\\mathcal{N} \\cup \\lbrace 0 \\rbrace\\) represents Natural numbers with zero and a sequence of number as output values for each \\(N\\).\n Figure 4: Output of sequences of integers based on values of \\(N\\) from 0 to 4  Question: Determine the algorithm that was used to generate the numbers in this Figure. What is the result for \\(N=5\\)?\n   References [1] Hilton, AD; Lipp, GM; Rodger, SH, Translation from Problem to Code in Seven Steps, Comped 2019 Proceedings of the Acm Conference on Global Computing Education (2019), pp.¬†78-84.\n Answers Example 3 Algorithm sketch 3 N must be a Natural number with zero start with N = n, where n represents the value Minimum value = 4 * N Maximum value = 9 * N + 6 Increment of the sequence = 3 x[1] = Minimum value While x[i] is less than the Maximum value x[i] = x[i-1] + Increment of the sequence x is the answer # N = 6 N=5 seq \u0026lt;- seq(4*N, 9*N+6, 3) cat(\u0026quot;The answer is\u0026quot;, seq) ## The answer is 20 23 26 29 32 35 38 41 44 47 50   Citation For attribution, please cite this work as:  Oliveira T.P. (2020, Dec.¬†16). The seven steps of a programer\n BibTeX citation  @misc{oliveira2020seven, author = {Oliveira, Thiago}, title = {The seven steps of a programer}, url = {https://prof-thiagooliveira.netlify.app/post/the-seven-steps-of-a-programer/}, year = {2020} } Did you find this page helpful? Consider sharing it üôå\n ","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608119583,"objectID":"2a434fb16cdef7db0fdaca23027b096c","permalink":"https://prof-thiagooliveira.netlify.com/post/the-seven-steps-of-a-programer/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/post/the-seven-steps-of-a-programer/","section":"post","summary":"Overview of the Seven Steps The seven steps proposed by Hilton et al.¬†(2019) is very interesting strategy to start a new project that involves programming process, where a summary of the entire process is shown in the Figure 1.","tags":["Algorithm","C++","Computer Science"],"title":"The seven steps of a programer","type":"post"},{"authors":["Thiago de Paula Oliveira","Georgie Bruinvels","Charles Pedlar","Brian Moore","John Newell"],"categories":null,"content":"","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"019a1dece72ceaa27e7429675873733b","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-scireports/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/publication/2020-scireports/","section":"publication","summary":"Development of an appropriate parametric state-space formulation for the marginal distribution of standard menstrual cycles for female athletes","tags":["Menstrual cycle","Longitudinal data","State-Space Models","Athletes","Statistical Modelling","Hierarchical data"],"title":"Modelling menstrual cycle length in athletes using state-space models","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral"],"categories":null,"content":"Abstract:\nThe continuously growing number of COVID-19 cases pressures healthcare services worldwide. Accurate short-term forecasting is thus vital to support country-level policy making. The strategies adopted by countries to combat the pandemic vary, generating different uncertainty levels about the actual number of cases. Accounting for the hierarchical structure of the data and accommodating extra-variability is therefore fundamental. We introduce a new modelling framework to describe the course of the pandemic with great accuracy, and provide short-term daily forecasts for every country in the world. We show that our model generates highly accurate forecasts up to six days ahead, and use estimated model components to cluster countries based on recent events. We introduce statistical novelty in terms of modelling the autoregressive parameter as a function of time, increasing predictive power and flexibility to adapt to each country. Our model can also be used to forecast the number of deaths, study the effects of covariates (such as lockdown policies), and generate forecasts for smaller regions within countries. Consequently, it has strong implications for global planning and decision making. We constantly update forecasts and make all results freely available to any country in the world through an online Shiny dashboard.\n","date":1605178800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605178800,"objectID":"e2d66752b29503eaf43d2b19263b5f0e","permalink":"https://prof-thiagooliveira.netlify.com/talk/global-short-term-forecasting-of-covid-19-cases/","publishdate":"2020-11-10T12:00:00Z","relpermalink":"/talk/global-short-term-forecasting-of-covid-19-cases/","section":"event","summary":"Accurate short-term forecasting is thus vital to support country-level policy making during COVID-19 outbreak","tags":["COVID-19","Outbreak","State-Space Model","Longitudinal Data"],"title":"Global Short-Term Forecasting of Covid-19 Cases","type":"event"},{"authors":["Heloisa Thomazi Kleina1","Karla Kudlawiec","Mariana B. Esteves,","Marco A. Dalb√≥","Thiago de Paula Oliveira","Nathalie Maluta","Jo√£o R. S. Lopes","Louise L. May-De-Mio1"],"categories":null,"content":"","date":1597622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597622400,"objectID":"4d8941c87310fec1359b93fd5113237a","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-plant_pathology/","publishdate":"2020-08-17T00:00:00Z","relpermalink":"/publication/2020-plant_pathology/","section":"publication","summary":"We focused to investigate the host non preference and suitability for xylem-sap feeding among plum genotypes that differ in bacterial leaf scald intensity in the field.","tags":["Phytopathology","Statistical Modelling","Experimental design","Agriculture","Mixed-Effects Models","Generalized linear models","Hierarchical data"],"title":"Settling and feeding behavior of sharpshooter vectors on plum genotypes with different susceptibility levels to leaf scald disease (Xylella fastidiosa)","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral","Silvio Sandoval Zocchi","Clarice G. B. Demetrio","John Hinde"],"categories":null,"content":"Supplementary notes were added here:\n lcc package  Github: https://github.com/Prof-ThiagoOliveira/lcc CRAN: https://CRAN.R-project.org/package=lcc    ","date":1597276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597276800,"objectID":"e82d87d3e10df07ca946d00c59174724","permalink":"https://prof-thiagooliveira.netlify.com/publication/2020-lccpeerj/","publishdate":"2020-08-13T00:00:00Z","relpermalink":"/publication/2020-lccpeerj/","section":"publication","summary":"Describes the statistical package lcc using three real examples.","tags":["R Package","Bootstrap Confidence Intervals","Statistical Methods","Longitudinal data","Concordance Correlation Coefficient","Accuracy","Precision","Mixed-Effects Model"],"title":"lcc: an R package to estimate the concordance correlation, Pearson correlation, and accuracy over time","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral"],"categories":null,"content":"Abstract:\nThe continuously growing number of COVID-19 cases pressures healthcare services worldwide. Accurate short-term forecasting is thus vital to support country-level policy making. The strategies adopted by countries to combat the pandemic vary, generating different uncertainty levels about the actual number of cases. Accounting for the hierarchical structure of the data and accommodating extra-variability is therefore fundamental. We introduce a new modelling framework to describe the course of the pandemic with great accuracy, and provide short-term daily forecasts for every country in the world. We show that our model generates highly accurate forecasts up to six days ahead, and use estimated model components to cluster countries based on recent events. We introduce statistical novelty in terms of modelling the autoregressive parameter as a function of time, increasing predictive power and flexibility to adapt to each country. Our model can also be used to forecast the number of deaths, study the effects of covariates (such as lockdown policies), and generate forecasts for smaller regions within countries. Consequently, it has strong implications for global planning and decision making. We constantly update forecasts and make all results freely available to any country in the world through an online Shiny dashboard.\n","date":1591016400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591016400,"objectID":"7924c6f4fa53e4a406ec75819fe98ecf","permalink":"https://prof-thiagooliveira.netlify.com/talk/global-short-term-forecasting-of-covid-19-cases/","publishdate":"2020-05-30T00:00:00Z","relpermalink":"/talk/global-short-term-forecasting-of-covid-19-cases/","section":"event","summary":"Accurate short-term forecasting is thus vital to support country-level policy making during COVID-19 outbreak","tags":["COVID-19","Outbreak","State-Space Model","Longitudinal Data"],"title":"Global Short-Term Forecasting of Covid-19 Cases","type":"event"},{"authors":["Thiago de Paula Oliveira","John Newell"],"categories":null,"content":"Abstract:\nIn basketball, the athlete performance evaluation are generally based on variants of plus-minus and PER statistics Optimizing Athlete Performance calculated through multiple regression, ridge, or lasso models using likelihood-based or Bayesian approach. We developed a novel methodology based on principal components analysis and multilevel model to create new indexes such as Oliveira-Newell Score that can be used to evaluate player performance during a match, relevance score used to rank players in a season, and the consistence score used to evaluate the player contribution for their team based on random effects.\n","date":1587463200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587463200,"objectID":"78bc82b7e9a4f5e1a4fef630537a702d","permalink":"https://prof-thiagooliveira.netlify.com/talk/estimating-nba-athlete-performance-using-multilevel-models/","publishdate":"2020-04-21T10:00:00Z","relpermalink":"/talk/estimating-nba-athlete-performance-using-multilevel-models/","section":"event","summary":"Athlete performance evaluation based on novel methodology using principal components and multilevel models","tags":["Athlete Performance","Agreement Measures","Multilevel Model","Longitudinal Data"],"title":"Estimating NBA athlete performance using multilevel models","type":"event"},{"authors":null,"categories":null,"content":"","date":1578614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578614400,"objectID":"a079055becb5e8745abc07e89c75c78b","permalink":"https://prof-thiagooliveira.netlify.com/project/genetic-project/","publishdate":"2020-01-10T00:00:00Z","relpermalink":"/project/genetic-project/","section":"project","summary":"Quantitative genetics is a field of biology studying the effect of genetic and environmental factors on quantitative traits.","tags":["Statistics","Statistical modelling","Genetics","Genomics","Plant breeding"],"title":"Statistical models applied to quantitative genetics and genomics of plant breeding","type":"project"},{"authors":["Thiago de Paula Oliveira","John Newell"],"categories":null,"content":"Abstract:\nThe ability to predict menstrual cycle length to a high degree of precision enables female athletes to track their period and tailortheir training and nutrition correspondingly knowing when to push harder when to prioritise recovery and how to minimise theimpact of menstrual symptoms on performance. Such individualisation is possible if cycle length can be predicted to a highdegree of accuracy. To achieve this, a hybrid predictive model was built using data on 16,990 cycles collected from a sampleof 2,178 women (mean age 33.89 years, range 14.98 - 47.10, number of menstrual cycles ranging from 4 - 53). To capture thewithin-subject temporal correlation, a mixed-effect state-space model was fitted incorporating a Bayesian approach for processforecasting to predict the duration (in days) of the next menstrual cycle. The modelling procedure was split into three steps(i)a time trend component using a random walk with an overdispersion parameter, (ii) an autocorrelation component using anautoregressive moving-average (ARMA) model, and (iii) a linear predictor to account for covariates (e.g. injury, stomach cramps,training intensity). The inclusion of an overdispersion parameter suggested that26.81% [24.14%,29.58%]of cycles in the samplewere overdispersed where the random walk standard deviation under a non-overdispersed cycle is1.0530 [1.0060,1.0526]days whileunder an overdispersed cycle it increased to4.7386 [4.5379,4.9492]days. To assess the performance and prediction accuracy ofthe model, each woman‚Äôs last observation was used as test data. The Root Mean Square Error (RMSE), Concordance CorrelationCoefficient (CCC) and Pearson correlation coefficient (r) between the observed and predicted values were calculated. The modelhad an RMSE of 1.6126 days, a precision of 0.7501 and overall accuracy of 0.9855. In the absence of hormonal measurements,knowing how aspects of physiology and psychology are changing across the menstrual cycle has the potential to help femaleathletes personalise their training, nutrition and recovery tailored to their cycle to sustain peak performance at the highest leveland gain a competitive edge. In conclusion, the hybrid model presented here is a useful approach for predicting menstrual cyclelength which in turn can be used to support female athlete wellness to optimise performance ","date":1570179600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570179600,"objectID":"27444c4282bba3c0d95ddb2c8d585400","permalink":"https://prof-thiagooliveira.netlify.com/talk/modelling-menstrual-cycle-length-using-state-space-models/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/modelling-menstrual-cycle-length-using-state-space-models/","section":"event","summary":"Times are changing. At an elite level, female athletes and coaches across the globe are now starting to work with the menstrual cycle to gain a performance edge. By tracking the menstrual cycle, and knowing how, why and when hormone fluctuations affect female physiology, an athlete's training, nutrition and recovery can be tailored to their cycle to sustain peak performance\n","tags":["Bayesian Approach","State Space Models","Cycle Length","Performance","Autoregressive Models"],"title":"Modelling menstrual cycle length using state space models","type":"event"},{"authors":["Gustavo V. Popin","Arthur K. B. Santos","Thiago de Paula Oliveira","Pl√≠nio B. de Camargo","Carlos E. P. Cerri","Marcos Siqueira-Neto"],"categories":null,"content":"Supplementary notes were added here, including figures.\n","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563235200,"objectID":"5e8883e7dec03615d2d23522b2cb0e5a","permalink":"https://prof-thiagooliveira.netlify.com/publication/2019-soil/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-soil/","section":"publication","summary":"Evaluate the effects of global warming and straw removal on gas emissions","tags":["Global warming","Statistical Modelling","Gas emissions"],"title":"Sugarcane straw management for bioenergy: effects of global warming on greenhouse gas emissions and soil carbon storage","type":"publication"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral","John Hinde","Silvio Sandoval Zocchi","Clarice Garcia Borges Demetrio"],"categories":null,"content":"Abstract:\nWe present the lcc package, available from the Comprehensive R Archive Network (CRAN). The package implements estimation procedures for the longitudinal concordance correlation (LCC), using fixed effects and variance components estimates from linear mixed models. The LCC is a quantity that measures the extent of agreement between two (or more) methods used to evaluate a response variable of interest and is frequently applied in medicine, pharmacology, and agronomy. The main features of the package are the estimation and inference of the extent of agreement using numerical and graphical summaries. Moreover, our approach presents flexibility in the sense that it accommodates both balanced and unbalanced experimental designs, allows for different within-group error structures, while also allowing for the inclusion of covariates in the linear predictor to control systematic variations in the response. We illustrate our methodology by comparing different methods used to measure the peel colour of fruit as an assessment of ripeness.\n","date":1559898000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559898000,"objectID":"502f85002fd9bb5031c4a64b2cc56113","permalink":"https://prof-thiagooliveira.netlify.com/talk/the-longitudinal-concordance-correlation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/the-longitudinal-concordance-correlation/","section":"event","summary":"Abstract:\nWe present the lcc package, available from the Comprehensive R Archive Network (CRAN). The package implements estimation procedures for the longitudinal concordance correlation (LCC), using fixed effects and variance components estimates from linear mixed models.","tags":["R Package","Agreement Measures","Mixed-Effects Model","Longitudinal Data"],"title":"The longitudinal concordance correlation","type":"event"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://prof-thiagooliveira.netlify.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Thiago de Paula Oliveira","Rafael de Andrade Moral","Silvio S. Zocchi","Clarice G. B. Demetrio","John Hinde"],"categories":null,"content":"","date":1542326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542326400,"objectID":"45351a12a748374de8bdd61a5c271dc4","permalink":"https://prof-thiagooliveira.netlify.com/publication/2019-lcc-package/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-lcc-package/","section":"publication","summary":"We proposed an R package to estimate the longitudinal concordance correlation (LCC)","tags":["R Package","Bootstrap Confidence Intervals","Statistical Methods","Longitudinal data","Concordance Correlation Coefficient","Accuracy","Precision","Mixed-Effects Model"],"title":"lcc: Longitudinal Concordance Correlation","type":"publication"},{"authors":["Mariana B. Esteves","Heloisa T. Kleina","Tiago de Melo Sales","Thiago de Paula Oliveira","Idemauro A.R. Lara","Rodrigo P.P. Almeida","Helvecio D. Coletta-Filho","Jo√£o R.S. Lopes"],"categories":null,"content":"","date":1541808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541808000,"objectID":"6af98efad9f8f156e5616058ca2fbfe4","permalink":"https://prof-thiagooliveira.netlify.com/publication/2019-phitopatology/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2019-phitopatology/","section":"publication","summary":"Adapt and validate an in vitro acquisition system for X. fastidiosa by sharpshooters.","tags":["Phytopathology","Statistical Modelling","Experimental design","Agriculture","Mixed-Effects Models","Generalized linear models"],"title":"Transmission efficiency of xylella fastidiosa subsp. Pauca sequence types by sharpshooter vectors after in vitro acquisition","type":"publication"},{"authors":["Thiago de Paula Oliveira","Silvio S. Zocchi","John Hinde"],"categories":null,"content":"Supplementary notes were added here, including code and data.\n","date":1521676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521676800,"objectID":"f1a5a0e3d674f9b95525b671e1f8a5a5","permalink":"https://prof-thiagooliveira.netlify.com/publication/2018-lcc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2018-lcc/","section":"publication","summary":"We proposed a longitudinal concordance correlation (LCC) to estimate agreement over time among methods","tags":["Longitudinal data","Concordance Correlation Coefficient","Accuracy","Precision","Statistical Modelling","Statistical Methods","R Package","Bootstrap Confidence Intervals","Mixed-Effects Model"],"title":"Longitudinal concordance correlation function based on variance components: an application in fruit color analysis","type":"publication"},{"authors":null,"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"4cd459fb2711ceefd52c827ba1c5dd33","permalink":"https://prof-thiagooliveira.netlify.com/project/sport-project/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/sport-project/","section":"project","summary":"Uniquely blending Data Science and Sports Science to generate customized strategies by athlete","tags":["Statistical modelling","Athlete performance","Biostatistics","Shiny App","Sports"],"title":"The use of biostatistics for optimizing athletes performance","type":"project"},{"authors":["Thiago de Paula Oliveira","Silvio S. Zocchi","Angelo P. M. Jacomino"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"0352785b21d368da5202302fd9ba162c","permalink":"https://prof-thiagooliveira.netlify.com/publication/2017-papaya/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017-papaya/","section":"publication","summary":"It is recommended the usage of digital image analysis to access the fruit peel color when it has non-uniform coloration.","tags":["Fruit","Postharvest","Colour","Statistics"],"title":"Measuring color hue in 'Sunrise Solo' papaya using a flatbed scanner","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8c594946bd84a73e02ff7c1a2aefd944","permalink":"https://prof-thiagooliveira.netlify.com/project/color-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/color-project/","section":"project","summary":"Promote the usage of image analysis as well as development of statistical methodologies for that purpose.","tags":["Statistics","Image Analysis","Digital Images","Circular Statistics","Agriculture"],"title":"Measuring color using image analyis","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c6477ef5e689a66792634a2bc67116b5","permalink":"https://prof-thiagooliveira.netlify.com/project/concordance-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/concordance-project/","section":"project","summary":"Estimating longitudinal concordance correlation function","tags":["Agreement","Precision","Accuracy","R packages"],"title":"The lcc Package","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://prof-thiagooliveira.netlify.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]